{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Good Code for Hierarchical Classification\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import FATS\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# The different classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stars_label(data, label):\n",
    "    '''Set variable names to specific class label'''\n",
    "    stars = data[data.True_class_labels == label]\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Hierarchical Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_layer():\n",
    "    '''\n",
    "    We define first layer of the hierarchical tree. The first layer consists of Eclipsing Binaries, Rotational,\n",
    "    and Pulsating \n",
    "    '''\n",
    "    \n",
    "    # First Layer\n",
    "    eclipsing_binary_train       = pd.concat([contact_Bi_train, semi_det_Bi_train], axis=0)\n",
    "    eclipsing_binary_train_class = np.full(len(eclipsing_binary_train), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_train       = rot_train\n",
    "    rotational_train_class = np.full(len(rotational_train),rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_train       = pd.concat([RRab_train, RRc_train, RRd_train, blazhko_train, LPV_train, delta_scuti_train, ACEP_train, cep_ii_train] ,axis=0)\n",
    "    pulsating_train_class = np.full(len(pulsating_train), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_train has {}\".format(eclipsing_binary_train.shape))\n",
    "    print(\"pulsating_train has {}\".format(pulsating_train.shape))\n",
    "    print(\"rotational_train has {}\".format(rotational_train.shape))\n",
    "\n",
    "    eclipsing_binary_test       = pd.concat([contact_Bi_test, semi_det_Bi_test], axis=0)\n",
    "    eclipsing_binary_test_class = np.full(len(eclipsing_binary_test), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_test       = rot_test\n",
    "    rotational_test_class = np.full(len(rotational_test), rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_test       = pd.concat([RRab_test, RRc_test, RRd_test, blazhko_test, LPV_test, delta_scuti_test, ACEP_test, cep_ii_test] ,axis=0)\n",
    "    pulsating_test_class = np.full(len(pulsating_test), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_test has {}\".format(eclipsing_binary_test.shape))\n",
    "    print(\"pulsating_test has {}\".format(pulsating_test.shape))\n",
    "    print(\"rotational_test has {}\".format(rotational_test.shape))\n",
    "    \n",
    "    first_layer_train       = pd.concat([eclipsing_binary_train, rotational_train, pulsating_train], axis=0)\n",
    "    first_layer_train_class = np.concatenate((eclipsing_binary_train_class, rotational_train_class, pulsating_train_class), axis=0)\n",
    "    training_data_FL        = pd.DataFrame(first_layer_train)\n",
    "    training_data_FL['New_label'] = first_layer_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    first_layer_test       = pd.concat([eclipsing_binary_test, rotational_test, pulsating_test], axis=0)\n",
    "    first_layer_test_class = np.concatenate((eclipsing_binary_test_class, rotational_test_class, pulsating_test_class), axis=0)\n",
    "    testing_data_FL        = pd.DataFrame(first_layer_test)\n",
    "    testing_data_FL['New_label'] = first_layer_test_class\n",
    "    \n",
    "    y_FL_training, y_FL_training_counts = np.unique(first_layer_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_FL, testing_data_FL, y_FL_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for first Branch: Eclipsing Binaries (Ecl & EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_layer_EB():\n",
    "    \n",
    "    # Second Layer Eclipsing Binary    \n",
    "    ecl_train = contact_Bi_train\n",
    "    ecl_train_class = np.full(len(ecl_train), true_class_5, dtype=int)\n",
    "\n",
    "    EA_train       = semi_det_Bi_train\n",
    "    EA_train_class = np.full(len(EA_train),true_class_6, dtype=int)\n",
    " \n",
    "    print(\"ecl train has {}\".format(ecl_train.shape))\n",
    "    print(\"EA_train has {}\".format(EA_train.shape))\n",
    "\n",
    "    ecl_test       = contact_Bi_test\n",
    "    ecl_test_class = np.full(len(ecl_test), true_class_5, dtype=int)\n",
    "\n",
    "    EA_test       = semi_det_Bi_test\n",
    "    EA_test_class = np.full(len(EA_test), true_class_6, dtype=int)\n",
    "\n",
    "    print(\"ecl_test has {}\".format(ecl_test.shape))\n",
    "    print(\"EA_test has {}\".format(EA_test.shape))\n",
    "\n",
    "    \n",
    "    second_layer_EB_train       = pd.concat([ecl_train, EA_train], axis=0)\n",
    "    second_layer_EB_train_class = np.concatenate((ecl_train_class,EA_train_class), axis=0)\n",
    "    training_data_SL_EB         = pd.DataFrame(second_layer_EB_train)\n",
    "    training_data_SL_EB['New_label'] = second_layer_EB_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_EB_test       = pd.concat([ecl_test, EA_test], axis=0)\n",
    "    second_layer_EB_test_class = np.concatenate((ecl_test_class, EA_test_class), axis=0)\n",
    "    testing_data_SL_EB         = pd.DataFrame(second_layer_EB_test)\n",
    "    testing_data_SL_EB['New_label'] = second_layer_EB_test_class\n",
    "    \n",
    "    y_SL_EB_training, y_SL_EB_training_counts = np.unique(second_layer_EB_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for 2nd Branch: RLCD\n",
    "### RR Lyrae, LPV, Cepheid and $\\delta$-Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2 RR Lyrae, LPV, Cepheid, Delta-Scuti\n",
    "def second_layer_RLCD():\n",
    "    \n",
    "    # First Layer\n",
    "    RR_Lyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    RR_Lyrae_train_class = np.full(len(RR_Lyrae_train), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_train_class = np.full(len(LPV_train),LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_train       = pd.concat([ACEP_train,cep_ii_train] ,axis=0)\n",
    "    cepheids_train_class = np.full(len(cepheids_train), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_train       = delta_scuti_train\n",
    "    ds_train_class = np.full(len(ds_train), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR Lyrae train has {}\".format(RR_Lyrae_train.shape))\n",
    "    print(\"LPV train has {}\".format(LPV_train.shape))\n",
    "    print(\"Cepheids train has {}\".format(cepheids_train.shape))\n",
    "    print(\"Delta Scuti train has {}\".format(ds_train.shape))\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_test_class = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_test       = delta_scuti_test\n",
    "    ds_test_class = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR_Lyrae_test has {}\".format(RR_Lyrae_test.shape))\n",
    "    print(\"LPV_test has {}\".format(LPV_test.shape))\n",
    "    print(\"cepheids_test has {}\".format(cepheids_test.shape))\n",
    "    print(\"Delta Scuti test has {}\".format(ds_test.shape))\n",
    "    \n",
    "    second_layer_RLCD_train       = pd.concat([RR_Lyrae_train,LPV_train,cepheids_train,ds_train], axis=0)\n",
    "    second_layer_RLCD_train_class = np.concatenate((RR_Lyrae_train_class,LPV_train_class,cepheids_train_class,ds_train_class), axis=0)\n",
    "    training_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_train)\n",
    "    training_data_SL_RLCD['New_label'] = second_layer_RLCD_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    y_SL_RLCD_training, y_SL_RLCD_training_counts = np.unique(second_layer_RLCD_train_class, return_counts=True)\n",
    "\n",
    "    print(y_SL_RLCD_training)\n",
    "    print(y_SL_RLCD_training_counts)\n",
    "    return training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for first Branch: RRLyrae\n",
    "### RRab, RRc, RRd, and Blazhko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_RRLyrae():\n",
    "    \n",
    "    # Third Layer\n",
    "    RRab_train_class    = np.full(len(RRab_train), true_class_1, dtype=int)\n",
    "    RRc_train_class     = np.full(len(RRc_train), true_class_2, dtype=int)\n",
    "    RRd_train_class     = np.full(len(RRd_train), true_class_3, dtype=int)\n",
    "    blazhko_train_class = np.full(len(blazhko_train), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab train has {}\".format(RRab_train.shape))\n",
    "    print(\"RRc train has {}\".format(RRc_train.shape))\n",
    "    print(\"RRd train has {}\".format(RRd_train.shape))\n",
    "    print(\"Blazhko train has {}\".format(blazhko_train.shape))\n",
    "    \n",
    "    RRab_test_class    = np.full(len(RRab_test), true_class_1, dtype=int)\n",
    "    RRc_test_class     = np.full(len(RRc_test), true_class_2, dtype=int)\n",
    "    RRd_test_class     = np.full(len(RRd_test), true_class_3, dtype=int)\n",
    "    blazhko_test_class = np.full(len(blazhko_test), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab test has {}\".format(RRab_test.shape))\n",
    "    print(\"RRc test has {}\".format(RRc_test.shape))\n",
    "    print(\"RRd test has {}\".format(RRd_test.shape))\n",
    "    print(\"Blazhko test has {}\".format(blazhko_test.shape))\n",
    "\n",
    "    \n",
    "    third_layer_RRLyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    third_layer_RRLyrae_train_class = np.concatenate((RRab_train_class,RRc_train_class,RRd_train_class,blazhko_train_class), axis=0)\n",
    "    training_data_TL_RRLyrae        = pd.DataFrame(third_layer_RRLyrae_train)\n",
    "    training_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_RRLyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    third_layer_RRLyrae_test_class = np.concatenate((RRab_test_class,RRc_test_class,RRd_test_class,blazhko_test_class), axis=0)\n",
    "    testing_data_TL_RRLyrae         = pd.DataFrame(third_layer_RRLyrae_test)\n",
    "    testing_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_test_class\n",
    "    \n",
    "    y_TL_RRLyrae_training, y_TL_RRLyrae_training_counts = np.unique(third_layer_RRLyrae_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for 2nd Branch: Cepheids\n",
    "### ACEP and Cep-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_Cepheids():\n",
    "    \n",
    "    # Third Layer\n",
    "    ACEP_train_class   = np.full(len(ACEP_train), true_class_10, dtype=int)\n",
    "    cep_ii_train_class = np.full(len(cep_ii_train), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP train has {}\".format(ACEP_train.shape))\n",
    "    print(\"Cep-II train has {}\".format(cep_ii_train.shape))\n",
    "\n",
    "\n",
    "    ACEP_test_class   = np.full(len(ACEP_test), true_class_10, dtype=int)\n",
    "    cep_ii_test_class = np.full(len(cep_ii_test), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP test has {}\".format(ACEP_test.shape))\n",
    "    print(\"Cep-II test has {}\".format(cep_ii_test.shape))\n",
    "    \n",
    "    third_layer_cep_train       = pd.concat([ACEP_train,cep_ii_train], axis=0)\n",
    "    third_layer_cep_train_class = np.concatenate((ACEP_train_class,cep_ii_train_class), axis=0)\n",
    "    training_data_TL_cep        = pd.DataFrame(third_layer_cep_train)\n",
    "    training_data_TL_cep['New_label'] = third_layer_cep_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_cep_test       = pd.concat([ACEP_test,cep_ii_test], axis=0)\n",
    "    third_layer_cep_test_class = np.concatenate((ACEP_test_class,cep_ii_test_class), axis=0)\n",
    "    testing_data_TL_cep        = pd.DataFrame(third_layer_cep_test)\n",
    "    testing_data_TL_cep['New_label'] = third_layer_cep_test_class\n",
    "    \n",
    "    y_TL_cep_training, y_TL_cep_training_counts = np.unique(third_layer_cep_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test):\n",
    "    scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "    X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "    y_train_label         = x_train.New_label\n",
    "    filename_train        = x_train.File_Name\n",
    "\n",
    "    X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "    y_test_label         = x_test.New_label\n",
    "    filename_test        = x_test.File_Name\n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(X_train,y_train,classifer, param_grid, n_iter, cv, filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    "    \n",
    "    misclassified     = np.where(y_test != ypred)[0]\n",
    " \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()\n",
    "        \n",
    "    return ypred, accuracy, MCC, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=16)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis_rf(X_train, y_train, types,save_model=False):\n",
    "    n_estimators      = np.arange(50,1000,100)\n",
    "    max_features      = ['auto', 'sqrt', 'log2']\n",
    "    min_samples_split = np.arange(1,20,1)\n",
    "    #max_depth         = np.arange(1,10,2)\n",
    "    param_grid        = dict(n_estimators=n_estimators, max_features=max_features, \\\n",
    "                              min_samples_split=min_samples_split)#,max_depth=max_depth\n",
    "\n",
    "        \n",
    "    opt_parameters_rf = gridsearch(X_train,y_train,RandomForestClassifier(), param_grid, n_iter = 2, cv = 5, filename= results_dir + types+'_RF_hyparameters.txt')\n",
    "    fit_model = model_save(RandomForestClassifier(**opt_parameters_rf), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types+'_RF_model.sav', save_model=save_model)\n",
    "    return opt_parameters_rf, fit_model\n",
    "\n",
    "def final_prediction(fitModel,X_train, y_train, X_test, y_test, classes, types, nClasses,load_model=False):\n",
    "\n",
    "    ypred, accuracy, MCC, conf_mat = model_fit(fitModel,filename_model= results_dir + types +'_RF_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='Random Forest Classifier',classes=classes, filename =results_dir + types+'_RF',load_model=load_model)\n",
    "\n",
    "\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='Random Forest Classifier',\\\n",
    "                                  plot_title= plots_dir + types + '_RF', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Reds)\n",
    "\n",
    "    return ypred, accuracy, MCC, conf_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analysis_XGB(X_train, y_train, types,save_model=False,multi=True):\n",
    "    eta       = [0.01]    \n",
    "    if multi:\n",
    "        objective = ['multi:softmax']\n",
    "    else: \n",
    "        objective = ['binary:logistic']\n",
    "\n",
    "    max_depth = np.arange(1,12,2)\n",
    "    subsample = [0.5]\n",
    "    param_grid  = dict(eta=eta,objective=objective,max_depth=max_depth,subsample=subsample)\n",
    "        \n",
    "    opt_parameters_XGB = gridsearch( X_train, y_train,XGBClassifier(), param_grid, n_iter = 5, cv = 5, filename= results_dir + types+ '_XGB_hyparameters.txt')\n",
    "    fit_model = model_save(XGBClassifier(**opt_parameters_XGB), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    return ypred, accuracy, MCC, conf_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smote_augmentation(training,testing):\n",
    "    X_train_normalisation, y_train_np, filename_train, X_test_normalisation,\\\n",
    "    y_test_np, filename_test = normalisation(x_train=training,x_test=testing) \n",
    "    \n",
    "    y_label_bf = np.unique(y_train_np)\n",
    "    if augmentation:\n",
    "        for i in range(len(y_label_bf)):\n",
    "            print(\"Before OverSampling, counts of label {}: {}\".format(y_label_bf[i],(y_train_np[y_train_np==y_label_bf[i]]).shape))\n",
    "\n",
    "\n",
    "\n",
    "        # sm = SMOTE(random_state=2, ratio = 1.0,kind='svm')\n",
    "        sm = SMOTE(ratio = 'all') # Using Kind: Regular\n",
    "        X_train_aug, y_train_aug = sm.fit_sample(X_train_normalisation, y_train_np.ravel())\n",
    "        data_1 = pd.DataFrame(X_train_aug)\n",
    "        data_1['True_class_labels'] = y_train_aug\n",
    "        X_train_norm = data_1.iloc[:,0:nFeatures]\n",
    "        y_train_norm = data_1.iloc[:,nFeatures]\n",
    "\n",
    "        y_label_af = np.unique(y_train_norm)\n",
    "        print('-'*70)\n",
    "        for j in range(len(y_label_af)):\n",
    "                print(\"After OverSampling, counts of label {}: {}\".format(y_label_af[j],y_train_norm.loc[y_train_norm==y_label_af[j]].shape))\n",
    "\n",
    "\n",
    "        X_train = X_train_norm   \n",
    "        y_train = y_train_norm\n",
    "        y_test  = y_test_np\n",
    "        X_test  = X_test_normalisation\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data consists of (23143, 9) samples\n"
     ]
    }
   ],
   "source": [
    "nFeatures = 7\n",
    "\n",
    "# The directory to save the files\n",
    "plots_dir                 = './hierarchical-results_SMOTE/plots/'\n",
    "results_dir               = './hierarchical-results_SMOTE/results/'\n",
    "misclassify_dir           = r'./hierarchical-results_SMOTE/results/Misclassification_'\n",
    "\n",
    "\n",
    "eclipsing_label = 20;rotational_label = 21;pulsating_label = 22;RR_Lyrae_label = 23; LPV_label = 24;\\\n",
    "delta_scuti_label = 25; cepheids_label   = 26\n",
    "\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "n_splits         = 5\n",
    "data_preparation = True\n",
    "multi_class      = True\n",
    "augmentation     = True\n",
    "save_model       = False\n",
    "load_model       = False\n",
    "feature_directory         ='./data/'\n",
    "feature_data     = 'features_alldata.csv'\n",
    "features         = pd.read_csv(feature_directory+feature_data)\n",
    "print('The data consists of {} samples'.format(features.shape))\n",
    "\n",
    "\n",
    "acc_rf_FL = [];mcc_rf_FL = [];acc_xgb_FL = [];mcc_xgb_FL = [];\\\n",
    "acc_rf_SL_EB = [];mcc_rf_SL_EB = [];acc_xgb_SL_EB = [];mcc_xgb_SL_EB = [];\\\n",
    "acc_rf_SL_RLCD = [];mcc_rf_SL_RLCD = [];acc_xgb_SL_RLCD = [];mcc_xgb_SL_RLCD = [];\\\n",
    "acc_rf_TL_RL = [];mcc_rf_TL_RL = [];acc_xgb_TL_RL = [];mcc_xgb_TL_RL = [];\\\n",
    "acc_rf_TL_Cep = [];mcc_rf_TL_Cep = [];acc_xgb_TL_Cep = [];mcc_xgb_TL_Cep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipsing_binary_train has (7214, 9)\n",
      "pulsating_train has (8387, 9)\n",
      "rotational_train has (2908, 9)\n",
      "eclipsing_binary_test has (1804, 9)\n",
      "pulsating_test has (2102, 9)\n",
      "rotational_test has (728, 9)\n",
      "Before OverSampling, counts of label 20: (7214,)\n",
      "Before OverSampling, counts of label 21: (2908,)\n",
      "Before OverSampling, counts of label 22: (8387,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (8387,)\n",
      "After OverSampling, counts of label 21: (8387,)\n",
      "After OverSampling, counts of label 22: (8387,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.77217295  0.14412417  0.08370288]\n",
      " [ 0.16346154  0.67307692  0.16346154]\n",
      " [ 0.07992388  0.06089439  0.85918173]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.7804878   0.15243902  0.06707317]\n",
      " [ 0.14697802  0.71153846  0.14148352]\n",
      " [ 0.06517602  0.06898192  0.86584206]]\n",
      "ecl train has (3607, 9)\n",
      "EA_train has (3607, 9)\n",
      "ecl_test has (902, 9)\n",
      "EA_test has (902, 9)\n",
      "Before OverSampling, counts of label 5: (3607,)\n",
      "Before OverSampling, counts of label 6: (3607,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3607,)\n",
      "After OverSampling, counts of label 6: (3607,)\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.90909091  0.09090909]\n",
      " [ 0.07649667  0.92350333]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "Normalized confusion matrix\n",
      "[[ 0.90133038  0.09866962]\n",
      " [ 0.07427938  0.92572062]]\n",
      "RR Lyrae train has (6998, 9)\n",
      "LPV train has (1028, 9)\n",
      "Cepheids train has (244, 9)\n",
      "Delta Scuti train has (117, 9)\n",
      "RR_Lyrae_test has (1752, 9)\n",
      "LPV_test has (258, 9)\n",
      "cepheids_test has (62, 9)\n",
      "Delta Scuti test has (30, 9)\n",
      "[23 24 25 26]\n",
      "[6998 1028  117  244]\n",
      "Before OverSampling, counts of label 23: (6998,)\n",
      "Before OverSampling, counts of label 24: (1028,)\n",
      "Before OverSampling, counts of label 25: (117,)\n",
      "Before OverSampling, counts of label 26: (244,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (6998,)\n",
      "After OverSampling, counts of label 24: (6998,)\n",
      "After OverSampling, counts of label 25: (6998,)\n",
      "After OverSampling, counts of label 26: (6998,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.99086758  0.          0.00228311  0.00684932]\n",
      " [ 0.          0.99224806  0.          0.00775194]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.14516129  0.          0.          0.85483871]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.9902968   0.          0.00171233  0.00799087]\n",
      " [ 0.          1.          0.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.09677419  0.          0.          0.90322581]]\n",
      "RRab train has (3460, 9)\n",
      "RRc train has (3001, 9)\n",
      "RRd train has (401, 9)\n",
      "Blazhko train has (136, 9)\n",
      "RRab test has (865, 9)\n",
      "RRc test has (751, 9)\n",
      "RRd test has (101, 9)\n",
      "Blazhko test has (35, 9)\n",
      "Before OverSampling, counts of label 1: (3460,)\n",
      "Before OverSampling, counts of label 2: (3001,)\n",
      "Before OverSampling, counts of label 3: (401,)\n",
      "Before OverSampling, counts of label 4: (136,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3460,)\n",
      "After OverSampling, counts of label 2: (3460,)\n",
      "After OverSampling, counts of label 3: (3460,)\n",
      "After OverSampling, counts of label 4: (3460,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.92369942  0.00231214  0.00578035  0.06820809]\n",
      " [ 0.00133156  0.89613848  0.1011984   0.00133156]\n",
      " [ 0.03960396  0.46534653  0.4950495   0.        ]\n",
      " [ 0.65714286  0.          0.          0.34285714]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.91676301  0.00231214  0.00578035  0.07514451]\n",
      " [ 0.00133156  0.89347537  0.10386152  0.00133156]\n",
      " [ 0.02970297  0.40594059  0.56435644  0.        ]\n",
      " [ 0.42857143  0.          0.          0.57142857]]\n",
      "ACEP train has (122, 9)\n",
      "Cep-II train has (122, 9)\n",
      "ACEP test has (31, 9)\n",
      "Cep-II test has (31, 9)\n",
      "Before OverSampling, counts of label 10: (122,)\n",
      "Before OverSampling, counts of label 12: (122,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (122,)\n",
      "After OverSampling, counts of label 12: (122,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.87096774  0.12903226]\n",
      " [ 0.12903226  0.87096774]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 1}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 1}\n",
      "Normalized confusion matrix\n",
      "[[ 0.83870968  0.16129032]\n",
      " [ 0.19354839  0.80645161]]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 1 is finished\n",
      "eclipsing_binary_train has (7214, 9)\n",
      "pulsating_train has (8389, 9)\n",
      "rotational_train has (2909, 9)\n",
      "eclipsing_binary_test has (1804, 9)\n",
      "pulsating_test has (2100, 9)\n",
      "rotational_test has (727, 9)\n",
      "Before OverSampling, counts of label 20: (7214,)\n",
      "Before OverSampling, counts of label 21: (2909,)\n",
      "Before OverSampling, counts of label 22: (8389,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (8389,)\n",
      "After OverSampling, counts of label 21: (8389,)\n",
      "After OverSampling, counts of label 22: (8389,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.76496674  0.14523282  0.08980044]\n",
      " [ 0.15543329  0.71939477  0.12517194]\n",
      " [ 0.06761905  0.07285714  0.85952381]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.75776053  0.15354767  0.0886918 ]\n",
      " [ 0.12792297  0.75515818  0.11691884]\n",
      " [ 0.06238095  0.07952381  0.85809524]]\n",
      "ecl train has (3607, 9)\n",
      "EA_train has (3607, 9)\n",
      "ecl_test has (902, 9)\n",
      "EA_test has (902, 9)\n",
      "Before OverSampling, counts of label 5: (3607,)\n",
      "Before OverSampling, counts of label 6: (3607,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3607,)\n",
      "After OverSampling, counts of label 6: (3607,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.92904656  0.07095344]\n",
      " [ 0.0864745   0.9135255 ]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.92793792  0.07206208]\n",
      " [ 0.08425721  0.91574279]]\n",
      "RR Lyrae train has (6999, 9)\n",
      "LPV train has (1029, 9)\n",
      "Cepheids train has (244, 9)\n",
      "Delta Scuti train has (117, 9)\n",
      "RR_Lyrae_test has (1751, 9)\n",
      "LPV_test has (257, 9)\n",
      "cepheids_test has (62, 9)\n",
      "Delta Scuti test has (30, 9)\n",
      "[23 24 25 26]\n",
      "[6999 1029  117  244]\n",
      "Before OverSampling, counts of label 23: (6999,)\n",
      "Before OverSampling, counts of label 24: (1029,)\n",
      "Before OverSampling, counts of label 25: (117,)\n",
      "Before OverSampling, counts of label 26: (244,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (6999,)\n",
      "After OverSampling, counts of label 24: (6999,)\n",
      "After OverSampling, counts of label 25: (6999,)\n",
      "After OverSampling, counts of label 26: (6999,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.98800685  0.          0.00228441  0.00970874]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.03333333  0.          0.96666667  0.        ]\n",
      " [ 0.08064516  0.          0.          0.91935484]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.98972016  0.          0.00228441  0.00799543]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.03333333  0.          0.96666667  0.        ]\n",
      " [ 0.03225806  0.          0.          0.96774194]]\n",
      "RRab train has (3460, 9)\n",
      "RRc train has (3001, 9)\n",
      "RRd train has (401, 9)\n",
      "Blazhko train has (137, 9)\n",
      "RRab test has (865, 9)\n",
      "RRc test has (751, 9)\n",
      "RRd test has (101, 9)\n",
      "Blazhko test has (34, 9)\n",
      "Before OverSampling, counts of label 1: (3460,)\n",
      "Before OverSampling, counts of label 2: (3001,)\n",
      "Before OverSampling, counts of label 3: (401,)\n",
      "Before OverSampling, counts of label 4: (137,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3460,)\n",
      "After OverSampling, counts of label 2: (3460,)\n",
      "After OverSampling, counts of label 3: (3460,)\n",
      "After OverSampling, counts of label 4: (3460,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.9433526   0.          0.00346821  0.05317919]\n",
      " [ 0.00266312  0.90279627  0.09454061  0.        ]\n",
      " [ 0.05940594  0.42574257  0.5049505   0.00990099]\n",
      " [ 0.67647059  0.          0.          0.32352941]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.9132948   0.          0.00462428  0.08208092]\n",
      " [ 0.00133156  0.87882823  0.11984021  0.        ]\n",
      " [ 0.06930693  0.3960396   0.52475248  0.00990099]\n",
      " [ 0.64705882  0.          0.          0.35294118]]\n",
      "ACEP train has (122, 9)\n",
      "Cep-II train has (122, 9)\n",
      "ACEP test has (31, 9)\n",
      "Cep-II test has (31, 9)\n",
      "Before OverSampling, counts of label 10: (122,)\n",
      "Before OverSampling, counts of label 12: (122,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (122,)\n",
      "After OverSampling, counts of label 12: (122,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.96774194  0.03225806]\n",
      " [ 0.16129032  0.83870968]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/matplotlib/pyplot.py:516: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "Normalized confusion matrix\n",
      "[[ 0.93548387  0.06451613]\n",
      " [ 0.16129032  0.83870968]]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 2 is finished\n",
      "eclipsing_binary_train has (7214, 9)\n",
      "pulsating_train has (8392, 9)\n",
      "rotational_train has (2909, 9)\n",
      "eclipsing_binary_test has (1804, 9)\n",
      "pulsating_test has (2097, 9)\n",
      "rotational_test has (727, 9)\n",
      "Before OverSampling, counts of label 20: (7214,)\n",
      "Before OverSampling, counts of label 21: (2909,)\n",
      "Before OverSampling, counts of label 22: (8392,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (8392,)\n",
      "After OverSampling, counts of label 21: (8392,)\n",
      "After OverSampling, counts of label 22: (8392,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.76940133  0.13192905  0.09866962]\n",
      " [ 0.14855571  0.70976616  0.14167813]\n",
      " [ 0.07868383  0.08106819  0.84024797]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.77161863  0.13691796  0.09146341]\n",
      " [ 0.13617607  0.73590096  0.12792297]\n",
      " [ 0.06771578  0.08869814  0.84358608]]\n",
      "ecl train has (3607, 9)\n",
      "EA_train has (3607, 9)\n",
      "ecl_test has (902, 9)\n",
      "EA_test has (902, 9)\n",
      "Before OverSampling, counts of label 5: (3607,)\n",
      "Before OverSampling, counts of label 6: (3607,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3607,)\n",
      "After OverSampling, counts of label 6: (3607,)\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.90022173  0.09977827]\n",
      " [ 0.08093126  0.91906874]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 3}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 3}\n",
      "Normalized confusion matrix\n",
      "[[ 0.89800443  0.10199557]\n",
      " [ 0.07206208  0.92793792]]\n",
      "RR Lyrae train has (7001, 9)\n",
      "LPV train has (1029, 9)\n",
      "Cepheids train has (244, 9)\n",
      "Delta Scuti train has (118, 9)\n",
      "RR_Lyrae_test has (1749, 9)\n",
      "LPV_test has (257, 9)\n",
      "cepheids_test has (62, 9)\n",
      "Delta Scuti test has (29, 9)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  244]\n",
      "Before OverSampling, counts of label 23: (7001,)\n",
      "Before OverSampling, counts of label 24: (1029,)\n",
      "Before OverSampling, counts of label 25: (118,)\n",
      "Before OverSampling, counts of label 26: (244,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (7001,)\n",
      "After OverSampling, counts of label 24: (7001,)\n",
      "After OverSampling, counts of label 25: (7001,)\n",
      "After OverSampling, counts of label 26: (7001,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.99085192  0.          0.00171527  0.00743282]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.08064516  0.01612903  0.          0.90322581]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.99028016  0.          0.00228702  0.00743282]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.0483871   0.01612903  0.          0.93548387]]\n",
      "RRab train has (3460, 9)\n",
      "RRc train has (3002, 9)\n",
      "RRd train has (402, 9)\n",
      "Blazhko train has (137, 9)\n",
      "RRab test has (865, 9)\n",
      "RRc test has (750, 9)\n",
      "RRd test has (100, 9)\n",
      "Blazhko test has (34, 9)\n",
      "Before OverSampling, counts of label 1: (3460,)\n",
      "Before OverSampling, counts of label 2: (3002,)\n",
      "Before OverSampling, counts of label 3: (402,)\n",
      "Before OverSampling, counts of label 4: (137,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3460,)\n",
      "After OverSampling, counts of label 2: (3460,)\n",
      "After OverSampling, counts of label 3: (3460,)\n",
      "After OverSampling, counts of label 4: (3460,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.94104046  0.00231214  0.00231214  0.05433526]\n",
      " [ 0.004       0.88533333  0.11066667  0.        ]\n",
      " [ 0.05        0.42        0.53        0.        ]\n",
      " [ 0.58823529  0.          0.02941176  0.38235294]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.92485549  0.00115607  0.00346821  0.07052023]\n",
      " [ 0.004       0.86533333  0.13066667  0.        ]\n",
      " [ 0.04        0.38        0.57        0.01      ]\n",
      " [ 0.47058824  0.          0.02941176  0.5       ]]\n",
      "ACEP train has (122, 9)\n",
      "Cep-II train has (122, 9)\n",
      "ACEP test has (31, 9)\n",
      "Cep-II test has (31, 9)\n",
      "Before OverSampling, counts of label 10: (122,)\n",
      "Before OverSampling, counts of label 12: (122,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (122,)\n",
      "After OverSampling, counts of label 12: (122,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.96774194  0.03225806]\n",
      " [ 0.09677419  0.90322581]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 1}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 1}\n",
      "Normalized confusion matrix\n",
      "[[ 0.96774194  0.03225806]\n",
      " [ 0.16129032  0.83870968]]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 3 is finished\n",
      "eclipsing_binary_train has (7214, 9)\n",
      "pulsating_train has (8394, 9)\n",
      "rotational_train has (2909, 9)\n",
      "eclipsing_binary_test has (1804, 9)\n",
      "pulsating_test has (2095, 9)\n",
      "rotational_test has (727, 9)\n",
      "Before OverSampling, counts of label 20: (7214,)\n",
      "Before OverSampling, counts of label 21: (2909,)\n",
      "Before OverSampling, counts of label 22: (8394,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (8394,)\n",
      "After OverSampling, counts of label 21: (8394,)\n",
      "After OverSampling, counts of label 22: (8394,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.77050998  0.14689579  0.08259424]\n",
      " [ 0.14718019  0.74828061  0.1045392 ]\n",
      " [ 0.08019093  0.08019093  0.83961814]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.7594235   0.16186253  0.07871397]\n",
      " [ 0.12517194  0.78679505  0.08803301]\n",
      " [ 0.06157518  0.08353222  0.8548926 ]]\n",
      "ecl train has (3607, 9)\n",
      "EA_train has (3607, 9)\n",
      "ecl_test has (902, 9)\n",
      "EA_test has (902, 9)\n",
      "Before OverSampling, counts of label 5: (3607,)\n",
      "Before OverSampling, counts of label 6: (3607,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3607,)\n",
      "After OverSampling, counts of label 6: (3607,)\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.90687361  0.09312639]\n",
      " [ 0.0631929   0.9368071 ]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "Normalized confusion matrix\n",
      "[[ 0.91463415  0.08536585]\n",
      " [ 0.06762749  0.93237251]]\n",
      "RR Lyrae train has (7001, 9)\n",
      "LPV train has (1029, 9)\n",
      "Cepheids train has (246, 9)\n",
      "Delta Scuti train has (118, 9)\n",
      "RR_Lyrae_test has (1749, 9)\n",
      "LPV_test has (257, 9)\n",
      "cepheids_test has (60, 9)\n",
      "Delta Scuti test has (29, 9)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  246]\n",
      "Before OverSampling, counts of label 23: (7001,)\n",
      "Before OverSampling, counts of label 24: (1029,)\n",
      "Before OverSampling, counts of label 25: (118,)\n",
      "Before OverSampling, counts of label 26: (246,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (7001,)\n",
      "After OverSampling, counts of label 24: (7001,)\n",
      "After OverSampling, counts of label 25: (7001,)\n",
      "After OverSampling, counts of label 26: (7001,)\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.98627787  0.          0.00114351  0.01257862]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.10344828  0.          0.89655172  0.        ]\n",
      " [ 0.06666667  0.          0.          0.93333333]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 7}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 7}\n",
      "Normalized confusion matrix\n",
      "[[  9.88564894e-01   0.00000000e+00   5.71755289e-04   1.08633505e-02]\n",
      " [  0.00000000e+00   9.96108949e-01   0.00000000e+00   3.89105058e-03]\n",
      " [  1.37931034e-01   0.00000000e+00   8.62068966e-01   0.00000000e+00]\n",
      " [  3.33333333e-02   1.66666667e-02   0.00000000e+00   9.50000000e-01]]\n",
      "RRab train has (3460, 9)\n",
      "RRc train has (3002, 9)\n",
      "RRd train has (402, 9)\n",
      "Blazhko train has (137, 9)\n",
      "RRab test has (865, 9)\n",
      "RRc test has (750, 9)\n",
      "RRd test has (100, 9)\n",
      "Blazhko test has (34, 9)\n",
      "Before OverSampling, counts of label 1: (3460,)\n",
      "Before OverSampling, counts of label 2: (3002,)\n",
      "Before OverSampling, counts of label 3: (402,)\n",
      "Before OverSampling, counts of label 4: (137,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3460,)\n",
      "After OverSampling, counts of label 2: (3460,)\n",
      "After OverSampling, counts of label 3: (3460,)\n",
      "After OverSampling, counts of label 4: (3460,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.93641618  0.00115607  0.00231214  0.06011561]\n",
      " [ 0.004       0.912       0.084       0.        ]\n",
      " [ 0.05        0.45        0.49        0.01      ]\n",
      " [ 0.5         0.          0.          0.5       ]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.92601156  0.00115607  0.00231214  0.07052023]\n",
      " [ 0.004       0.888       0.108       0.        ]\n",
      " [ 0.04        0.39        0.56        0.01      ]\n",
      " [ 0.47058824  0.          0.          0.52941176]]\n",
      "ACEP train has (123, 9)\n",
      "Cep-II train has (123, 9)\n",
      "ACEP test has (30, 9)\n",
      "Cep-II test has (30, 9)\n",
      "Before OverSampling, counts of label 10: (123,)\n",
      "Before OverSampling, counts of label 12: (123,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (123,)\n",
      "After OverSampling, counts of label 12: (123,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.93333333  0.06666667]\n",
      " [ 0.23333333  0.76666667]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "Normalized confusion matrix\n",
      "[[ 0.9  0.1]\n",
      " [ 0.2  0.8]]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 4 is finished\n",
      "eclipsing_binary_train has (7216, 9)\n",
      "pulsating_train has (8394, 9)\n",
      "rotational_train has (2909, 9)\n",
      "eclipsing_binary_test has (1802, 9)\n",
      "pulsating_test has (2095, 9)\n",
      "rotational_test has (727, 9)\n",
      "Before OverSampling, counts of label 20: (7216,)\n",
      "Before OverSampling, counts of label 21: (2909,)\n",
      "Before OverSampling, counts of label 22: (8394,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (8394,)\n",
      "After OverSampling, counts of label 21: (8394,)\n",
      "After OverSampling, counts of label 22: (8394,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.77857936  0.13207547  0.08934517]\n",
      " [ 0.16781293  0.69876204  0.13342503]\n",
      " [ 0.07350835  0.06921241  0.85727924]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.77857936  0.13706992  0.08435072]\n",
      " [ 0.13755158  0.72764787  0.13480055]\n",
      " [ 0.06205251  0.07303103  0.86491647]]\n",
      "ecl train has (3608, 9)\n",
      "EA_train has (3608, 9)\n",
      "ecl_test has (901, 9)\n",
      "EA_test has (901, 9)\n",
      "Before OverSampling, counts of label 5: (3608,)\n",
      "Before OverSampling, counts of label 6: (3608,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3608,)\n",
      "After OverSampling, counts of label 6: (3608,)\n",
      "{'n_estimators': 550, 'min_samples_split': 5, 'max_features': 'sqrt'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.91342952  0.08657048]\n",
      " [ 0.07103219  0.92896781]]\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 5}\n",
      "Normalized confusion matrix\n",
      "[[ 0.91231964  0.08768036]\n",
      " [ 0.0754717   0.9245283 ]]\n",
      "RR Lyrae train has (7001, 9)\n",
      "LPV train has (1029, 9)\n",
      "Cepheids train has (246, 9)\n",
      "Delta Scuti train has (118, 9)\n",
      "RR_Lyrae_test has (1749, 9)\n",
      "LPV_test has (257, 9)\n",
      "cepheids_test has (60, 9)\n",
      "Delta Scuti test has (29, 9)\n",
      "[23 24 25 26]\n",
      "[7001 1029  118  246]\n",
      "Before OverSampling, counts of label 23: (7001,)\n",
      "Before OverSampling, counts of label 24: (1029,)\n",
      "Before OverSampling, counts of label 25: (118,)\n",
      "Before OverSampling, counts of label 26: (246,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (7001,)\n",
      "After OverSampling, counts of label 24: (7001,)\n",
      "After OverSampling, counts of label 25: (7001,)\n",
      "After OverSampling, counts of label 26: (7001,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.99028016  0.          0.00114351  0.00857633]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.03448276  0.          0.96551724  0.        ]\n",
      " [ 0.11666667  0.03333333  0.          0.85      ]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.99028016  0.          0.00114351  0.00857633]\n",
      " [ 0.          0.99610895  0.          0.00389105]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.08333333  0.03333333  0.          0.88333333]]\n",
      "RRab train has (3460, 9)\n",
      "RRc train has (3002, 9)\n",
      "RRd train has (402, 9)\n",
      "Blazhko train has (137, 9)\n",
      "RRab test has (865, 9)\n",
      "RRc test has (750, 9)\n",
      "RRd test has (100, 9)\n",
      "Blazhko test has (34, 9)\n",
      "Before OverSampling, counts of label 1: (3460,)\n",
      "Before OverSampling, counts of label 2: (3002,)\n",
      "Before OverSampling, counts of label 3: (402,)\n",
      "Before OverSampling, counts of label 4: (137,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3460,)\n",
      "After OverSampling, counts of label 2: (3460,)\n",
      "After OverSampling, counts of label 3: (3460,)\n",
      "After OverSampling, counts of label 4: (3460,)\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.93872832  0.00231214  0.00462428  0.05433526]\n",
      " [ 0.004       0.86533333  0.13066667  0.        ]\n",
      " [ 0.06        0.31        0.61        0.02      ]\n",
      " [ 0.61764706  0.          0.          0.38235294]]\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.91445087  0.00231214  0.00693642  0.07630058]\n",
      " [ 0.004       0.84933333  0.14666667  0.        ]\n",
      " [ 0.06        0.31        0.61        0.02      ]\n",
      " [ 0.52941176  0.          0.02941176  0.44117647]]\n",
      "ACEP train has (123, 9)\n",
      "Cep-II train has (123, 9)\n",
      "ACEP test has (30, 9)\n",
      "Cep-II test has (30, 9)\n",
      "Before OverSampling, counts of label 10: (123,)\n",
      "Before OverSampling, counts of label 12: (123,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (123,)\n",
      "After OverSampling, counts of label 12: (123,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "{'n_estimators': 750, 'min_samples_split': 4, 'max_features': 'auto'}\n",
      "Normalized confusion matrix\n",
      "[[ 0.96666667  0.03333333]\n",
      " [ 0.23333333  0.76666667]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "{'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 9}\n",
      "Normalized confusion matrix\n",
      "[[ 0.96666667  0.03333333]\n",
      " [ 0.2         0.8       ]]\n",
      "--------------------------------------------------\n",
      "Feature Extraction for Split 5 is finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105717590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137b67350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1370a1d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138215d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114ca0210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13757c890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13716b250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a46ed90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a47b590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ae4bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115084490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1056ca9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1365e0c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114cbb4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1155e84d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1057175d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137a70c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137237b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ab52f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ab631d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c532910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13742cf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138889c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13741c050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137837410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13929a310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a695dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x137520610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117d0a650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c72ff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115c6e510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1056ca990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13b9e4f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ec95fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114953ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114958590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11837ab90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151fdd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13edf1310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11add03d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1181f6290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117089950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a1a1210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120b792d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a19ef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117cb52d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13ddf3850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1220d06d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13edf10d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1183d9090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X   = features.iloc[:,0:nFeatures]\n",
    "y   = features.True_class_labels\n",
    "skf       = StratifiedKFold(n_splits=n_splits, shuffle=True) \n",
    "split_num = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    if data_preparation: \n",
    "        X_training      = features.iloc[train_index]\n",
    "        X_testing       = features.iloc[test_index]\n",
    "\n",
    "        \n",
    "        RRab_train        = stars_label(X_training, true_class_1)\n",
    "        RRc_train         = stars_label(X_training, true_class_2) \n",
    "        RRd_train         = stars_label(X_training, true_class_3)\n",
    "        blazhko_train     = stars_label(X_training, true_class_4)\n",
    "        contact_Bi_train  = stars_label(X_training, true_class_5)\n",
    "        semi_det_Bi_train = stars_label(X_training, true_class_6)\n",
    "        rot_train         = stars_label(X_training, true_class_7)\n",
    "        LPV_train         = stars_label(X_training, true_class_8)\n",
    "        delta_scuti_train = stars_label(X_training, true_class_9)\n",
    "        ACEP_train        = stars_label(X_training, true_class_10)\n",
    "        cep_ii_train      = stars_label(X_training, true_class_12)\n",
    "\n",
    "        RRab_test        = stars_label(X_testing, true_class_1)\n",
    "        RRc_test         = stars_label(X_testing, true_class_2) \n",
    "        RRd_test         = stars_label(X_testing, true_class_3)\n",
    "        blazhko_test     = stars_label(X_testing, true_class_4)\n",
    "        contact_Bi_test  = stars_label(X_testing, true_class_5)\n",
    "        semi_det_Bi_test = stars_label(X_testing, true_class_6)\n",
    "        rot_test         = stars_label(X_testing, true_class_7)\n",
    "        LPV_test         = stars_label(X_testing, true_class_8)\n",
    "        delta_scuti_test = stars_label(X_testing, true_class_9)\n",
    "        ACEP_test        = stars_label(X_testing, true_class_10)\n",
    "        cep_ii_test      = stars_label(X_testing, true_class_12)\n",
    "        \n",
    "        '-----------------------------------------------------------------------------'\n",
    "                                        # FIRST LAYER\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_FL, testing_data_FL, y_FL_training_counts = first_layer()\n",
    "        \n",
    "        X_train_FL, y_train_FL, X_test_FL, y_test_FL = smote_augmentation(training_data_FL, testing_data_FL)\n",
    "        \n",
    "        # Random Forest Classifier    \n",
    "        if multi_class:\n",
    "            classes_types_FL = ['Eclipsing','Rotational','Pulsating']\n",
    "            types_FL         ='Type_FL_'+str(split_num)\n",
    "            nClasses_FL      = len(classes_types_FL)\n",
    "\n",
    "        else:\n",
    "            classes_types_FL = ['Eclipsing','Rotational']\n",
    "            types_FL         ='Type_Binary_Split_'+str(split_num)\n",
    "            nClasses_FL      = 2\n",
    "\n",
    "        opt_rf_FL, fit_model_rf_FL = analysis_rf(X_train_FL, y_train_FL, types_FL, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "        print(opt_rf_FL)   \n",
    "        ypred_rf_FL, accuracy_rf_FL, MCC_rf_FL, conf_mat_rf_FL = final_prediction(fit_model_rf_FL,X_train_FL, y_train_FL, X_test_FL, y_test_FL, classes_types_FL, types_FL, nClasses_FL,load_model)\n",
    "\n",
    "        # XGBoost Classifier   \n",
    "        opt_xgb_FL, fit_model_xgb_FL = analysis_XGB(X_train_FL, y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "        print(opt_xgb_FL)    \n",
    "        ypred_xgb_FL, accuracy_xgb_FL, MCC_xgb_FL, conf_mat_xgb_FL = final_prediction_XGB(fit_model_xgb_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, classes_types_FL, types_FL, nClasses_FL, load_model) \n",
    "\n",
    "        acc_rf_FL.append(accuracy_rf_FL)\n",
    "        mcc_rf_FL.append(MCC_rf_FL)\n",
    "        acc_xgb_FL.append(accuracy_xgb_FL)\n",
    "        mcc_xgb_FL.append(MCC_xgb_FL)\n",
    "\n",
    "        '-------------------------------------------------------------------------------'\n",
    "                                # SECOND LAYER ECLIPSING BINARY\n",
    "        '-------------------------------------------------------------------------------'\n",
    "        training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts = second_layer_EB()\n",
    "        X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB,= smote_augmentation(training_data_SL_EB, testing_data_SL_EB)\n",
    "                \n",
    "        # Random Forest Classifier    \n",
    "\n",
    "        classes_types_SL_EB = ['Ecl','EA']\n",
    "        types_SL_EB         ='Type_SL_Ecl_EA_'+str(split_num)\n",
    "        nClasses_SL_EB      = 2\n",
    "\n",
    "        opt_rf_SL_EB, fit_model_rf_SL_EB = analysis_rf(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "        print(opt_rf_FL)   \n",
    "        ypred_rf_SL_EB, accuracy_rf_SL_EB, MCC_rf_SL_EB, conf_mat_rf_SL_EB = final_prediction(fit_model_rf_SL_EB,X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB,load_model)\n",
    "\n",
    "        # XGBoost Classifier   \n",
    "        opt_xgb_SL_EB, fit_model_xgb_SL_EB = analysis_XGB(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training\n",
    "        print(opt_xgb_SL_EB)    \n",
    "        ypred_xgb_SL_EB, accuracy_xgb_SL_EB, MCC_xgb_SL_EB, conf_mat_xgb_SL_EB = final_prediction_XGB(fit_model_xgb_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "\n",
    "        acc_rf_SL_EB.append(accuracy_rf_SL_EB)\n",
    "        mcc_rf_SL_EB.append(MCC_rf_SL_EB)\n",
    "        acc_xgb_SL_EB.append(accuracy_xgb_SL_EB)\n",
    "        mcc_xgb_SL_EB.append(MCC_xgb_SL_EB)\n",
    "\n",
    "        '-----------------------------------------------------------------------------'\n",
    "                            # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts = second_layer_RLCD()\n",
    "        X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD = smote_augmentation(training_data_SL_RLCD,testing_data_SL_RLCD)\n",
    "\n",
    "\n",
    "        # Random Forest Classifier    \n",
    "        classes_types_SL_RLCD = ['RR Lyrae','LPV', 'Cepheids', '$\\delta$-Scuti']\n",
    "        types_SL_RLCD         ='Type_SL_RLCD_'+str(split_num)\n",
    "        nClasses_SL_RLCD      = len(classes_types_SL_RLCD)\n",
    "\n",
    "        opt_rf_SL_RLCD, fit_model_rf_SL_RLCD = analysis_rf(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "        print(opt_rf_SL_RLCD)   \n",
    "        ypred_rf_SL_RLCD, accuracy_rf_SL_RLCD, MCC_rf_SL_RLCD, conf_mat_rf_SL_RLCD = final_prediction(fit_model_rf_SL_RLCD,X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD,load_model)\n",
    "\n",
    "        # XGBoost Classifier   \n",
    "        opt_xgb_SL_RLCD, fit_model_xgb_SL_RLCD = analysis_XGB(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented when no training\n",
    "        print(opt_xgb_SL_RLCD)    \n",
    "        ypred_xgb_SL_RLCD, accuracy_xgb_SL_RLCD, MCC_xgb_SL_RLCD, conf_mat_xgb_SL_RLCD = final_prediction_XGB(fit_model_xgb_SL_RLCD, X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD, load_model) \n",
    "\n",
    "        acc_rf_SL_RLCD.append(accuracy_rf_SL_RLCD)\n",
    "        mcc_rf_SL_RLCD.append(MCC_rf_SL_RLCD)\n",
    "        acc_xgb_SL_RLCD.append(accuracy_xgb_SL_RLCD)\n",
    "        mcc_xgb_SL_RLCD.append(MCC_xgb_SL_RLCD)\n",
    "\n",
    "        '-----------------------------------------------------------------------------'\n",
    "                        # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts = third_layer_RRLyrae()\n",
    "        \n",
    "        X_train_TL_RL, y_train_TL_RL,X_test_TL_RL, y_test_TL_RL = smote_augmentation(training_data_TL_RRLyrae,testing_data_TL_RRLyrae)\n",
    "\n",
    "        \n",
    "        # Random Forest Classifier    \n",
    "        classes_types_TL_RL = ['RRab', 'RRc', 'RRd', \"Blazhko\"]\n",
    "        types_TL_RL         ='Type_TL_RRLyrae_'+str(split_num)\n",
    "        nClasses_TL_RL      = len(classes_types_TL_RL)\n",
    "\n",
    "        opt_rf_TL_RL, fit_model_rf_TL_RL = analysis_rf(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "        print(opt_rf_TL_RL)   \n",
    "        ypred_rf_TL_RL, accuracy_rf_TL_RL, MCC_rf_TL_RL, conf_mat_rf_TL_RL = final_prediction(fit_model_rf_TL_RL,X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL,load_model)\n",
    "\n",
    "        # XGBoost Classifier   \n",
    "        opt_xgb_TL_RL, fit_model_xgb_TL_RL = analysis_XGB(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training\n",
    "        print(opt_xgb_TL_RL)    \n",
    "        ypred_xgb_TL_RL, accuracy_xgb_TL_RL, MCC_xgb_TL_RL, conf_mat_xgb_TL_RL = final_prediction_XGB(fit_model_xgb_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "\n",
    "        acc_rf_TL_RL.append(accuracy_rf_TL_RL)\n",
    "        mcc_rf_TL_RL.append(MCC_rf_TL_RL)\n",
    "        acc_xgb_TL_RL.append(accuracy_xgb_TL_RL)\n",
    "        mcc_xgb_TL_RL.append(MCC_xgb_TL_RL)\n",
    "\n",
    "\n",
    "        '-----------------------------------------------------------------------------'\n",
    "                                # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "        '-----------------------------------------------------------------------------'\n",
    "        training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts = third_layer_Cepheids()\n",
    "        \n",
    "        X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep = smote_augmentation(training_data_TL_cep,testing_data_TL_cep)\n",
    "\n",
    "        # Random Forest Classifier    \n",
    "\n",
    "        classes_types_TL_Cep = ['ACEP','CEP-II']\n",
    "        types_TL_Cep         ='Type_TL_Cepheids_'+str(split_num)\n",
    "        nClasses_TL_Cep      = 2\n",
    "\n",
    "        opt_rf_TL_Cep, fit_model_rf_TL_Cep = analysis_rf(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "        print(opt_rf_TL_Cep)   \n",
    "        ypred_rf_TL_Cep, accuracy_rf_TL_Cep, MCC_rf_TL_Cep, conf_mat_rf_TL_Cep = final_prediction(fit_model_rf_TL_Cep,X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep,load_model)\n",
    "\n",
    "        # XGBoost Classifier   \n",
    "        opt_xgb_TL_Cep, fit_model_xgb_TL_Cep = analysis_XGB(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training\n",
    "        print(opt_xgb_TL_Cep)    \n",
    "        ypred_xgb_TL_Cep, accuracy_xgb_TL_Cep, MCC_xgb_TL_Cep, conf_mat_xgb_TL_Cep = final_prediction_XGB(fit_model_xgb_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "\n",
    "        acc_rf_TL_Cep.append(accuracy_rf_TL_Cep)\n",
    "        mcc_rf_TL_Cep.append(MCC_rf_TL_Cep)\n",
    "        acc_xgb_TL_Cep.append(accuracy_xgb_TL_Cep)\n",
    "        mcc_xgb_TL_Cep.append(MCC_xgb_TL_Cep)\n",
    "   \n",
    "        \n",
    "    print('-'*50)\n",
    "    print('Feature Extraction for Split {} is finished'.format(split_num))  \n",
    "    split_num += 1\n",
    "    \n",
    "metrics = open(\"./hierarchical-results_SMOTE/metrics.txt\", 'w')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf_FL)*100,np.std(acc_rf_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf_FL)*100,np.std(mcc_rf_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf_SL_EB)*100,np.std(acc_rf_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf_SL_EB)*100,np.std(mcc_rf_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf_SL_RLCD)*100,np.std(acc_rf_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf_SL_RLCD)*100,np.std(mcc_rf_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf_TL_RL)*100,np.std(acc_rf_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf_TL_RL)*100,np.std(mcc_rf_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf_TL_Cep)*100,np.std(acc_rf_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf_TL_Cep)*100,np.std(mcc_rf_TL_Cep)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_FL)*100,np.std(acc_xgb_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_FL)*100,np.std(mcc_xgb_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_EB)*100,np.std(acc_xgb_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_EB)*100,np.std(mcc_xgb_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_RLCD)*100,np.std(acc_xgb_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_RLCD)*100,np.std(mcc_xgb_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_RL)*100,np.std(acc_xgb_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_RL)*100,np.std(mcc_xgb_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_Cep)*100,np.std(acc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_Cep)*100,np.std(mcc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
