{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Good Code for Hierarchical Classification\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import FATS\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "http://rikunert.com/SMOTE_explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE,ADASYN\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "# The different classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stars_label(data, label):\n",
    "    '''Set variable names to specific class label'''\n",
    "    stars = data[data.True_class_labels == label]\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Hierarchical Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_layer():\n",
    "    '''\n",
    "    We define first layer of the hierarchical tree. The first layer consists of Eclipsing Binaries, Rotational,\n",
    "    and Pulsating \n",
    "    '''\n",
    "    \n",
    "    # First Layer\n",
    "    eclipsing_binary_train       = pd.concat([contact_Bi_train, semi_det_Bi_train], axis=0)\n",
    "    eclipsing_binary_train_class = np.full(len(eclipsing_binary_train), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_train       = rot_train\n",
    "    rotational_train_class = np.full(len(rotational_train),rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_train       = pd.concat([RRab_train, RRc_train, RRd_train, blazhko_train, LPV_train, delta_scuti_train, ACEP_train, cep_ii_train] ,axis=0)\n",
    "    pulsating_train_class = np.full(len(pulsating_train), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_train has {}\".format(eclipsing_binary_train.shape))\n",
    "    print(\"pulsating_train has {}\".format(pulsating_train.shape))\n",
    "    print(\"rotational_train has {}\".format(rotational_train.shape))\n",
    "\n",
    "    eclipsing_binary_test       = pd.concat([contact_Bi_test, semi_det_Bi_test], axis=0)\n",
    "    eclipsing_binary_test_class = np.full(len(eclipsing_binary_test), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_test       = rot_test\n",
    "    rotational_test_class = np.full(len(rotational_test), rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_test       = pd.concat([RRab_test, RRc_test, RRd_test, blazhko_test, LPV_test, delta_scuti_test, ACEP_test, cep_ii_test] ,axis=0)\n",
    "    pulsating_test_class = np.full(len(pulsating_test), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_test has {}\".format(eclipsing_binary_test.shape))\n",
    "    print(\"pulsating_test has {}\".format(pulsating_test.shape))\n",
    "    print(\"rotational_test has {}\".format(rotational_test.shape))\n",
    "    \n",
    "    first_layer_train       = pd.concat([eclipsing_binary_train, rotational_train, pulsating_train], axis=0)\n",
    "    first_layer_train_class = np.concatenate((eclipsing_binary_train_class, rotational_train_class, pulsating_train_class), axis=0)\n",
    "    training_data_FL        = pd.DataFrame(first_layer_train)\n",
    "    training_data_FL['New_label'] = first_layer_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    first_layer_test       = pd.concat([eclipsing_binary_test, rotational_test, pulsating_test], axis=0)\n",
    "    first_layer_test_class = np.concatenate((eclipsing_binary_test_class, rotational_test_class, pulsating_test_class), axis=0)\n",
    "    testing_data_FL        = pd.DataFrame(first_layer_test)\n",
    "    testing_data_FL['New_label'] = first_layer_test_class\n",
    "    \n",
    "    y_FL_training, y_FL_training_counts = np.unique(first_layer_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_FL, testing_data_FL, y_FL_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for first Branch: Eclipsing Binaries (Ecl & EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_layer_EB():\n",
    "    \n",
    "    # Second Layer Eclipsing Binary    \n",
    "    ecl_train = contact_Bi_train\n",
    "    ecl_train_class = np.full(len(ecl_train), true_class_5, dtype=int)\n",
    "\n",
    "    EA_train       = semi_det_Bi_train\n",
    "    EA_train_class = np.full(len(EA_train),true_class_6, dtype=int)\n",
    " \n",
    "    print(\"ecl train has {}\".format(ecl_train.shape))\n",
    "    print(\"EA_train has {}\".format(EA_train.shape))\n",
    "\n",
    "    ecl_test       = contact_Bi_test\n",
    "    ecl_test_class = np.full(len(ecl_test), true_class_5, dtype=int)\n",
    "\n",
    "    EA_test       = semi_det_Bi_test\n",
    "    EA_test_class = np.full(len(EA_test), true_class_6, dtype=int)\n",
    "\n",
    "    print(\"ecl_test has {}\".format(ecl_test.shape))\n",
    "    print(\"EA_test has {}\".format(EA_test.shape))\n",
    "\n",
    "    \n",
    "    second_layer_EB_train       = pd.concat([ecl_train, EA_train], axis=0)\n",
    "    second_layer_EB_train_class = np.concatenate((ecl_train_class,EA_train_class), axis=0)\n",
    "    training_data_SL_EB         = pd.DataFrame(second_layer_EB_train)\n",
    "    training_data_SL_EB['New_label'] = second_layer_EB_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_EB_test       = pd.concat([ecl_test, EA_test], axis=0)\n",
    "    second_layer_EB_test_class = np.concatenate((ecl_test_class, EA_test_class), axis=0)\n",
    "    testing_data_SL_EB         = pd.DataFrame(second_layer_EB_test)\n",
    "    testing_data_SL_EB['New_label'] = second_layer_EB_test_class\n",
    "    \n",
    "    y_SL_EB_training, y_SL_EB_training_counts = np.unique(second_layer_EB_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for 2nd Branch: RLCD\n",
    "### RR Lyrae, LPV, Cepheid and $\\delta$-Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2 RR Lyrae, LPV, Cepheid, Delta-Scuti\n",
    "def second_layer_RLCD():\n",
    "    \n",
    "    # First Layer\n",
    "    RR_Lyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    RR_Lyrae_train_class = np.full(len(RR_Lyrae_train), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_train_class = np.full(len(LPV_train),LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_train       = pd.concat([ACEP_train,cep_ii_train] ,axis=0)\n",
    "    cepheids_train_class = np.full(len(cepheids_train), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_train       = delta_scuti_train\n",
    "    ds_train_class = np.full(len(ds_train), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR Lyrae train has {}\".format(RR_Lyrae_train.shape))\n",
    "    print(\"LPV train has {}\".format(LPV_train.shape))\n",
    "    print(\"Cepheids train has {}\".format(cepheids_train.shape))\n",
    "    print(\"Delta Scuti train has {}\".format(ds_train.shape))\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_test_class = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_test       = delta_scuti_test\n",
    "    ds_test_class = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR_Lyrae_test has {}\".format(RR_Lyrae_test.shape))\n",
    "    print(\"LPV_test has {}\".format(LPV_test.shape))\n",
    "    print(\"cepheids_test has {}\".format(cepheids_test.shape))\n",
    "    print(\"Delta Scuti test has {}\".format(ds_test.shape))\n",
    "    \n",
    "    second_layer_RLCD_train       = pd.concat([RR_Lyrae_train,LPV_train,cepheids_train,ds_train], axis=0)\n",
    "    second_layer_RLCD_train_class = np.concatenate((RR_Lyrae_train_class,LPV_train_class,cepheids_train_class,ds_train_class), axis=0)\n",
    "    training_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_train)\n",
    "    training_data_SL_RLCD['New_label'] = second_layer_RLCD_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    y_SL_RLCD_training, y_SL_RLCD_training_counts = np.unique(second_layer_RLCD_train_class, return_counts=True)\n",
    "\n",
    "    print(y_SL_RLCD_training)\n",
    "    print(y_SL_RLCD_training_counts)\n",
    "    return training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for first Branch: RRLyrae\n",
    "### RRab, RRc, RRd, and Blazhko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_RRLyrae():\n",
    "    \n",
    "    # Third Layer\n",
    "    RRab_train_class    = np.full(len(RRab_train), true_class_1, dtype=int)\n",
    "    RRc_train_class     = np.full(len(RRc_train), true_class_2, dtype=int)\n",
    "    RRd_train_class     = np.full(len(RRd_train), true_class_3, dtype=int)\n",
    "    blazhko_train_class = np.full(len(blazhko_train), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab train has {}\".format(RRab_train.shape))\n",
    "    print(\"RRc train has {}\".format(RRc_train.shape))\n",
    "    print(\"RRd train has {}\".format(RRd_train.shape))\n",
    "    print(\"Blazhko train has {}\".format(blazhko_train.shape))\n",
    "    \n",
    "    RRab_test_class    = np.full(len(RRab_test), true_class_1, dtype=int)\n",
    "    RRc_test_class     = np.full(len(RRc_test), true_class_2, dtype=int)\n",
    "    RRd_test_class     = np.full(len(RRd_test), true_class_3, dtype=int)\n",
    "    blazhko_test_class = np.full(len(blazhko_test), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab test has {}\".format(RRab_test.shape))\n",
    "    print(\"RRc test has {}\".format(RRc_test.shape))\n",
    "    print(\"RRd test has {}\".format(RRd_test.shape))\n",
    "    print(\"Blazhko test has {}\".format(blazhko_test.shape))\n",
    "\n",
    "    \n",
    "    third_layer_RRLyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    third_layer_RRLyrae_train_class = np.concatenate((RRab_train_class,RRc_train_class,RRd_train_class,blazhko_train_class), axis=0)\n",
    "    training_data_TL_RRLyrae        = pd.DataFrame(third_layer_RRLyrae_train)\n",
    "    training_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_RRLyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    third_layer_RRLyrae_test_class = np.concatenate((RRab_test_class,RRc_test_class,RRd_test_class,blazhko_test_class), axis=0)\n",
    "    testing_data_TL_RRLyrae         = pd.DataFrame(third_layer_RRLyrae_test)\n",
    "    testing_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_test_class\n",
    "    \n",
    "    y_TL_RRLyrae_training, y_TL_RRLyrae_training_counts = np.unique(third_layer_RRLyrae_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for 2nd Branch: Cepheids\n",
    "### ACEP and Cep-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_Cepheids():\n",
    "    \n",
    "    # Third Layer\n",
    "    ACEP_train_class   = np.full(len(ACEP_train), true_class_10, dtype=int)\n",
    "    cep_ii_train_class = np.full(len(cep_ii_train), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP train has {}\".format(ACEP_train.shape))\n",
    "    print(\"Cep-II train has {}\".format(cep_ii_train.shape))\n",
    "\n",
    "\n",
    "    ACEP_test_class   = np.full(len(ACEP_test), true_class_10, dtype=int)\n",
    "    cep_ii_test_class = np.full(len(cep_ii_test), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP test has {}\".format(ACEP_test.shape))\n",
    "    print(\"Cep-II test has {}\".format(cep_ii_test.shape))\n",
    "    \n",
    "    third_layer_cep_train       = pd.concat([ACEP_train,cep_ii_train], axis=0)\n",
    "    third_layer_cep_train_class = np.concatenate((ACEP_train_class,cep_ii_train_class), axis=0)\n",
    "    training_data_TL_cep        = pd.DataFrame(third_layer_cep_train)\n",
    "    training_data_TL_cep['New_label'] = third_layer_cep_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_cep_test       = pd.concat([ACEP_test,cep_ii_test], axis=0)\n",
    "    third_layer_cep_test_class = np.concatenate((ACEP_test_class,cep_ii_test_class), axis=0)\n",
    "    testing_data_TL_cep        = pd.DataFrame(third_layer_cep_test)\n",
    "    testing_data_TL_cep['New_label'] = third_layer_cep_test_class\n",
    "    \n",
    "    y_TL_cep_training, y_TL_cep_training_counts = np.unique(third_layer_cep_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test,label):\n",
    "    scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "    X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "    y_train_label         = x_train.New_label\n",
    "    filename_train        = x_train.File_Name\n",
    "\n",
    "    X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "    y_test_label         = x_test[label]\n",
    "    filename_test        = x_test.File_Name\n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(X_train,y_train,classifer, param_grid, n_iter, cv, filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    "    balance_accuracy = balanced_accuracy_score(y_test, ypred)\n",
    "\n",
    "     \n",
    "    misclassified     = np.where(y_test != ypred)[0]\n",
    " \n",
    "    \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write(\"Balanced Accuracy: \"    + \"%f\" % float(balance_accuracy)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report using imabalanced metrics\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report_imbalanced(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()\n",
    "        \n",
    "    return ypred, balance_accuracy, MCC, conf_mat, misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_misclassification(misclassified,y_test,test_data,ypred, save_dir=r'c:\\data\\np.txt'):\n",
    "    test_data['Prediction'] = ypred\n",
    "    new_DF = test_data.drop(test_data.index[misclassified]) # This dataset is the test set after removing the misclassification which are used in the next layer   \n",
    "    misclassified_data = test_data.iloc[misclassified] # Dataframe to store all misclassification\n",
    "    misclassified_data.to_csv(save_dir, sep=' ',index=None)\n",
    "    print('Test set has shape {}'.format(test_data.shape))\n",
    "    print('Misclassified data has shape {}'.format(misclassified_data.shape))\n",
    "    print('New test set has shape {}'.format(new_DF.shape))\n",
    "    return misclassified_data, new_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=18)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf',bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smote_augmentation(training,testing,label,aug_tech='ADASYN'):\n",
    "    X_train_normalisation, y_train_np, filename_train, X_test_normalisation,\\\n",
    "    y_test_np, filename_test = normalisation(training,testing,label) \n",
    "    \n",
    "    \n",
    "    y_label_bf = np.unique(y_train_np)\n",
    "    if augmentation:\n",
    "        for i in range(len(y_label_bf)):\n",
    "            print(\"Before OverSampling, counts of label {}: {}\".format(y_label_bf[i],(y_train_np[y_train_np==y_label_bf[i]]).shape))\n",
    "\n",
    "        if (aug_tech == 'ADASYN'):\n",
    "            ada          = ADASYN(ratio ='all')#\n",
    "            X_train_aug, y_train_aug = ada.fit_sample(X_train_normalisation, y_train_np.ravel())\n",
    "            data_1 = pd.DataFrame(X_train_aug)\n",
    "            data_1['True_class_labels'] = y_train_aug\n",
    "            X_train_norm = data_1.iloc[:,0:nFeatures]\n",
    "            y_train_norm = data_1.iloc[:,nFeatures]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "\n",
    "            # sm = SMOTE(random_state=2, ratio = 1.0,kind='svm')\n",
    "            sm = SMOTE(ratio = 'all')\n",
    "            X_train_aug, y_train_aug = sm.fit_sample(X_train_normalisation, y_train_np.ravel())\n",
    "            data_1 = pd.DataFrame(X_train_aug)\n",
    "            data_1['True_class_labels'] = y_train_aug\n",
    "            X_train_norm = data_1.iloc[:,0:nFeatures]\n",
    "            y_train_norm = data_1.iloc[:,nFeatures]\n",
    "\n",
    "\n",
    "        y_label_af = np.unique(y_train_norm)\n",
    "        print('-'*70)\n",
    "        for j in range(len(y_label_af)):\n",
    "                print(\"After OverSampling, counts of label {}: {}\".format(y_label_af[j],y_train_norm.loc[y_train_norm==y_label_af[j]].shape))\n",
    "\n",
    "\n",
    "        X_train = X_train_norm   \n",
    "        y_train = y_train_norm\n",
    "        y_test  = y_test_np\n",
    "        X_test  = X_test_normalisation\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nFeatures = 7\n",
    "\n",
    "# The directory to save the files\n",
    "plots_dir                 = './hierarchical-results_SMOTE/plots/'\n",
    "results_dir               = './hierarchical-results_SMOTE/results/'\n",
    "misclassify_dir           = r'./hierarchical-results_SMOTE/results/Misclassification_'\n",
    "\n",
    "\n",
    "eclipsing_label = 20;rotational_label = 21;pulsating_label = 22;RR_Lyrae_label = 23; LPV_label = 24;\\\n",
    "delta_scuti_label = 25; cepheids_label   = 26\n",
    "\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "n_splits         = 5\n",
    "data_preparation = True\n",
    "multi_class      = True\n",
    "augmentation     = True\n",
    "save_model       = False\n",
    "load_model       = False\n",
    "\n",
    "\n",
    "\n",
    "acc_xgb_FL = [];mcc_xgb_FL = [];\\\n",
    "acc_xgb_SL_EB = [];mcc_xgb_SL_EB = [];\\\n",
    "acc_xgb_SL_RLCD = [];mcc_xgb_SL_RLCD = [];\\\n",
    "acc_xgb_TL_RL = [];mcc_xgb_TL_RL = [];\\\n",
    "acc_xgb_TL_Cep = [];mcc_xgb_TL_Cep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set contains 16195 samples\n",
      "The test set contains 21242 samples\n"
     ]
    }
   ],
   "source": [
    "train_dir = './data/training_set_features/'\n",
    "test_dir  = './data/test_set_features/'\n",
    "\n",
    "# Load features for the different variable stars\n",
    "features_1_tr  = pd.read_csv(train_dir+'Type'+str(true_class_1)+'_features.csv')\n",
    "features_2_tr  = pd.read_csv(train_dir+'Type'+str(true_class_2)+'_features.csv')\n",
    "features_3_tr  = pd.read_csv(train_dir+'Type'+str(true_class_3)+'_features.csv')\n",
    "features_4_tr  = pd.read_csv(train_dir+'Type'+str(true_class_4)+'_features.csv')\n",
    "features_5_tr  = pd.read_csv(train_dir+'Type'+str(true_class_5)+'_features.csv')\n",
    "features_6_tr  = pd.read_csv(train_dir+'Type'+str(true_class_6)+'_features.csv')\n",
    "features_7_tr  = pd.read_csv(train_dir+'Type'+str(true_class_7)+'_features.csv')\n",
    "features_8_tr  = pd.read_csv(train_dir+'Type'+str(true_class_8)+'_features.csv')\n",
    "features_9_tr  = pd.read_csv(train_dir+'Type'+str(true_class_9)+'_features.csv')\n",
    "features_10_tr = pd.read_csv(train_dir+'Type'+str(true_class_10)+'_features.csv')\n",
    "features_12_tr = pd.read_csv(train_dir+'Type'+str(true_class_12)+'_features.csv')\n",
    "\n",
    "features_1_te  = pd.read_csv(test_dir+'Type'+str(true_class_1)+'_features.csv')\n",
    "features_2_te  = pd.read_csv(test_dir+'Type'+str(true_class_2)+'_features.csv')\n",
    "features_3_te  = pd.read_csv(test_dir+'Type'+str(true_class_3)+'_features.csv')\n",
    "features_4_te  = pd.read_csv(test_dir+'Type'+str(true_class_4)+'_features.csv')\n",
    "features_5_te  = pd.read_csv(test_dir+'Type'+str(true_class_5)+'_features.csv')\n",
    "features_6_te  = pd.read_csv(test_dir+'Type'+str(true_class_6)+'_features.csv')\n",
    "features_7_te  = pd.read_csv(test_dir+'Type'+str(true_class_7)+'_features.csv')\n",
    "features_8_te  = pd.read_csv(test_dir+'Type'+str(true_class_8)+'_features.csv')\n",
    "features_9_te  = pd.read_csv(test_dir+'Type'+str(true_class_9)+'_features.csv')\n",
    "features_10_te = pd.read_csv(test_dir+'Type'+str(true_class_10)+'_features.csv')\n",
    "features_12_te = pd.read_csv(test_dir+'Type'+str(true_class_12)+'_features.csv')\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_training = shuffle(pd.concat([features_1_tr,features_2_tr,features_3_tr,features_4_tr,features_5_tr,\\\n",
    "            features_6_tr,features_7_tr,features_8_tr,features_9_tr,features_10_tr,features_12_tr],\\\n",
    "            ignore_index=True))\n",
    "\n",
    "\n",
    "X_testing = shuffle(pd.concat([features_1_te,features_2_te,features_3_te,features_4_te,features_5_te,\\\n",
    "            features_6_te,features_7_te,features_8_te,features_9_te,features_10_te,features_12_te],\\\n",
    "            ignore_index=True))\n",
    "\n",
    "print('The training set contains {} samples'.format(X_training.shape[0]))\n",
    "print('The test set contains {} samples'.format(X_testing.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>RA</th>\n",
       "      <th>Dec</th>\n",
       "      <th>Period</th>\n",
       "      <th>V_CSS</th>\n",
       "      <th>Npts</th>\n",
       "      <th>V_amp</th>\n",
       "      <th>Type</th>\n",
       "      <th>Prior_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3039001008411</td>\n",
       "      <td>0.10758</td>\n",
       "      <td>-39.61419</td>\n",
       "      <td>0.514380</td>\n",
       "      <td>14.709183</td>\n",
       "      <td>230</td>\n",
       "      <td>0.260621</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3041001010523</td>\n",
       "      <td>0.13228</td>\n",
       "      <td>-41.48192</td>\n",
       "      <td>0.589281</td>\n",
       "      <td>19.164993</td>\n",
       "      <td>145</td>\n",
       "      <td>0.829397</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025001011039</td>\n",
       "      <td>0.15796</td>\n",
       "      <td>-25.18378</td>\n",
       "      <td>0.616695</td>\n",
       "      <td>17.559777</td>\n",
       "      <td>147</td>\n",
       "      <td>0.485934</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3043001016067</td>\n",
       "      <td>0.18672</td>\n",
       "      <td>-43.13301</td>\n",
       "      <td>0.333486</td>\n",
       "      <td>15.206383</td>\n",
       "      <td>212</td>\n",
       "      <td>0.371547</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3029001015115</td>\n",
       "      <td>0.20548</td>\n",
       "      <td>-28.81702</td>\n",
       "      <td>0.583042</td>\n",
       "      <td>14.818986</td>\n",
       "      <td>196</td>\n",
       "      <td>0.832350</td>\n",
       "      <td>1</td>\n",
       "      <td>TZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_Name       RA       Dec    Period      V_CSS  Npts     V_amp  \\\n",
       "0  3039001008411  0.10758 -39.61419  0.514380  14.709183   230  0.260621   \n",
       "1  3041001010523  0.13228 -41.48192  0.589281  19.164993   145  0.829397   \n",
       "2  3025001011039  0.15796 -25.18378  0.616695  17.559777   147  0.485934   \n",
       "3  3043001016067  0.18672 -43.13301  0.333486  15.206383   212  0.371547   \n",
       "4  3029001015115  0.20548 -28.81702  0.583042  14.818986   196  0.832350   \n",
       "\n",
       "   Type Prior_ID  \n",
       "0     5      NaN  \n",
       "1     1      NaN  \n",
       "2     1      NaN  \n",
       "3     2      NaN  \n",
       "4     1       TZ  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_data = pd.read_csv('../SSS_Per_Tab.txt', delim_whitespace=True,\\\n",
    "names=[\"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\"])\n",
    "\n",
    "period_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "periods = period_data[['File_Name', 'Period']]\n",
    "X_training['File_Name'] = X_training['File_Name'].astype(int)\n",
    "X_testing['File_Name'] = X_testing['File_Name'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Period</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>True_class_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>0.252408</td>\n",
       "      <td>15.127189</td>\n",
       "      <td>0.214170</td>\n",
       "      <td>0.386290</td>\n",
       "      <td>0.426550</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.584049</td>\n",
       "      <td>3021118101049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>0.326478</td>\n",
       "      <td>14.899610</td>\n",
       "      <td>0.051581</td>\n",
       "      <td>-0.575099</td>\n",
       "      <td>0.091425</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.490288</td>\n",
       "      <td>3021106012619</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>-0.192648</td>\n",
       "      <td>17.554317</td>\n",
       "      <td>0.206104</td>\n",
       "      <td>-0.476685</td>\n",
       "      <td>0.381700</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>3053087050522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>0.009453</td>\n",
       "      <td>15.452825</td>\n",
       "      <td>0.141750</td>\n",
       "      <td>-1.380833</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.009173</td>\n",
       "      <td>0.323825</td>\n",
       "      <td>3039081022775</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10747</th>\n",
       "      <td>0.269189</td>\n",
       "      <td>15.408603</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>0.276853</td>\n",
       "      <td>0.091550</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.836064</td>\n",
       "      <td>3035072026244</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2         3         4         5    Period  \\\n",
       "1474   0.252408  15.127189  0.214170  0.386290  0.426550  0.014158  0.584049   \n",
       "6197   0.326478  14.899610  0.051581 -0.575099  0.091425  0.003462  0.490288   \n",
       "590   -0.192648  17.554317  0.206104 -0.476685  0.381700  0.011741  0.632308   \n",
       "3934   0.009453  15.452825  0.141750 -1.380833  0.221600  0.009173  0.323825   \n",
       "10747  0.269189  15.408603  0.046285  0.276853  0.091550  0.003004  0.836064   \n",
       "\n",
       "           File_Name  True_class_labels  \n",
       "1474   3021118101049                  1  \n",
       "6197   3021106012619                  5  \n",
       "590    3053087050522                  1  \n",
       "3934   3039081022775                  2  \n",
       "10747  3035072026244                  6  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = X_training.join(periods.set_index('File_Name'), on='File_Name')\n",
    "training_set = training_set[['0','1','2','3','4','5','Period', 'File_Name', 'True_class_labels']]\n",
    "training_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>Period</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>True_class_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12017</th>\n",
       "      <td>0.380879</td>\n",
       "      <td>13.461285</td>\n",
       "      <td>0.096354</td>\n",
       "      <td>-0.869283</td>\n",
       "      <td>0.16640</td>\n",
       "      <td>0.007158</td>\n",
       "      <td>0.296013</td>\n",
       "      <td>3025103017906</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201</th>\n",
       "      <td>0.237309</td>\n",
       "      <td>17.474505</td>\n",
       "      <td>0.132649</td>\n",
       "      <td>0.100873</td>\n",
       "      <td>0.25040</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>1.026221</td>\n",
       "      <td>3037064043447</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>-0.160557</td>\n",
       "      <td>14.540966</td>\n",
       "      <td>0.110238</td>\n",
       "      <td>-1.281666</td>\n",
       "      <td>0.17210</td>\n",
       "      <td>0.007581</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>3043077034884</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16971</th>\n",
       "      <td>0.111753</td>\n",
       "      <td>13.735530</td>\n",
       "      <td>0.157419</td>\n",
       "      <td>-0.696811</td>\n",
       "      <td>0.25690</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.347880</td>\n",
       "      <td>3043038037433</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11607</th>\n",
       "      <td>0.473653</td>\n",
       "      <td>14.733777</td>\n",
       "      <td>0.078445</td>\n",
       "      <td>-0.172412</td>\n",
       "      <td>0.15665</td>\n",
       "      <td>0.005324</td>\n",
       "      <td>0.378357</td>\n",
       "      <td>3045076027157</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0          1         2         3        4         5    Period  \\\n",
       "12017  0.380879  13.461285  0.096354 -0.869283  0.16640  0.007158  0.296013   \n",
       "20201  0.237309  17.474505  0.132649  0.100873  0.25040  0.007591  1.026221   \n",
       "2105  -0.160557  14.540966  0.110238 -1.281666  0.17210  0.007581  0.304000   \n",
       "16971  0.111753  13.735530  0.157419 -0.696811  0.25690  0.011461  0.347880   \n",
       "11607  0.473653  14.733777  0.078445 -0.172412  0.15665  0.005324  0.378357   \n",
       "\n",
       "           File_Name  True_class_labels  \n",
       "12017  3025103017906                  5  \n",
       "20201  3037064043447                  7  \n",
       "2105   3043077034884                  2  \n",
       "16971  3043038037433                  5  \n",
       "11607  3045076027157                  5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_set = X_testing.join(periods.set_index('File_Name'), on='File_Name')\n",
    "testing_set = testing_set[['0','1','2','3','4','5','Period', 'File_Name', 'True_class_labels']]\n",
    "testing_set.iloc[150:155,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RRab_train        = stars_label(training_set, true_class_1)\n",
    "RRc_train         = stars_label(training_set, true_class_2) \n",
    "RRd_train         = stars_label(training_set, true_class_3)\n",
    "blazhko_train     = stars_label(training_set, true_class_4)\n",
    "contact_Bi_train  = stars_label(training_set, true_class_5)\n",
    "semi_det_Bi_train = stars_label(training_set, true_class_6)\n",
    "rot_train         = stars_label(training_set, true_class_7)\n",
    "LPV_train         = stars_label(training_set, true_class_8)\n",
    "delta_scuti_train = stars_label(training_set, true_class_9)\n",
    "ACEP_train        = stars_label(training_set, true_class_10)\n",
    "cep_ii_train      = stars_label(training_set, true_class_12)\n",
    "\n",
    "RRab_test        = stars_label(testing_set, true_class_1)\n",
    "RRc_test         = stars_label(testing_set, true_class_2) \n",
    "RRd_test         = stars_label(testing_set, true_class_3)\n",
    "blazhko_test     = stars_label(testing_set, true_class_4)\n",
    "contact_Bi_test  = stars_label(testing_set, true_class_5)\n",
    "semi_det_Bi_test = stars_label(testing_set, true_class_6)\n",
    "rot_test         = stars_label(testing_set, true_class_7)\n",
    "LPV_test         = stars_label(testing_set, true_class_8)\n",
    "delta_scuti_test = stars_label(testing_set, true_class_9)\n",
    "ACEP_test        = stars_label(testing_set, true_class_10)\n",
    "cep_ii_test      = stars_label(testing_set, true_class_12)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eclipsing_binary_train has (6312, 9)\n",
      "pulsating_train has (7338, 9)\n",
      "rotational_train has (2545, 9)\n",
      "eclipsing_binary_test has (17000, 9)\n",
      "pulsating_test has (3151, 9)\n",
      "rotational_test has (1091, 9)\n",
      "Before OverSampling, counts of label 20: (6312,)\n",
      "Before OverSampling, counts of label 21: (2545,)\n",
      "Before OverSampling, counts of label 22: (7338,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 20: (7338,)\n",
      "After OverSampling, counts of label 21: (7338,)\n",
      "After OverSampling, counts of label 22: (7338,)\n",
      "Accuracy 0.805 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 11}\n",
      "Accuracy 0.809 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 13}\n",
      "Accuracy 0.788 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 7}\n",
      "Accuracy 0.808 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.08, 'max_depth': 13}\n",
      "Accuracy 0.799 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 9}\n",
      "Accuracy 0.753 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.0, 'max_depth': 1}\n",
      "Accuracy 0.769 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 4}\n",
      "Accuracy 0.797 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 9}\n",
      "Accuracy 0.769 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 4}\n",
      "Accuracy 0.769 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 4}\n",
      "Accuracy 0.769 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.782 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 6}\n",
      "Accuracy 0.768 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.05, 'max_depth': 4}\n",
      "Accuracy 0.803 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 10}\n",
      "Accuracy 0.806 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.0, 'max_depth': 12}\n",
      "Accuracy 0.782 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 6}\n",
      "Accuracy 0.810 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 13}\n",
      "Accuracy 0.803 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 11}\n",
      "Accuracy 0.811 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Accuracy 0.782 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.07, 'max_depth': 6}\n",
      "100%|██████████| 20/20 [15:45<00:00, 52.57s/it, best loss: 0.189477496754]\n",
      "{'eta': 0.03, 'max_depth': 13, 'x_subsample': 0.6000000000000001}\n",
      "Test set has shape (21242, 11)\n",
      "Misclassified data has shape (5947, 11)\n",
      "New test set has shape (15295, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.68988235  0.21064706  0.09947059]\n",
      " [ 0.131989    0.73418882  0.13382218]\n",
      " [ 0.05268169  0.06950175  0.87781657]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11be37d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                                # FIRST LAYER\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_FL, testing_data_FL, y_FL_training_counts = first_layer()\n",
    "X_train_FL, y_train_FL, X_test_FL, y_test_FL = smote_augmentation(training_data_FL, testing_data_FL,label='New_label',aug_tech='SMOTE')#aug_tech='ADASYN'\n",
    "\n",
    "if multi_class:\n",
    "    classes_types_FL = ['Eclipsing','Rotational','Pulsating']\n",
    "    types_FL         ='Type_FL'\n",
    "    nClasses_FL      = len(classes_types_FL)\n",
    "\n",
    "else:\n",
    "    classes_types_FL = ['Eclipsing','Rotational']\n",
    "    types_FL         ='Type_Binary'\n",
    "    nClasses_FL      = 2\n",
    "    \n",
    "\n",
    "X_tr = X_train_FL; y_tr=y_train_FL\n",
    "\n",
    "    n_estimators      = np.arange(50,1000,100)\n",
    "    max_features      = ['auto', 'sqrt', 'log2']\n",
    "    min_samples_split = np.arange(1,20,1)\n",
    "    class_weight = ['balanced']\n",
    "    #max_depth         = np.arange(1,10,2)\n",
    "    param_grid        = dict(n_estimators=n_estimators, max_features=max_features, \\\n",
    "                              min_samples_split=min_samples_split,class_weight=class_weight)\n",
    "\n",
    "        \n",
    "    opt_parameters_rf = gridsearch(RandomForestClassifier(), param_grid, n_iter = 10, cv = 5, filename= results_dir + types + layer + '_RF_hyparameters.txt')\n",
    "    model_save(RandomForestClassifier(**opt_parameters_rf), X_train=X_train, y_train=y_train, filename_model= results_dir + types + layer + '_RF_model.sav')\n",
    "\n",
    "\n",
    "def objectives_xgb(space):\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "\n",
    "#     classifier = XGBClassifier(max_depth=space['max_depth'])\n",
    "\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "def hyperparameter_optimization(multi=False):\n",
    "    if multi:\n",
    "        objective = 'multi:softmax'\n",
    "    else: \n",
    "        objective = 'binary:logistic'\n",
    "        \n",
    "    space ={'objective':objective,'max_depth':hp.choice('max_depth', np.arange(1,15,dtype=int)),\\\n",
    "            'eta': hp.quniform ('eta', 0.0, 0.09, 0.01),\\\n",
    "            'subsample': hp.quniform ('x_subsample', 0.4,0.8,0.1)}\n",
    "\n",
    "#     space ={'objective':objective,'max_depth':hp.choice('max_depth', np.arange(1,13,dtype=int))}\n",
    "\n",
    "    trials         = Trials()\n",
    "    opt_parameters = fmin(fn=objectives_xgb,space=space,algo=tpe.suggest,max_evals=20,trials=trials)\n",
    "    print(opt_parameters)\n",
    "    \n",
    "    return opt_parameters\n",
    "\n",
    "def analysis_XGB(X_train, y_train, types,save_model=False,multi=False):\n",
    "    if multi:\n",
    "        objective = 'multi:softmax'\n",
    "    else: \n",
    "        objective = 'binary:logistic'\n",
    "    opt_parameters_XGB = hyperparameter_optimization(multi=multi)\n",
    "    fit_model = model_save(XGBClassifier(objective=objective,**opt_parameters_XGB), X_train=X_train, y_train=y_train,\\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model\n",
    "\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, testing_set,classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat, misclassified  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "    misclassified_data, new_DF = find_misclassification(misclassified,y_test, test_data=testing_set,ypred=ypred, save_dir=results_dir + types +'Misclassification_XGB.csv')\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    return ypred, accuracy, MCC, conf_mat, new_DF, misclassified\n",
    "\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_FL, fit_model_xgb_FL = analysis_XGB(X_train_FL,y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "ypred_xgb_FL, accuracy_xgb_FL, MCC_xgb_FL, conf_mat_xgb_FL,new_DF_xgb_FL, misclassified_xgb_FL  = final_prediction_XGB(fit_model_xgb_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, testing_data_FL, classes_types_FL, types_FL, nClasses_FL, load_model)         \n",
    "acc_xgb_FL.append(accuracy_xgb_FL)\n",
    "mcc_xgb_FL.append(MCC_xgb_FL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecl train has (3156, 9)\n",
      "EA_train has (3156, 9)\n",
      "ecl_test has (15647, 9)\n",
      "EA_test has (1353, 9)\n",
      "Before OverSampling, counts of label 5: (3156,)\n",
      "Before OverSampling, counts of label 6: (3156,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 5: (3156,)\n",
      "After OverSampling, counts of label 6: (3156,)\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 10}\n",
      "Accuracy 0.936 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 4}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 7}\n",
      "Accuracy 0.932 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.04, 'max_depth': 13}\n",
      "Accuracy 0.931 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 13}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 7}\n",
      "Accuracy 0.938 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 3}\n",
      "Accuracy 0.935 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.02, 'max_depth': 8}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.09, 'max_depth': 10}\n",
      "Accuracy 0.936 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 4}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 7}\n",
      "Accuracy 0.935 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 8}\n",
      "Accuracy 0.937 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 3}\n",
      "Accuracy 0.932 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 11}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 8}\n",
      "Accuracy 0.936 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.09, 'max_depth': 5}\n",
      "Accuracy 0.931 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 13}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.07, 'max_depth': 11}\n",
      "Accuracy 0.934 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 7}\n",
      "Accuracy 0.935 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 2}\n",
      "100%|██████████| 20/20 [01:13<00:00,  3.38s/it, best loss: 0.0622607775482]\n",
      "{'eta': 0.06, 'max_depth': 2, 'x_subsample': 0.7000000000000001}\n",
      "Test set has shape (11728, 11)\n",
      "Misclassified data has shape (1128, 11)\n",
      "New test set has shape (10600, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.89646901  0.10353099]\n",
      " [ 0.03751914  0.96248086]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d228d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-------------------------------------------------------------------------------'\n",
    "                        # SECOND LAYER ECLIPSING BINARY\n",
    "'-------------------------------------------------------------------------------'\n",
    "training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts = second_layer_EB()\n",
    "\n",
    "all_testing_set_SL_EB = new_DF_xgb_FL\n",
    "#         all_testing_set_SL_EB = new_DF_xgb_FL.drop(['New_label', 'Prediction'],axis=1)\n",
    "ecl_class_test = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_5]\n",
    "EA_class_test  = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_6]\n",
    "testing_set_SL_EB  = pd.concat([ecl_class_test,EA_class_test], axis=0)\n",
    "\n",
    "X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB = smote_augmentation(training_data_SL_EB, testing_set_SL_EB,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "# Random Forest Classifier    \n",
    "classes_types_SL_EB = ['Ecl','EA']\n",
    "types_SL_EB         ='Type_SL_Ecl_EA'\n",
    "nClasses_SL_EB      = 2\n",
    "\n",
    "X_tr = X_train_SL_EB; y_tr=y_train_SL_EB\n",
    "\n",
    "def objectives_xgb(space):\n",
    "#     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_SL_EB, fit_model_xgb_SL_EB = analysis_XGB(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training  \n",
    "ypred_xgb_SL_EB, accuracy_xgb_SL_EB, MCC_xgb_SL_EB, conf_mat_xgb_SL_EB,new_DF_xgb_SL_EB,misclassified_xgb_SL_EB = final_prediction_XGB(fit_model_xgb_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB,testing_set_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "\n",
    "acc_xgb_SL_EB.append(accuracy_xgb_SL_EB)\n",
    "mcc_xgb_SL_EB.append(MCC_xgb_SL_EB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RR Lyrae train has (6122, 9)\n",
      "LPV train has (900, 9)\n",
      "Cepheids train has (214, 9)\n",
      "Delta Scuti train has (102, 9)\n",
      "RR_Lyrae_test has (2628, 9)\n",
      "LPV_test has (386, 9)\n",
      "cepheids_test has (92, 9)\n",
      "Delta Scuti test has (45, 9)\n",
      "[23 24 25 26]\n",
      "[6122  900  102  214]\n",
      "Before OverSampling, counts of label 23: (6122,)\n",
      "Before OverSampling, counts of label 24: (900,)\n",
      "Before OverSampling, counts of label 25: (102,)\n",
      "Before OverSampling, counts of label 26: (214,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 23: (6122,)\n",
      "After OverSampling, counts of label 24: (6122,)\n",
      "After OverSampling, counts of label 25: (6122,)\n",
      "After OverSampling, counts of label 26: (6122,)\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.02, 'max_depth': 4}\n",
      "Accuracy 0.992 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.07, 'max_depth': 2}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.07, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.02, 'max_depth': 9}\n",
      "Accuracy 0.984 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.09, 'max_depth': 1}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.06, 'max_depth': 11}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 11}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.0, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 10}\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 14}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.02, 'max_depth': 7}\n",
      "Accuracy 0.994 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.02, 'max_depth': 3}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.09, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 9}\n",
      "100%|██████████| 20/20 [12:13<00:00, 40.11s/it, best loss: 0.00383826864079]\n",
      "{'eta': 0.07, 'max_depth': 8, 'x_subsample': 0.6000000000000001}\n",
      "Test set has shape (2766, 11)\n",
      "Misclassified data has shape (23, 11)\n",
      "New test set has shape (2743, 11)\n",
      "Normalized confusion matrix\n",
      "[[  9.93037424e-01   0.00000000e+00   8.70322019e-04   6.09225413e-03]\n",
      " [  0.00000000e+00   9.97282609e-01   0.00000000e+00   2.71739130e-03]\n",
      " [  2.27272727e-02   0.00000000e+00   9.77272727e-01   0.00000000e+00]\n",
      " [  8.92857143e-02   0.00000000e+00   0.00000000e+00   9.10714286e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b533350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_SL_RLCD, testing_data1_SL_RLCD, y_SL_RLCD_training_counts = second_layer_RLCD()\n",
    "all_testing_set_SL_RLCD  = new_DF_xgb_FL\n",
    "all_testing_set_SL_RLCD  = new_DF_xgb_FL.drop(['New_label', 'Prediction'],axis=1)\n",
    "\n",
    "RRab_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_1]\n",
    "RRc_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_2]\n",
    "RRd_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_3]\n",
    "blazhko_test             = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_4]\n",
    "LPV_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_8]\n",
    "ACEP_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_10]\n",
    "cep_ii_test              = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_12]\n",
    "delta_scuti_test         = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_9]\n",
    "\n",
    "RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "LPV_test_class      = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "ds_test             = delta_scuti_test\n",
    "ds_test_class       = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "testing_set_SL_RLCD          = testing_data_SL_RLCD\n",
    "\n",
    "X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD = smote_augmentation(training_data_SL_RLCD,testing_set_SL_RLCD,label='New_label',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_SL_RLCD; y_tr=y_train_SL_RLCD\n",
    "\n",
    "def objectives_xgb(space):\n",
    "#     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# XGBoost Classifier    \n",
    "classes_types_SL_RLCD = ['RR Lyrae','LPV', 'Cepheids', '$\\delta$-Scuti']\n",
    "types_SL_RLCD         ='Type_SL_RLCD'\n",
    "nClasses_SL_RLCD      = len(classes_types_SL_RLCD)\n",
    "\n",
    "opt_xgb_SL_RLCD, fit_model_xgb_SL_RLCD = analysis_XGB(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented if you don't want to train the algorithm  \n",
    "ypred_xgb_SL_RLCD, accuracy_xgb_SL_RLCD, MCC_xgb_SL_RLCD, conf_mat_xgb_SL_RLCD, new_DF_xgb_SL_RLCD,misclassified_xgb_SL_RLCD = final_prediction_XGB(fit_model_xgb_SL_RLCD,X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD,testing_set_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD,load_model)\n",
    "\n",
    "acc_xgb_SL_RLCD.append(accuracy_xgb_SL_RLCD)\n",
    "mcc_xgb_SL_RLCD.append(MCC_xgb_SL_RLCD)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRab train has (3026, 9)\n",
      "RRc train has (2626, 9)\n",
      "RRd train has (351, 9)\n",
      "Blazhko train has (119, 9)\n",
      "RRab test has (1215, 9)\n",
      "RRc test has (910, 9)\n",
      "RRd test has (122, 9)\n",
      "Blazhko test has (51, 9)\n",
      "Before OverSampling, counts of label 1: (3026,)\n",
      "Before OverSampling, counts of label 2: (2626,)\n",
      "Before OverSampling, counts of label 3: (351,)\n",
      "Before OverSampling, counts of label 4: (119,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 1: (3026,)\n",
      "After OverSampling, counts of label 2: (3026,)\n",
      "After OverSampling, counts of label 3: (3026,)\n",
      "After OverSampling, counts of label 4: (3026,)\n",
      "Accuracy 0.919 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.04, 'max_depth': 7}\n",
      "Accuracy 0.888 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.846 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 2}\n",
      "Accuracy 0.929 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 9}\n",
      "Accuracy 0.911 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 6}\n",
      "Accuracy 0.926 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.02, 'max_depth': 8}\n",
      "Accuracy 0.888 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.07, 'max_depth': 4}\n",
      "Accuracy 0.932 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.03, 'max_depth': 12}\n",
      "Accuracy 0.934 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.02, 'max_depth': 12}\n",
      "Accuracy 0.901 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.05, 'max_depth': 5}\n",
      "Accuracy 0.863 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 3}\n",
      "Accuracy 0.926 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 8}\n",
      "Accuracy 0.920 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 7}\n",
      "Accuracy 0.912 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 6}\n",
      "Accuracy 0.925 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.07, 'max_depth': 8}\n",
      "Accuracy 0.927 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.09, 'max_depth': 9}\n",
      "Accuracy 0.936 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 14}\n",
      "Accuracy 0.842 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 2}\n",
      "Accuracy 0.901 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.08, 'max_depth': 5}\n",
      "Accuracy 0.885 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.09, 'max_depth': 4}\n",
      "100%|██████████| 20/20 [08:54<00:00, 22.98s/it, best loss: 0.064026266263]\n",
      "{'eta': 0.05, 'max_depth': 13, 'x_subsample': 0.6000000000000001}\n",
      "Test set has shape (2282, 11)\n",
      "Misclassified data has shape (299, 11)\n",
      "New test set has shape (1983, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.93100582  0.          0.00249377  0.06650042]\n",
      " [ 0.          0.86203091  0.13796909  0.        ]\n",
      " [ 0.04918033  0.45901639  0.47540984  0.01639344]\n",
      " [ 0.50980392  0.          0.01960784  0.47058824]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1217082d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "            # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts = third_layer_RRLyrae()\n",
    "all_testing_set_TL_RL  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "RRab_class_test    = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_1]\n",
    "RRc_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_2]\n",
    "RRd_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_3]\n",
    "blazhko_class_test = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_4]\n",
    "testing_set_TL_RL  = pd.concat([RRab_class_test,RRc_class_test,RRd_class_test,blazhko_class_test], axis=0)\n",
    "\n",
    "X_train_TL_RL, y_train_TL_RL,X_test_TL_RL, y_test_TL_RL = smote_augmentation(training_data_TL_RRLyrae,testing_set_TL_RL,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_TL_RL; y_tr=y_train_TL_RL\n",
    "\n",
    "def objectives_xgb(space):\n",
    "#     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "classes_types_TL_RL = ['RRab', 'RRc', 'RRd', \"Blazhko\"]\n",
    "types_TL_RL         ='Type_TL_RRLyrae'\n",
    "nClasses_TL_RL      = len(classes_types_TL_RL)\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_TL_RL, fit_model_xgb_TL_RL = analysis_XGB(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training \n",
    "ypred_xgb_TL_RL, accuracy_xgb_TL_RL, MCC_xgb_TL_RL, conf_mat_xgb_TL_RL, new_DF_xgb_TL_RL, misclassified_xgb_TL_RL = final_prediction_XGB(fit_model_xgb_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL,testing_set_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "\n",
    "acc_xgb_TL_RL.append(accuracy_xgb_TL_RL)\n",
    "mcc_xgb_TL_RL.append(MCC_xgb_TL_RL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACEP train has (107, 9)\n",
      "Cep-II train has (107, 9)\n",
      "ACEP test has (36, 9)\n",
      "Cep-II test has (20, 9)\n",
      "Before OverSampling, counts of label 10: (107,)\n",
      "Before OverSampling, counts of label 12: (107,)\n",
      "----------------------------------------------------------------------\n",
      "After OverSampling, counts of label 10: (107,)\n",
      "After OverSampling, counts of label 12: (107,)\n",
      "Accuracy 0.870 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 10}\n",
      "Accuracy 0.860 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 5}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 9}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 8}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 2}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.06, 'max_depth': 12}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 5}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 3}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 13}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.08, 'max_depth': 5}\n",
      "Accuracy 0.870 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 8}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.02, 'max_depth': 8}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 4}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 13}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.07, 'max_depth': 8}\n",
      "Accuracy 0.870 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 6}\n",
      "Accuracy 0.860 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 6}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.06, 'max_depth': 12}\n",
      "Accuracy 0.875 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.07, 'max_depth': 9}\n",
      "Accuracy 0.865 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.02, 'max_depth': 5}\n",
      "100%|██████████| 20/20 [00:04<00:00,  4.50it/s, best loss: 0.125324675325]\n",
      "{'eta': 0.06, 'max_depth': 11, 'x_subsample': 0.4}\n",
      "Test set has shape (51, 11)\n",
      "Misclassified data has shape (7, 11)\n",
      "New test set has shape (44, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.90909091  0.09090909]\n",
      " [ 0.22222222  0.77777778]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1217b7d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'-----------------------------------------------------------------------------'\n",
    "                        # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "'-----------------------------------------------------------------------------'\n",
    "training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts = third_layer_Cepheids()\n",
    "all_testing_set_TL_Cep  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "ACEP_class_test    = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_10]\n",
    "Cep_ii_class_test  = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_12]\n",
    "testing_set_TL_Cep = pd.concat([ACEP_class_test,Cep_ii_class_test], axis=0)\n",
    "\n",
    "X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep = smote_augmentation(training_data_TL_cep,testing_set_TL_Cep,label='True_class_labels',aug_tech='SMOTE')\n",
    "\n",
    "X_tr = X_train_TL_Cep; y_tr=y_train_TL_Cep\n",
    "\n",
    "def objectives_xgb(space):\n",
    "#     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "    classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                               subsample = space['subsample'])\n",
    "\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    \n",
    "    accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "    CrossValMean = accuracies.mean()\n",
    "\n",
    "    print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "    return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "classes_types_TL_Cep = ['ACEP','CEP-II']\n",
    "types_TL_Cep         ='Type_TL_Cepheids'\n",
    "nClasses_TL_Cep      = 2\n",
    "\n",
    "# XGBoost Classifier   \n",
    "opt_xgb_TL_Cep, fit_model_xgb_TL_Cep = analysis_XGB(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training\n",
    "ypred_xgb_TL_Cep, accuracy_xgb_TL_Cep, MCC_xgb_TL_Cep, conf_mat_xgb_TL_Cep, new_DF_xgb_TL_Cep, misclassified_xgb_TL_Cep = final_prediction_XGB(fit_model_xgb_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, testing_set_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "\n",
    "acc_xgb_TL_Cep.append(accuracy_xgb_TL_Cep)\n",
    "mcc_xgb_TL_Cep.append(MCC_xgb_TL_Cep)\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "\n",
    "metrics = open(\"./hierarchical-results_SMOTE/metrics.txt\", 'w')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_FL)*100,np.std(acc_xgb_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_FL)*100,np.std(mcc_xgb_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_EB)*100,np.std(acc_xgb_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_EB)*100,np.std(mcc_xgb_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_RLCD)*100,np.std(acc_xgb_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_RLCD)*100,np.std(mcc_xgb_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_RL)*100,np.std(acc_xgb_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_RL)*100,np.std(mcc_xgb_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_Cep)*100,np.std(acc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_Cep)*100,np.std(mcc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
