{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Good Code for Hierarchical Classification\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import FATS\n",
    "\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "from george import kernels\n",
    "import george\n",
    "import scipy.optimize as op\n",
    "\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "# The different classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix,balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Section Needs to be changed each time\n",
    "\n",
    "nFeatures = 7 # Number of features considered\n",
    "\n",
    "# All Labels of the different variabe stars\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "\n",
    "eclipsing_label = 20;rotational_label = 21;pulsating_label = 22;RR_Lyrae_label = 23; LPV_label = 24;\\\n",
    "delta_scuti_label = 25; cepheids_label   = 26\n",
    "\n",
    "# The directory to save the files\n",
    "plots_dir                 = './hierarchical-results_gp/plots/'\n",
    "results_dir               = './hierarchical-results_gp/results/'\n",
    "misclassify_dir           = r'./hierarchical-results_gp/results/Misclassification_'\n",
    "dotfile_dir               = './hierarchical-results_gp/Decision_tree_plots/'\n",
    "savedir_decision_boundary ='./hierarchical-results_gp/decision_boundary_plots/'\n",
    "features_dir              ='./hierarchical-results_gp/feature_analysis/'\n",
    "lC_dir                    = '../SSS_Per_Var_Cat/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stars_label(data, label):\n",
    "    '''Set variable names to specific class label'''\n",
    "    stars = data[data.Type == label]\n",
    "    return stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer Hierarchical Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_layer():\n",
    "    '''\n",
    "    We define first layer of the hierarchical tree. The first layer consists of Eclipsing Binaries, Rotational,\n",
    "    and Pulsating \n",
    "    '''\n",
    "    \n",
    "    # First Layer\n",
    "    eclipsing_binary_train       = pd.concat([contact_Bi_train, semi_det_Bi_train], axis=0)\n",
    "    eclipsing_binary_train_class = np.full(len(eclipsing_binary_train), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_train       = rot_train\n",
    "    rotational_train_class = np.full(len(rotational_train),rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_train       = pd.concat([RRab_train, RRc_train, RRd_train, blazhko_train, LPV_train, delta_scuti_train, ACEP_train, cep_ii_train] ,axis=0)\n",
    "    pulsating_train_class = np.full(len(pulsating_train), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_train has {}\".format(eclipsing_binary_train.shape))\n",
    "    print(\"pulsating_train has {}\".format(pulsating_train.shape))\n",
    "    print(\"rotational_train has {}\".format(rotational_train.shape))\n",
    "\n",
    "    eclipsing_binary_test       = pd.concat([contact_Bi_test, semi_det_Bi_test], axis=0)\n",
    "    eclipsing_binary_test_class = np.full(len(eclipsing_binary_test), eclipsing_label, dtype=int)\n",
    "\n",
    "    rotational_test       = rot_test\n",
    "    rotational_test_class = np.full(len(rotational_test), rotational_label, dtype=int)\n",
    "\n",
    "    pulsating_test       = pd.concat([RRab_test, RRc_test, RRd_test, blazhko_test, LPV_test, delta_scuti_test, ACEP_test, cep_ii_test] ,axis=0)\n",
    "    pulsating_test_class = np.full(len(pulsating_test), pulsating_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"eclipsing_binary_test has {}\".format(eclipsing_binary_test.shape))\n",
    "    print(\"pulsating_test has {}\".format(pulsating_test.shape))\n",
    "    print(\"rotational_test has {}\".format(rotational_test.shape))\n",
    "    \n",
    "    first_layer_train       = pd.concat([eclipsing_binary_train, rotational_train, pulsating_train], axis=0)\n",
    "    first_layer_train_class = np.concatenate((eclipsing_binary_train_class, rotational_train_class, pulsating_train_class), axis=0)\n",
    "    training_data_FL        = pd.DataFrame(first_layer_train)\n",
    "    training_data_FL['New_label'] = first_layer_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    first_layer_test       = pd.concat([eclipsing_binary_test, rotational_test, pulsating_test], axis=0)\n",
    "    first_layer_test_class = np.concatenate((eclipsing_binary_test_class, rotational_test_class, pulsating_test_class), axis=0)\n",
    "    testing_data_FL        = pd.DataFrame(first_layer_test)\n",
    "    testing_data_FL['New_label'] = first_layer_test_class\n",
    "    \n",
    "    y_FL_training, y_FL_training_counts = np.unique(first_layer_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_FL, testing_data_FL, y_FL_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for first Branch: Eclipsing Binaries (Ecl & EA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def second_layer_EB():\n",
    "    \n",
    "    # Second Layer Eclipsing Binary    \n",
    "    ecl_train = contact_Bi_train\n",
    "    ecl_train_class = np.full(len(ecl_train), true_class_5, dtype=int)\n",
    "\n",
    "    EA_train       = semi_det_Bi_train\n",
    "    EA_train_class = np.full(len(EA_train),true_class_6, dtype=int)\n",
    " \n",
    "    print(\"ecl train has {}\".format(ecl_train.shape))\n",
    "    print(\"EA_train has {}\".format(EA_train.shape))\n",
    "\n",
    "    ecl_test       = contact_Bi_test\n",
    "    ecl_test_class = np.full(len(ecl_test), true_class_5, dtype=int)\n",
    "\n",
    "    EA_test       = semi_det_Bi_test\n",
    "    EA_test_class = np.full(len(EA_test), true_class_6, dtype=int)\n",
    "\n",
    "    print(\"ecl_test has {}\".format(ecl_test.shape))\n",
    "    print(\"EA_test has {}\".format(EA_test.shape))\n",
    "\n",
    "    \n",
    "    second_layer_EB_train       = pd.concat([ecl_train, EA_train], axis=0)\n",
    "    second_layer_EB_train_class = np.concatenate((ecl_train_class,EA_train_class), axis=0)\n",
    "    training_data_SL_EB         = pd.DataFrame(second_layer_EB_train)\n",
    "    training_data_SL_EB['New_label'] = second_layer_EB_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_EB_test       = pd.concat([ecl_test, EA_test], axis=0)\n",
    "    second_layer_EB_test_class = np.concatenate((ecl_test_class, EA_test_class), axis=0)\n",
    "    testing_data_SL_EB         = pd.DataFrame(second_layer_EB_test)\n",
    "    testing_data_SL_EB['New_label'] = second_layer_EB_test_class\n",
    "    \n",
    "    y_SL_EB_training, y_SL_EB_training_counts = np.unique(second_layer_EB_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer Hierarchical level for 2nd Branch: RLCD\n",
    "### RR Lyrae, LPV, Cepheid and $\\delta$-Scuti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2 RR Lyrae, LPV, Cepheid, Delta-Scuti\n",
    "def second_layer_RLCD():\n",
    "    \n",
    "    # First Layer\n",
    "    RR_Lyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    RR_Lyrae_train_class = np.full(len(RR_Lyrae_train), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_train_class = np.full(len(LPV_train),LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_train       = pd.concat([ACEP_train,cep_ii_train] ,axis=0)\n",
    "    cepheids_train_class = np.full(len(cepheids_train), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_train       = delta_scuti_train\n",
    "    ds_train_class = np.full(len(ds_train), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR Lyrae train has {}\".format(RR_Lyrae_train.shape))\n",
    "    print(\"LPV train has {}\".format(LPV_train.shape))\n",
    "    print(\"Cepheids train has {}\".format(cepheids_train.shape))\n",
    "    print(\"Delta Scuti train has {}\".format(ds_train.shape))\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "\n",
    "    LPV_test_class = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    \n",
    "    ds_test       = delta_scuti_test\n",
    "    ds_test_class = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "\n",
    "    print(\"RR_Lyrae_test has {}\".format(RR_Lyrae_test.shape))\n",
    "    print(\"LPV_test has {}\".format(LPV_test.shape))\n",
    "    print(\"cepheids_test has {}\".format(cepheids_test.shape))\n",
    "    print(\"Delta Scuti test has {}\".format(ds_test.shape))\n",
    "    \n",
    "    second_layer_RLCD_train       = pd.concat([RR_Lyrae_train,LPV_train,cepheids_train,ds_train], axis=0)\n",
    "    second_layer_RLCD_train_class = np.concatenate((RR_Lyrae_train_class,LPV_train_class,cepheids_train_class,ds_train_class), axis=0)\n",
    "    training_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_train)\n",
    "    training_data_SL_RLCD['New_label'] = second_layer_RLCD_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    y_SL_RLCD_training, y_SL_RLCD_training_counts = np.unique(second_layer_RLCD_train_class, return_counts=True)\n",
    "\n",
    "    print(y_SL_RLCD_training)\n",
    "    print(y_SL_RLCD_training_counts)\n",
    "    return training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for first Branch: RRLyrae\n",
    "### RRab, RRc, RRd, and Blazhko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_RRLyrae():\n",
    "    \n",
    "    # Third Layer\n",
    "    RRab_train_class    = np.full(len(RRab_train), true_class_1, dtype=int)\n",
    "    RRc_train_class     = np.full(len(RRc_train), true_class_2, dtype=int)\n",
    "    RRd_train_class     = np.full(len(RRd_train), true_class_3, dtype=int)\n",
    "    blazhko_train_class = np.full(len(blazhko_train), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab train has {}\".format(RRab_train.shape))\n",
    "    print(\"RRc train has {}\".format(RRc_train.shape))\n",
    "    print(\"RRd train has {}\".format(RRd_train.shape))\n",
    "    print(\"Blazhko train has {}\".format(blazhko_train.shape))\n",
    "    \n",
    "    RRab_test_class    = np.full(len(RRab_test), true_class_1, dtype=int)\n",
    "    RRc_test_class     = np.full(len(RRc_test), true_class_2, dtype=int)\n",
    "    RRd_test_class     = np.full(len(RRd_test), true_class_3, dtype=int)\n",
    "    blazhko_test_class = np.full(len(blazhko_test), true_class_4, dtype=int)\n",
    "\n",
    "    print(\"RRab test has {}\".format(RRab_test.shape))\n",
    "    print(\"RRc test has {}\".format(RRc_test.shape))\n",
    "    print(\"RRd test has {}\".format(RRd_test.shape))\n",
    "    print(\"Blazhko test has {}\".format(blazhko_test.shape))\n",
    "\n",
    "    \n",
    "    third_layer_RRLyrae_train       = pd.concat([RRab_train,RRc_train,RRd_train,blazhko_train], axis=0)\n",
    "    third_layer_RRLyrae_train_class = np.concatenate((RRab_train_class,RRc_train_class,RRd_train_class,blazhko_train_class), axis=0)\n",
    "    training_data_TL_RRLyrae        = pd.DataFrame(third_layer_RRLyrae_train)\n",
    "    training_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_RRLyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    third_layer_RRLyrae_test_class = np.concatenate((RRab_test_class,RRc_test_class,RRd_test_class,blazhko_test_class), axis=0)\n",
    "    testing_data_TL_RRLyrae         = pd.DataFrame(third_layer_RRLyrae_test)\n",
    "    testing_data_TL_RRLyrae['New_label'] = third_layer_RRLyrae_test_class\n",
    "    \n",
    "    y_TL_RRLyrae_training, y_TL_RRLyrae_training_counts = np.unique(third_layer_RRLyrae_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Layer Hierarchical level for 2nd Branch: Cepheids\n",
    "### ACEP and Cep-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 3 RR Lyrae classes\n",
    "def third_layer_Cepheids():\n",
    "    \n",
    "    # Third Layer\n",
    "    ACEP_train_class   = np.full(len(ACEP_train), true_class_10, dtype=int)\n",
    "    cep_ii_train_class = np.full(len(cep_ii_train), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP train has {}\".format(ACEP_train.shape))\n",
    "    print(\"Cep-II train has {}\".format(cep_ii_train.shape))\n",
    "\n",
    "\n",
    "    ACEP_test_class   = np.full(len(ACEP_test), true_class_10, dtype=int)\n",
    "    cep_ii_test_class = np.full(len(cep_ii_test), true_class_12, dtype=int)\n",
    "\n",
    "    print(\"ACEP test has {}\".format(ACEP_test.shape))\n",
    "    print(\"Cep-II test has {}\".format(cep_ii_test.shape))\n",
    "    \n",
    "    third_layer_cep_train       = pd.concat([ACEP_train,cep_ii_train], axis=0)\n",
    "    third_layer_cep_train_class = np.concatenate((ACEP_train_class,cep_ii_train_class), axis=0)\n",
    "    training_data_TL_cep        = pd.DataFrame(third_layer_cep_train)\n",
    "    training_data_TL_cep['New_label'] = third_layer_cep_train_class\n",
    "#     print(training_data_FL.shape)\n",
    "\n",
    "    third_layer_cep_test       = pd.concat([ACEP_test,cep_ii_test], axis=0)\n",
    "    third_layer_cep_test_class = np.concatenate((ACEP_test_class,cep_ii_test_class), axis=0)\n",
    "    testing_data_TL_cep        = pd.DataFrame(third_layer_cep_test)\n",
    "    testing_data_TL_cep['New_label'] = third_layer_cep_test_class\n",
    "    \n",
    "    y_TL_cep_training, y_TL_cep_training_counts = np.unique(third_layer_cep_train_class, return_counts=True)\n",
    "\n",
    "    \n",
    "    return training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculates how much each class should be augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_augmentation(nAugmentation, y_training_counts):    \n",
    "    '''\n",
    "    This section calculates the number of augmentation to be carried out for each specific class\n",
    "    Parameters: nAugmentation\n",
    "                Integer values. Specify the total number of samples to generate\n",
    "                \n",
    "                y_training_counts\n",
    "                A list of total number of examples each unique class has. For instance [Type 1: 1 Type 2: 3] has a \n",
    "                list of [3905 2898]\n",
    "                \n",
    "    Returns: number of samples\n",
    "             The number of times each class will be augmented\n",
    "    '''\n",
    "    number_of_samples = []\n",
    "    for i in range(len(y_training_counts)):\n",
    "        floatNsamples   = nAugmentation/y_training_counts[i] # Calculate the number of times the class need to be augmented - in float\n",
    "        nSamples        = Decimal(str(floatNsamples)).quantize(Decimal(\"1\"), rounding=ROUND_HALF_UP) # convert float to integer values\n",
    "        total_augmented = y_training_counts[i]*nSamples\n",
    "        number_of_samples.append(int(nSamples))\n",
    "        print('The number of sample in Class {} is {} and is now augmented by {} times. The augmented samples are {}'.format(i,y_training_counts[i],nSamples,total_augmented))\n",
    "    return number_of_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample period from true period distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_period(true_class,file_name,num_samples,N,magnitude, magnitude_err,distribution='Normal'):\n",
    "    '''\n",
    "    For each augmented sample, we will assign a period that has been sampled from the true period distribution \n",
    "    of their respective class.\n",
    "    \n",
    "    Parameters: true_class\n",
    "                The class of the variable star. Integer values varies from 0 to 13\n",
    "                \n",
    "                num_samples\n",
    "                The number of period to sample from the distribution. In our case we use num_samples=1 as we \n",
    "                sample 1 period each time\n",
    "                \n",
    "                distribution\n",
    "                1. Normal: The mean and the std of the true period distribution is calculated and we sample one period\n",
    "                           from a normal distribution using this mean and std\n",
    "                           \n",
    "                2.Random: We sample randomly one period from the true period distribution\n",
    "                \n",
    "    Returns: new_period\n",
    "             The new period for the augmented data set\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ascii_data  = pd.read_csv('../Ascii_SSS_Per_Table.txt',delim_whitespace=True,names = [\"SSS_ID\", \"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\", \"No_Name1\", 'No_Name2'])\n",
    "    types_stars = ascii_data[ascii_data.Type==true_class]\n",
    "    \n",
    "    \n",
    "    crts_period = types_stars[types_stars.File_Name.values.astype(int)==int(file_name)].Period.values[0]\n",
    "    crts_mean   = types_stars[types_stars.File_Name.values.astype(int)==int(file_name)].V_CSS.values[0]\n",
    "\n",
    "\n",
    "    snr                = np.sqrt((1/float(N))*np.sum(((magnitude - crts_mean)/magnitude_err)**2))\n",
    "    half_frequency     = 1/crts_period\n",
    "    uncertainty_freq   = half_frequency*np.sqrt(2/(N*snr**2))\n",
    "    uncertainty_period = 1/uncertainty_freq\n",
    "\n",
    "\n",
    "    #print('The half-width at half-maximum is {} ± {} days^-1'.format(half_frequency, uncertainty_freq))\n",
    "    std_period = (crts_period**2)*uncertainty_freq\n",
    "    \n",
    "    #print('The period is {} ± {} days'.format(crts_period, std_period))\n",
    "    \n",
    "    mu_p  = crts_period\n",
    "    std_p = std_period/3.0\n",
    "\n",
    "    if (distribution=='Normal'):\n",
    "        new_period  = np.abs(np.random.normal(mu_p, std_p, num_samples))\n",
    "    \n",
    "        #print('The true period is {} and sample period is {}'.format(mu,new_period))\n",
    "        \n",
    "    return new_period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_training(augmented_data,file_name,data_columns,features,true_class,new_label):\n",
    "    '''\n",
    "    Features extraction using FATS for training set (consists both real and augmented samples)\n",
    "    \n",
    "    Parameters: augmented_data\n",
    "                A table with the following format: Time, Std_Flux, True flux, Fake flux1, Fake flux2 ..., Fake fluxN\n",
    "                \n",
    "                file_name\n",
    "                The name for each files - to keep track of each variable stars\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                \n",
    "                true_class\n",
    "                The class of the variable stars. Integer values varies from 0 to 10\n",
    "                \n",
    "                new_label\n",
    "                The new label assigned to the aggregated classes. Integer values\n",
    "                \n",
    "    Returns: Feature_file\n",
    "             A table that contains the six features with true period for the real examples and sampling period for \n",
    "             augmented examples. The feature file has these columns\n",
    "             ['Skew','Mean','Std','SmallKurtosis','Amplitude','Meanvariance','Period','File_Name','True_class_label','New_label']\n",
    "    '''\n",
    "    feature_file = pd.DataFrame()\n",
    "        \n",
    "    #data  = pd.read_csv(augmented_data, sep=',', header=None)\n",
    "    data_aug = augmented_data\n",
    "    time_    = data_aug.iloc[:,0].values\n",
    "    mag_err_ = data_aug.iloc[:,1]\n",
    "    mag_     = data_aug.iloc[:,2]\n",
    "    N_       = data_aug.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    for j in range(2, data_aug.shape[1]):\n",
    "        magnitude_      = data_aug.iloc[:,j].values\n",
    "        lc              = np.array([ magnitude_, time_])\n",
    "        feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "        features_cal    = feature_extract.calculateFeature(lc)\n",
    "        features_name   = features_cal.result(method='features')\n",
    "        features_value  = features_cal.result(method='array')\n",
    "        features_df     = pd.DataFrame(features_value.reshape(1,len(features))) \n",
    "\n",
    "        features_df['Period']            = sampling_period(true_class,file_name,num_samples=1,N=N_,magnitude=mag_,magnitude_err=mag_err_,distribution='Normal')\n",
    "        features_df['File_Name']         = str(file_name)+'_'+str(j)\n",
    "        features_df['True_class_labels'] = true_class\n",
    "        features_df['New_label']         = new_label\n",
    "        feature_file                     = feature_file.append(features_df)\n",
    "\n",
    "    return feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmentation_and_featureExtraction(data_dir, data_,data_columns,features,update_ascii_period,\\\n",
    "                                      number_of_samples,save_folder_training = './data/GP/training_set/'):        \n",
    "    '''\n",
    "    Perform data augmentation using Gaussian Process and extract features using some functionality above (for e.g number_of_samples\n",
    "    and feature_extraction_training).This section finds all the filenames for each specific class, starting \n",
    "    from class 0 to class 12. For each specific class, it loads the filename, then perform data augmentation\n",
    "    of this object using the number of times this object needs to be augmented. Augmentation is done using\n",
    "    Gaussian process and fake light curves are randomly sampled within the 3-sigma interval. \n",
    "    \n",
    "    Parameters: data_dir\n",
    "                The directory that contains all the raw files (.dat)\n",
    "                \n",
    "                data_\n",
    "                The training data. Here for training data, we have given some aggregated classes a label where we\n",
    "                have put them under the 'New_label' columns.\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                \n",
    "                update_ascii_period\n",
    "                This data file include only filename that ends with '_1' with its correcponding true period from the \n",
    "                ascii catalog. This means that this data set consists of only real examples of variable stars. We need this\n",
    "                to get the true period.\n",
    "                \n",
    "                number_of_samples\n",
    "                We use the function above 'num_augmentation()' to find the number of times each class will be augmented\n",
    "                \n",
    "                save_folder_training\n",
    "                The directory we save the training features\n",
    "                \n",
    "    Returns: augmented_data\n",
    "             The real and augmented data Time Real_mag Fake_mag1 Fake_mag2... Fake_magN\n",
    "             \n",
    "             feature_file\n",
    "             The save file for features containing both real and augmented. This file will be loaded and use for \n",
    "             classification.\n",
    "    '''\n",
    "\n",
    "    foldername  = \"Split_1\"\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_training+foldername, 0o755)   \n",
    "        \n",
    "    files_trueClass = np.unique(data_.New_label.values)\n",
    "#     files_trueClass = files_trueClass[0:20] # Need to comment\n",
    "#     print(files_trueClass)\n",
    "\n",
    "    i=0\n",
    "    feature_file = pd.DataFrame()\n",
    "    for files in files_trueClass: \n",
    "        file_name = data_.File_Name[data_.New_label.values == files]\n",
    "        file_name = file_name.values\n",
    "#         file_name = file_name[0:20] # Need to comment\n",
    "#         print(file_name)\n",
    "                \n",
    "        for datafile in file_name:\n",
    "            file           = str(data_dir)+str(datafile)+'.txt'\n",
    "#            print(file)\n",
    "            data   = np.loadtxt(file)\n",
    "            time   = data[:,0]\n",
    "            x      = data[:,1]\n",
    "            y      = data[:,2]\n",
    "            yerr   = data[:,3]\n",
    "                        \n",
    "            augmented_data = pd.DataFrame() \n",
    "                        \n",
    "            kernel = np.var(y) * kernels.Matern52Kernel(metric=0.1, ndim=1)\n",
    "            gp     = george.GP(kernel)\n",
    "            comp   = gp.compute(x, yerr)\n",
    "            covariance_matrix = gp.get_matrix(x)\n",
    "            grad_likelihood   = gp.grad_log_likelihood(y, quiet=False)\n",
    "            likelihood        = gp.log_likelihood(y, quiet=False)\n",
    "\n",
    "            # Define the objective function (negative log-likelihood in this case).\n",
    "            def nll(p):\n",
    "                gp.set_parameter_vector(p)\n",
    "                ll = gp.log_likelihood(y, quiet=True)\n",
    "                return -ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "            # And the gradient of the objective function.\n",
    "            def grad_nll(p):\n",
    "                gp.set_parameter_vector(p)\n",
    "                return -gp.grad_log_likelihood(y, quiet=True)\n",
    "            \n",
    "            # Run the optimization routine.\n",
    "            p0 = gp.get_parameter_vector()\n",
    "            results = op.minimize(nll, p0, jac=grad_nll, method=\"L-BFGS-B\")\n",
    "\n",
    "            # Update the kernel and print the final log-likelihood.\n",
    "            gp.set_parameter_vector(results.x)\n",
    "\n",
    "            #Prediction: Calculate the mean and variance\n",
    "            x_pred         = x# np.linspace(min(x), max(x), 1000)#x # The coordinates where the predictive distribution should be computed.\n",
    "            pred, pred_var = gp.predict(y, x_pred, return_var=True)\n",
    "            samples_lc     = gp.sample_conditional(y, x_pred, number_of_samples[i])   \n",
    "\n",
    "            df         = pd.DataFrame(np.transpose(samples_lc))\n",
    "#             df.insert(loc=0, column='0', value=time)\n",
    "            df.insert(loc=0, column='0', value=x_pred)  \n",
    "            df.insert(loc=1, column='1', value=yerr)\n",
    "            df.insert(loc=2, column='2', value=y)\n",
    "\n",
    "            augmented_data = augmented_data.append(df)\n",
    "            true_labelling = period_data.Type[period_data.File_Name.values == datafile].values\n",
    "            \n",
    "            features_df    = feature_extraction_training(augmented_data=augmented_data,file_name=datafile,true_class=true_labelling[0],new_label=files,data_columns=data_columns,features=features)\n",
    "            feature_file   = feature_file.append(features_df)\n",
    "        i += 1\n",
    "    Newfeature_file     = feature_file.join(update_ascii_period.set_index('File_Name'), on='File_Name', lsuffix='_sample', rsuffix='_true')    \n",
    "    newFeature_data     = Newfeature_file\n",
    "\n",
    "    newFeature_data.Period_true.fillna(newFeature_data.Period_sample, inplace=True)\n",
    "    newFeature_data_df = newFeature_data.drop(labels='Period_sample', axis=1)\n",
    "    final_feature_file = newFeature_data_df[[0,1,2,3,4,5,'Period_true', 'File_Name', 'True_class_labels', 'New_label']]\n",
    "    final_feature_file = final_feature_file.rename(columns={'Period_true': 'Period'})\n",
    "    final_feature_file.to_csv(save_folder_training+foldername+'/Training_features.csv',index=None)#'/Type'+str(files)+'_features.csv',index=None)\n",
    "\n",
    "    return augmented_data, final_feature_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_test_set(data_dir,X_testing,data_columns,features,save_folder_test  = './data/GP/test_set/'):\n",
    "    '''\n",
    "    Perform extraction of features for test set.\n",
    "    \n",
    "    Parameters: data_dir\n",
    "                The directory that contains all the raw files (.dat)\n",
    "                \n",
    "                X_testing\n",
    "                The testing data. Here for testing data, we have given some aggregated classes a label where we\n",
    "                have put them under the 'New_label' columns.\n",
    "                \n",
    "                data_columns\n",
    "                The information the data has. Either data_columns = [magnitude', 'std magnitude', 'time']\n",
    "                or data_columns = ['magnitude', time']\n",
    "                \n",
    "                features\n",
    "                The list of features to be extracted: \n",
    "                features = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "                                \n",
    "                number_of_samples\n",
    "                We use the function above 'num_augmentation()' to find the number of times each class will be augmented\n",
    "                \n",
    "                save_folder_testing\n",
    "                The directory we save the testing features\n",
    "                \n",
    "    Returns: feature_file_testSet\n",
    "             The save file for features containing only real examples for test set. This file will be loaded and use for \n",
    "             classification.\n",
    "    '''\n",
    "#     periods           = ascii_data[['File_Name', 'Period']]\n",
    "    foldername        = \"Split_1\"\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_test+foldername, 0o755)\n",
    "    nFeatures    = len(features)\n",
    "    feature_file_test = pd.DataFrame()\n",
    "        \n",
    "    files_trueClass_test = np.unique(X_testing.New_label.values)\n",
    "#     files_trueClass_test = files_trueClass_test[0:20] # Need to comment\n",
    "#     print(files_trueClass_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_file_test = pd.DataFrame()\n",
    "    for files_test in files_trueClass_test:\n",
    "        file_name_test    = X_testing.File_Name[X_testing.New_label.values == files_test]\n",
    "        file_name_test = file_name_test.values\n",
    "#         file_name_test = file_name_test[0:20] # Need to comment\n",
    "#         print(file_name_test)\n",
    "        \n",
    "        for datafile_test in file_name_test:\n",
    "            file_test   = str(data_dir)+str(datafile_test)+'.txt'\n",
    "            data_test   = np.loadtxt(file_test)\n",
    "            time_test      = data_test[:,1]\n",
    "            magnitude_test = data_test[:,2]\n",
    "            std_mag_test   = data_test[:,3]\n",
    "            \n",
    "            lc              = np.array([ magnitude_test, time_test])\n",
    "            feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "            features_cal    = feature_extract.calculateFeature(lc)\n",
    "            features_name   = features_cal.result(method='features')\n",
    "            features_value  = features_cal.result(method='array')\n",
    "\n",
    "\n",
    "            features_df_test              = pd.DataFrame(features_value.reshape(1,len(features)))            \n",
    "            features_df_test['File_Name'] =  str(datafile_test)   \n",
    "            true_label_test               = period_data.Type[period_data.File_Name.values == datafile_test].values\n",
    "            features_df_test['True_class_labels'] = true_label_test\n",
    "            features_df_test['New_label']         = files_test\n",
    "            feature_file_test                     = feature_file_test.append(features_df_test)          \n",
    "        \n",
    "        feature_file_test['File_Name'] = feature_file_test['File_Name'].astype(int)\n",
    "        feature_file_testSet = feature_file_test.join(periods.set_index('File_Name'), on='File_Name')\n",
    "        feature_file_testSet = feature_file_testSet[[0,1,2,3,4,5,'Period', 'File_Name', 'True_class_labels','New_label']]\n",
    "#        print(feature_file_testSet)\n",
    "        feature_file_testSet.to_csv(save_folder_test+foldername+'/Test_features.csv',index=None)\n",
    "    return feature_file_testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_sets(data_dir,ascii_files):\n",
    "    files  = glob.glob(data_dir+'/*.txt')\n",
    "#     files  = files[0:20] # Need to comment\n",
    "#     print(len(files))\n",
    "\n",
    "    sample_set = pd.DataFrame()\n",
    "\n",
    "    for file in files:\n",
    "        filename = os.path.basename(str(file))\n",
    "        file_name = filename[0:-4]\n",
    "\n",
    "        select_file = ascii_files[ascii_files.File_Name.values.astype(int) == int(file_name)]\n",
    "        sample_set = sample_set.append(select_file)\n",
    "    return sample_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Classification Pipeline\n",
    "- Split our whole dataset into 5 fold stratified splitting, that is, we have split our dataset into training set and test set. \n",
    "\n",
    "- Using the training set, we augment each layer depending on how much we want to augment the data in such a way that we have balanced classes. Augmentation is carried out by sampling from normal distribution assuming the mean of the gaussian to be the magnitude and the std to be the error on the magnitude, provided in the data.\n",
    "- Using both the augmented data and the real data, we extract 6 features and then use the period from the ascii-catalogue to assign it to their respective class and for augmented data, we randomly sample from the true distribution of the true period. Therefore, we have 7 features that best describe each variable stars.\n",
    "- For the test set, we have use only real example and features are extracted and true period from the ascii file is assigned to the respective class.\n",
    "- The training and testing set features are saved in separate files such that for each iteration (in our case, there are 5 iterations), they are saved inseparate folders.\n",
    "- For the classification, we then load the features, perform a normalisation and classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test,label,normalisation=True):\n",
    "    if normalisation:\n",
    "        scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "        X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "        y_train_label         = x_train.New_label\n",
    "        filename_train        = x_train.File_Name\n",
    "\n",
    "        X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "        y_test_label         = x_test[label]\n",
    "        filename_test        = x_test.File_Name\n",
    "        \n",
    "    else:\n",
    "        #scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "        X_train_normalisation = pd.DataFrame(x_train.iloc[:,0:nFeatures])\n",
    "        y_train_label         = x_train.New_label\n",
    "        filename_train        = x_train.File_Name\n",
    "\n",
    "        X_test_normalisation = pd.DataFrame(x_test.iloc[:,0:nFeatures])\n",
    "        y_test_label         = x_test[label]\n",
    "        filename_test        = x_test.File_Name\n",
    "        \n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(classifer, param_grid, n_iter, cv, X_train,y_train,filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    "    balance_accuracy = balanced_accuracy_score(y_test, ypred)\n",
    "    misclassified  = np.where(y_test != ypred)[0]\n",
    "    \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write(\"Balanced Accuracy: \"    + \"%f\" % float(balance_accuracy)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report using imabalanced metrics\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report_imbalanced(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()    \n",
    "    \n",
    "    return ypred, balance_accuracy, MCC, conf_mat, misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_misclassification(misclassified,y_test,test_data,ypred, save_dir=r'c:\\data\\np.txt'):\n",
    "    test_data['Prediction'] = ypred\n",
    "    new_DF = test_data.drop(test_data.index[misclassified]) # This dataset is the test set after removing the misclassification which are used in the next layer   \n",
    "    misclassified_data = test_data.iloc[misclassified] # Dataframe to store all misclassification\n",
    "    misclassified_data.to_csv(save_dir, sep=' ',index=None)\n",
    "    print('Test set has shape {}'.format(test_data.shape))\n",
    "    print('Misclassified data has shape {}'.format(misclassified_data.shape))\n",
    "    print('New test set has shape {}'.format(new_DF.shape))\n",
    "    return misclassified_data, new_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=18)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf',bbox_inches = 'tight',pad_inches = 0.1)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analysis_XGB(X_train, y_train, types,save_model=False,multi=False):\n",
    "    opt_parameters_XGB = hyperparameter_optimization(multi=multi)\n",
    "    fit_model = model_save(RandomForestClassifier(**opt_parameters_XGB), X_train=X_train, y_train=y_train,\\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model\n",
    "\n",
    "\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, testing_set, classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat, misclassified  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "\n",
    "    misclassified_data, new_DF = find_misclassification(misclassified,y_test, test_data=testing_set,ypred=ypred, save_dir=results_dir + types +'Misclassification_XGB.csv')\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    return ypred, accuracy, MCC, conf_mat, new_DF, misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Part: DATA AUGMENTATION, FEATURE EXTRACTION and CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION AND FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_preparation = False\n",
    "data_dir_train = './data/training_phase_lc/'\n",
    "data_dir_test  = './data/test_phase_lc/'\n",
    "\n",
    "# Loading the photometry data to get the filenames and their respective classes\n",
    "ascii_data  = pd.read_csv('../Ascii_SSS_Per_Table.txt',delim_whitespace=True,names = [\"SSS_ID\", \"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\", \"No_Name1\", 'No_Name2'])\n",
    "ascii_files = ascii_data[['File_Name', 'Type']] # Selecting only the filename and the type of stars\n",
    "\n",
    "'''\n",
    "Peforming a downsammpling of Type 5- We downsample class 5 from 18000 to 4509. And we remove class 11 and 13.\n",
    "'''\n",
    "\n",
    "type_11     = ascii_files[ascii_files.Type==true_class_11]\n",
    "type_13     = ascii_files[ascii_files.Type==true_class_13]\n",
    "\n",
    "ascii_files = ascii_files.drop(type_11.index)\n",
    "ascii_files = ascii_files.drop(type_13.index)\n",
    "ascii_files.shape\n",
    "\n",
    "\n",
    "period_data                   = ascii_data[['Period', 'File_Name', 'Type']]# Use for Test set Period\n",
    "periods                       = ascii_data[['File_Name', 'Period']] # Use for Test set Period\n",
    "update_ascii_period           = pd.DataFrame(periods.File_Name.astype(str) + '_2') # Use for Training set Period\n",
    "update_ascii_period['Period'] = periods.Period\n",
    "\n",
    "if data_preparation: \n",
    "    X_training = data_sets(data_dir_train,ascii_files)\n",
    "    X_testing  = data_sets(data_dir_test,ascii_files) \n",
    "\n",
    "    data_columns = ['magnitude', 'time']\n",
    "    features     = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "\n",
    "\n",
    "    RRab_train        = stars_label(X_training, true_class_1)\n",
    "    RRc_train         = stars_label(X_training, true_class_2) \n",
    "    RRd_train         = stars_label(X_training, true_class_3)\n",
    "    blazhko_train     = stars_label(X_training, true_class_4)\n",
    "    contact_Bi_train  = stars_label(X_training, true_class_5)\n",
    "    semi_det_Bi_train = stars_label(X_training, true_class_6)\n",
    "    rot_train         = stars_label(X_training, true_class_7)\n",
    "    LPV_train         = stars_label(X_training, true_class_8)\n",
    "    delta_scuti_train = stars_label(X_training, true_class_9)\n",
    "    ACEP_train        = stars_label(X_training, true_class_10)\n",
    "    cep_ii_train      = stars_label(X_training, true_class_12)\n",
    "\n",
    "    RRab_test        = stars_label(X_testing, true_class_1)\n",
    "    RRc_test         = stars_label(X_testing, true_class_2) \n",
    "    RRd_test         = stars_label(X_testing, true_class_3)\n",
    "    blazhko_test     = stars_label(X_testing, true_class_4)\n",
    "    contact_Bi_test  = stars_label(X_testing, true_class_5)\n",
    "    semi_det_Bi_test = stars_label(X_testing, true_class_6)\n",
    "    rot_test         = stars_label(X_testing, true_class_7)\n",
    "    LPV_test         = stars_label(X_testing, true_class_8)\n",
    "    delta_scuti_test = stars_label(X_testing, true_class_9)\n",
    "    ACEP_test        = stars_label(X_testing, true_class_10)\n",
    "    cep_ii_test      = stars_label(X_testing, true_class_12)\n",
    "       \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                                    # FIRST LAYER\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    training_data_FL, testing_data_FL, y_FL_training_counts = first_layer()\n",
    "\n",
    "    # This part is calculating the number of times each class need to be augmented    \n",
    "    ns_FL = num_augmentation(nAugmentation=17000, y_training_counts=y_FL_training_counts)#17000\n",
    "    print(ns_FL)\n",
    "\n",
    "\n",
    "    # Each class is augmented using their respective number of samples and features are extracted\n",
    "    augmentation_data, feature_file = augmentation_and_featureExtraction(data_dir=data_dir_train, \\\n",
    "                                          data_ = training_data_FL,data_columns=data_columns,\\\n",
    "                                          features=features,update_ascii_period=update_ascii_period,\\\n",
    "                                          number_of_samples=ns_FL,save_folder_training = './data/GP/HC/layer1_EclRotPul/training_set/')\n",
    "\n",
    "    # Features from each class from the test set are extracted and save in a single \n",
    "    feature_file_testSet    = feature_extraction_test_set(data_dir=data_dir_test,\\\n",
    "                                  X_testing=testing_data_FL,data_columns=data_columns,\\\n",
    "                                  features=features,save_folder_test= './data/GP/HC/layer1_EclRotPul/test_set/')\n",
    "\n",
    "    '-------------------------------------------------------------------------------'\n",
    "                            # SECOND LAYER ECLIPSING BINARY\n",
    "    '-------------------------------------------------------------------------------'\n",
    "    training_data_SL_EB, testing_data_SL_EB, y_SL_EB_training_counts = second_layer_EB()\n",
    "\n",
    "    # This part is calculating the number of times each class need to be augmented    \n",
    "    ns_SL_EB = num_augmentation(nAugmentation=10000, y_training_counts=y_SL_EB_training_counts)#10000\n",
    "    print(ns_SL_EB)\n",
    "\n",
    "\n",
    "    # Each class is augmented using their respective number of samples and features are extracted\n",
    "    augmentation_data_SL_EB, feature_file_SL_EB = augmentation_and_featureExtraction(data_dir=data_dir_train, \\\n",
    "                                          data_ = training_data_SL_EB,data_columns=data_columns,\\\n",
    "                                          features=features,update_ascii_period=update_ascii_period,\\\n",
    "                                          number_of_samples=ns_SL_EB,save_folder_training = './data/GP/HC/layer2_EB/training_set/')\n",
    "\n",
    "    # Features from each class from the test set are extracted and save in a single \n",
    "    feature_file_testSet_SL_EB    = feature_extraction_test_set(data_dir=data_dir_test,\\\n",
    "                                  X_testing=testing_data_SL_EB,data_columns=data_columns,\\\n",
    "                                  features=features,save_folder_test= './data/GP/HC/layer2_EB/test_set/')\n",
    "\n",
    "    '-----------------------------------------------------------------------------'\n",
    "                        # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    training_data_SL_RLCD, testing_data_SL_RLCD, y_SL_RLCD_training_counts = second_layer_RLCD()\n",
    "\n",
    "\n",
    "    # This part is calculating the number of times each class need to be augmented    \n",
    "    ns_SL_RLCD = num_augmentation(nAugmentation=17000, y_training_counts=y_SL_RLCD_training_counts)#17000\n",
    "    print(ns_SL_RLCD)\n",
    "\n",
    "\n",
    "    # Each class is augmented using their respective number of samples and features are extracted\n",
    "    augmentation_data_SL_RLCD, feature_file_SL_RLCD = augmentation_and_featureExtraction(data_dir=data_dir_train, \\\n",
    "                                          data_ = training_data_SL_RLCD,data_columns=data_columns,\\\n",
    "                                          features=features,update_ascii_period=update_ascii_period,\\\n",
    "                                          number_of_samples=ns_SL_RLCD,save_folder_training = './data/GP/HC/layer2_RLCD/training_set/')\n",
    "\n",
    "    # Features from each class from the test set are extracted and save in a single \n",
    "    feature_file_testSet_SL_RLCD    = feature_extraction_test_set(data_dir=data_dir_test,\\\n",
    "                                  X_testing=testing_data_SL_RLCD,data_columns=data_columns,\\\n",
    "                                  features=features,save_folder_test= './data/GP/HC/layer2_RLCD/test_set/')\n",
    "\n",
    "    '-----------------------------------------------------------------------------'\n",
    "                    # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    training_data_TL_RRLyrae, testing_data_TL_RRLyrae, y_TL_RRLyrae_training_counts = third_layer_RRLyrae()\n",
    "\n",
    "    # This part is calculating the number of times each class need to be augmented    \n",
    "    ns_TL_RRLyrae = num_augmentation(nAugmentation=10000, y_training_counts=y_TL_RRLyrae_training_counts)#10000\n",
    "    print(ns_TL_RRLyrae)\n",
    "\n",
    "\n",
    "    # Each class is augmented using their respective number of samples and features are extracted\n",
    "    augmentation_data_TL_RRLyrae, feature_file_TL_RRLyrae = augmentation_and_featureExtraction(data_dir=data_dir_train, \\\n",
    "                                          data_ = training_data_TL_RRLyrae,data_columns=data_columns,\\\n",
    "                                          features=features,update_ascii_period=update_ascii_period,\\\n",
    "                                          number_of_samples=ns_TL_RRLyrae,save_folder_training = './data/GP/HC/layer3_RRLyrae/training_set/')\n",
    "\n",
    "    # Features from each class from the test set are extracted and save in a single \n",
    "    feature_file_testSet_TL_RRLyrae    = feature_extraction_test_set(data_dir=data_dir_test,\\\n",
    "                                  X_testing=testing_data_TL_RRLyrae,data_columns=data_columns,\\\n",
    "                                  features=features,save_folder_test= './data/GP/HC/layer3_RRLyrae/test_set/')\n",
    "\n",
    "    '-----------------------------------------------------------------------------'\n",
    "                            # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    training_data_TL_cep, testing_data_TL_cep, y_TL_cep_training_counts = third_layer_Cepheids()\n",
    "\n",
    "    # This part is calculating the number of times each class need to be augmented    \n",
    "    ns_TL_cep = num_augmentation(nAugmentation=5000, y_training_counts=y_TL_cep_training_counts)#5000\n",
    "    print(ns_TL_cep)\n",
    "\n",
    "\n",
    "    # Each class is augmented using their respective number of samples and features are extracted\n",
    "    augmentation_data_TL_cep, feature_file_TL_cep = augmentation_and_featureExtraction(data_dir=data_dir_train, \\\n",
    "                                          data_ = training_data_TL_cep,data_columns=data_columns,\\\n",
    "                                          features=features,update_ascii_period=update_ascii_period,\\\n",
    "                                          number_of_samples=ns_TL_cep,save_folder_training = './data/GP/HC/layer3_Cepheids/training_set/')\n",
    "\n",
    "    # Features from each class from the test set are extracted and save in a single \n",
    "    feature_file_testSet_TL_cep    = feature_extraction_test_set(data_dir=data_dir_test,\\\n",
    "                                  X_testing=testing_data_TL_cep,data_columns=data_columns,\\\n",
    "                                  features=features,save_folder_test= './data/GP/HC/layer3_Cepheids/test_set/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.835 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.08, 'max_depth': 11}\n",
      "Accuracy 0.835 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 11}\n",
      "Accuracy 0.835 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 11}\n",
      "Accuracy 0.827 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.0, 'max_depth': 8}\n",
      "Accuracy 0.832 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 10}\n",
      "Accuracy 0.832 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.07, 'max_depth': 11}\n",
      "Accuracy 0.829 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.08, 'max_depth': 9}\n",
      "Accuracy 0.838 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 12}\n",
      "Accuracy 0.831 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 9}\n",
      "Accuracy 0.822 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.0, 'max_depth': 7}\n",
      "Accuracy 0.819 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 6}\n",
      "Accuracy 0.827 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 8}\n",
      "Accuracy 0.839 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 14}\n",
      "Accuracy 0.783 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.02, 'max_depth': 2}\n",
      "Accuracy 0.830 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.01, 'max_depth': 10}\n",
      "Accuracy 0.798 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.07, 'max_depth': 3}\n",
      "Accuracy 0.839 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.01, 'max_depth': 14}\n",
      "Accuracy 0.836 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.09, 'max_depth': 11}\n",
      "Accuracy 0.797 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 3}\n",
      "Accuracy 0.784 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.07, 'max_depth': 2}\n",
      "100%|██████████| 20/20 [41:45<00:00, 94.93s/it, best loss: 0.161039215686] \n",
      "{'eta': 0.07, 'max_depth': 13, 'x_subsample': 0.7000000000000001}\n",
      "{'eta': 0.07, 'max_depth': 13, 'x_subsample': 0.7000000000000001}\n",
      "Test set has shape (21242, 11)\n",
      "Misclassified data has shape (5537, 11)\n",
      "New test set has shape (15705, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.71864706  0.18388235  0.09747059]\n",
      " [ 0.17415215  0.67461045  0.1512374 ]\n",
      " [ 0.07775309  0.04887337  0.87337353]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b51ca50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_class  = True\n",
    "n_splits = 1\n",
    "save_model=False\n",
    "load_model=False\n",
    "acc_xgb_FL = [];mcc_xgb_FL = [];\\\n",
    "acc_xgb_SL_EB = [];mcc_xgb_SL_EB = [];\\\n",
    "acc_xgb_SL_RLCD = [];mcc_xgb_SL_RLCD = [];\\\n",
    "acc_xgb_TL_RL = [];mcc_xgb_TL_RL = [];\\\n",
    "acc_xgb_TL_Cep = [];mcc_xgb_TL_Cep = []\n",
    "\n",
    "\n",
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                                # FIRST LAYER\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_FL = 17000#14000\n",
    "    training_dir_FL      = './data/GP/HC/layer1_EclRotPul/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir_FL       = './data/GP/HC/layer1_EclRotPul/test_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_FL  = pd.read_csv(training_dir_FL+'Training_features.csv',sep=',')\n",
    "    all_testing_set_FL   = pd.read_csv(testing_dir_FL+'Test_features.csv',sep=',')\n",
    "        \n",
    "    eclipsing_class  = all_training_set_FL[all_training_set_FL.New_label==eclipsing_label].sample(n=num_examples_FL)\n",
    "    rotational_class = all_training_set_FL[all_training_set_FL.New_label==rotational_label].sample(n=num_examples_FL)\n",
    "    pulsating_class  = all_training_set_FL[all_training_set_FL.New_label==pulsating_label].sample(n=num_examples_FL)\n",
    "\n",
    "    if multi_class:\n",
    "        training_set_FL = pd.concat([eclipsing_class,rotational_class,pulsating_class], axis=0)   \n",
    "        testing_set_FL = all_testing_set_FL\n",
    "    else:\n",
    "        training_set_FL = pd.concat([eclipsing_class,pulsating_class], axis=0) \n",
    "        testing_set_FL = all_testing_set_FL\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_FL, y_train_FL, filename_train_np_FL, X_test_FL, y_test_FL, filename_test_np_FL = normalisation(training_set_FL,testing_set_FL,label='New_label',normalisation=False)\n",
    "\n",
    "    X_tr = X_train_FL; y_tr=y_train_FL\n",
    "    \n",
    "    def objectives_xgb(space):\n",
    "        classifier = RandomForestClassifier(class_weight = space['class_weight'],n_estimators=space['n_estimators'],\\\n",
    "                                   min_samples_split = space['min_samples_split'])\n",
    "\n",
    "    #     classifier = XGBClassifier(max_depth=space['max_depth'])\n",
    "\n",
    "        classifier.fit(X_tr, y_tr)\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "        CrossValMean = accuracies.mean()\n",
    "\n",
    "        print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "        return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "    def hyperparameter_optimization(multi=False):\n",
    "\n",
    "        class_weight = 'balanced'\n",
    "\n",
    "        space ={'class_weight':class_weight,\\\n",
    "                'n_estimators': hp.choice('n_estimators', np.arange(300,500,dtype=int)),\\\n",
    "                'min_samples_split': hp.choice('min_samples_split',np.arange(1,20,dtype=int))}\n",
    "\n",
    "    #     space ={'objective':objective,'max_depth':hp.choice('max_depth', np.arange(1,13,dtype=int))}\n",
    "\n",
    "        trials         = Trials()\n",
    "        opt_parameters = fmin(fn=objectives_xgb,space=space,algo=tpe.suggest,max_evals=20,trials=trials)\n",
    "        print(opt_parameters)\n",
    "\n",
    "        return opt_parameters\n",
    "\n",
    "    # Random Forest Classifier    \n",
    "    if multi_class:\n",
    "        classes_types_FL = ['Eclipsing','Rotational','Pulsating']\n",
    "        types_FL         ='Type_FL_'+str(split_num)\n",
    "        nClasses_FL      = len(classes_types_FL)\n",
    "        \n",
    "    else:\n",
    "        classes_types_FL = ['Eclipsing','Rotational']\n",
    "        types_FL         ='Type_Binary_Split_'+str(split_num)\n",
    "        nClasses_FL      = 2\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_FL, fit_model_xgb_FL = analysis_XGB(X_train_FL, y_train_FL, types_FL, save_model,multi=True) # This part can be commented when no training\n",
    "    print(opt_xgb_FL)    \n",
    "    ypred_xgb_FL, accuracy_xgb_FL, MCC_xgb_FL, conf_mat_xgb_FL, new_DF_xgb_FL,misclassified_xgb_FL = final_prediction_XGB(fit_model_xgb_FL, X_train_FL, y_train_FL, X_test_FL, y_test_FL, testing_set_FL, classes_types_FL, types_FL, nClasses_FL, load_model) \n",
    "    \n",
    "    acc_xgb_FL.append(accuracy_xgb_FL)\n",
    "    mcc_xgb_FL.append(MCC_xgb_FL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.942 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.0, 'max_depth': 12}\n",
      "Accuracy 0.940 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.03, 'max_depth': 7}\n",
      "Accuracy 0.940 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 14}\n",
      "Accuracy 0.940 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 8}\n",
      "Accuracy 0.941 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 9}\n",
      "Accuracy 0.940 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.09, 'max_depth': 11}\n",
      "Accuracy 0.925 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 1}\n",
      "Accuracy 0.933 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 3}\n",
      "Accuracy 0.935 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.930 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.0, 'max_depth': 2}\n",
      "Accuracy 0.940 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.06, 'max_depth': 13}\n",
      "Accuracy 0.937 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 5}\n",
      "Accuracy 0.939 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.05, 'max_depth': 11}\n",
      "Accuracy 0.936 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.05, 'max_depth': 4}\n",
      "Accuracy 0.941 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.06, 'max_depth': 14}\n",
      "Accuracy 0.933 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.05, 'max_depth': 3}\n",
      "Accuracy 0.938 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 7}\n",
      "Accuracy 0.942 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 10}\n",
      "Accuracy 0.933 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 3}\n",
      "Accuracy 0.941 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 12}\n",
      "100%|██████████| 20/20 [03:29<00:00, 10.78s/it, best loss: 0.0579375]\n",
      "{'eta': 0.0, 'max_depth': 11, 'x_subsample': 0.7000000000000001}\n",
      "Test set has shape (12217, 11)\n",
      "Misclassified data has shape (1105, 11)\n",
      "New test set has shape (11112, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.90409325  0.09590675]\n",
      " [ 0.04542014  0.95457986]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12583e450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    '-------------------------------------------------------------------------------'\n",
    "                                # SECOND LAYER ECLIPSING BINARY\n",
    "    '-------------------------------------------------------------------------------'\n",
    "    num_examples_SL_EB     = 8000#7200\n",
    "    training_dir_SL_EB     = './data/GP/HC/layer2_EB/training_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_SL_EB = pd.read_csv(training_dir_SL_EB+'Training_features.csv',sep=',')\n",
    "    all_testing_set_SL_EB  = new_DF_xgb_FL\n",
    "        \n",
    "    ecl_class  = all_training_set_SL_EB[all_training_set_SL_EB.New_label==true_class_5].sample(n=num_examples_SL_EB)\n",
    "    EA_class   = all_training_set_SL_EB[all_training_set_SL_EB.New_label==true_class_6].sample(n=num_examples_SL_EB)\n",
    "\n",
    "    ecl_class_test = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_5]\n",
    "    EA_class_test  = all_testing_set_SL_EB[all_testing_set_SL_EB.True_class_labels==true_class_6]\n",
    "    \n",
    "    training_set_SL_EB = pd.concat([ecl_class,EA_class], axis=0) \n",
    "    testing_set_SL_EB  = pd.concat([ecl_class_test,EA_class_test], axis=0)\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_SL_EB, y_train_SL_EB, filename_train_np_SL_EB, X_test_SL_EB, y_test_SL_EB, filename_test_np_SL_EB = normalisation(training_set_SL_EB,testing_set_SL_EB,label='True_class_labels',normalisation=False)\n",
    "\n",
    "    X_tr = X_train_SL_EB; y_tr=y_train_SL_EB\n",
    "\n",
    "    def objectives_xgb(space):\n",
    "    #     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "        classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                                   subsample = space['subsample'])\n",
    "\n",
    "        classifier.fit(X_tr, y_tr)\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "        CrossValMean = accuracies.mean()\n",
    "\n",
    "        print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "        return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "    \n",
    "    classes_types_SL_EB = ['Ecl','EA']\n",
    "    types_SL_EB         ='Type_SL_Ecl_EA_'+str(split_num)\n",
    "    nClasses_SL_EB      = 2\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_SL_EB, fit_model_xgb_SL_EB = analysis_XGB(X_train_SL_EB, y_train_SL_EB, types_SL_EB, save_model,multi=False) # This part can be commented when no training  \n",
    "    ypred_xgb_SL_EB, accuracy_xgb_SL_EB, MCC_xgb_SL_EB, conf_mat_xgb_SL_EB, new_DF_xgb_SL_EB, misclassified_xgb_SL_EB = final_prediction_XGB(fit_model_xgb_SL_EB, X_train_SL_EB, y_train_SL_EB, X_test_SL_EB, y_test_SL_EB,testing_set_SL_EB, classes_types_SL_EB, types_SL_EB, nClasses_SL_EB, load_model) \n",
    "    \n",
    "    acc_xgb_SL_EB.append(accuracy_xgb_SL_EB)\n",
    "    mcc_xgb_SL_EB.append(MCC_xgb_SL_EB)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.997 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.01, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 8}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 6}\n",
      "Accuracy 0.997 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.05, 'max_depth': 10}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 7}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 6}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.05, 'max_depth': 7}\n",
      "Accuracy 0.990 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 2}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.08, 'max_depth': 9}\n",
      "Accuracy 0.982 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.07, 'max_depth': 1}\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.08, 'max_depth': 5}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.06, 'max_depth': 10}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.08, 'max_depth': 11}\n",
      "Accuracy 0.995 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 4}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.07, 'max_depth': 6}\n",
      "Accuracy 0.996 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 13}\n",
      "100%|██████████| 20/20 [39:47<00:00, 122.87s/it, best loss: 0.00347122302158]\n",
      "{'eta': 0.06, 'max_depth': 12, 'x_subsample': 0.8}\n",
      "Test set has shape (2752, 11)\n",
      "Misclassified data has shape (33, 11)\n",
      "New test set has shape (2719, 11)\n",
      "Normalized confusion matrix\n",
      "[[  9.89890110e-01   0.00000000e+00   8.79120879e-04   9.23076923e-03]\n",
      " [  0.00000000e+00   9.97326203e-01   0.00000000e+00   2.67379679e-03]\n",
      " [  2.22222222e-02   0.00000000e+00   9.77777778e-01   0.00000000e+00]\n",
      " [  1.37931034e-01   0.00000000e+00   0.00000000e+00   8.62068966e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e08e490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    '-----------------------------------------------------------------------------'\n",
    "                        # SECOND LAYER RR LYRAE PULSATING LPV CEPHEIDS\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_SL_RLCD     = 13900\n",
    "    training_dir_SL_RLCD     = './data/GP/HC/layer2_RLCD/training_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_SL_RLCD = pd.read_csv(training_dir_SL_RLCD+'Training_features.csv',sep=',')\n",
    "        \n",
    "    RR_Lyrae_class = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==RR_Lyrae_label].sample(n=num_examples_SL_RLCD)\n",
    "    LPV_class      = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==LPV_label].sample(n=num_examples_SL_RLCD)\n",
    "    Cep_class      = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==cepheids_label].sample(n=num_examples_SL_RLCD)\n",
    "    ds_class       = all_training_set_SL_RLCD[all_training_set_SL_RLCD.New_label==delta_scuti_label].sample(n=num_examples_SL_RLCD)\n",
    "\n",
    "    all_testing_set_SL_RLCD  = new_DF_xgb_FL.drop(['New_label', 'Prediction'],axis=1)\n",
    "    RRab_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_1]\n",
    "    RRc_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_2]\n",
    "    RRd_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_3]\n",
    "    blazhko_test             = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_4]\n",
    "    LPV_test                 = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_8]\n",
    "    ACEP_test                = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_10]\n",
    "    cep_ii_test              = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_12]\n",
    "    delta_scuti_test         = all_testing_set_SL_RLCD[all_testing_set_SL_RLCD.True_class_labels==true_class_9]\n",
    "\n",
    "    RR_Lyrae_test       = pd.concat([RRab_test,RRc_test,RRd_test,blazhko_test], axis=0)\n",
    "    RR_Lyrae_test_class = np.full(len(RR_Lyrae_test), RR_Lyrae_label, dtype=int)\n",
    "    LPV_test_class      = np.full(len(LPV_test), LPV_label, dtype=int)\n",
    "    cepheids_test       = pd.concat([ACEP_test, cep_ii_test] ,axis=0)\n",
    "    cepheids_test_class = np.full(len(cepheids_test), cepheids_label, dtype=int)\n",
    "    ds_test             = delta_scuti_test\n",
    "    ds_test_class       = np.full(len(ds_test), delta_scuti_label, dtype=int)\n",
    "\n",
    "    second_layer_RLCD_test       = pd.concat([RR_Lyrae_test,LPV_test,cepheids_test,ds_test], axis=0)\n",
    "    second_layer_RLCD_test_class = np.concatenate((RR_Lyrae_test_class,LPV_test_class,cepheids_test_class,ds_test_class), axis=0)\n",
    "    testing_data_SL_RLCD         = pd.DataFrame(second_layer_RLCD_test)\n",
    "    testing_data_SL_RLCD['New_label'] = second_layer_RLCD_test_class\n",
    "    \n",
    "    training_set_SL_RLCD = pd.concat([RR_Lyrae_class,LPV_class,Cep_class,ds_class], axis=0) \n",
    "    testing_set_SL_RLCD  = testing_data_SL_RLCD\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_SL_RLCD, y_train_SL_RLCD, filename_train_np_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD, filename_test_np_SL_RLCD = normalisation(training_set_SL_RLCD,testing_set_SL_RLCD, label='New_label',normalisation=False)\n",
    "\n",
    "    X_tr = X_train_SL_RLCD; y_tr=y_train_SL_RLCD\n",
    "\n",
    "    def objectives_xgb(space):\n",
    "    #     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "        classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                                   subsample = space['subsample'])\n",
    "\n",
    "        classifier.fit(X_tr, y_tr)\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "        CrossValMean = accuracies.mean()\n",
    "\n",
    "        print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "        return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "    \n",
    "    classes_types_SL_RLCD = ['RR Lyrae','LPV', 'Cepheids', '$\\delta$-Scuti']\n",
    "    types_SL_RLCD         ='Type_SL_RLCD_'+str(split_num)\n",
    "    nClasses_SL_RLCD      = len(classes_types_SL_RLCD)\n",
    "    \n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_SL_RLCD, fit_model_xgb_SL_RLCD = analysis_XGB(X_train_SL_RLCD, y_train_SL_RLCD, types_SL_RLCD, save_model,multi=True) # This part can be commented when no training   \n",
    "    ypred_xgb_SL_RLCD, accuracy_xgb_SL_RLCD, MCC_xgb_SL_RLCD, conf_mat_xgb_SL_RLCD, new_DF_xgb_SL_RLCD, misclassified_xgb_SL_RLCD = final_prediction_XGB(fit_model_xgb_SL_RLCD, X_train_SL_RLCD, y_train_SL_RLCD, X_test_SL_RLCD, y_test_SL_RLCD,testing_set_SL_RLCD, classes_types_SL_RLCD, types_SL_RLCD, nClasses_SL_RLCD, load_model) \n",
    "    \n",
    "    acc_xgb_SL_RLCD.append(accuracy_xgb_SL_RLCD)\n",
    "    mcc_xgb_SL_RLCD.append(MCC_xgb_SL_RLCD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.880 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.09, 'max_depth': 5}\n",
      "Accuracy 0.879 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 5}\n",
      "Accuracy 0.949 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 14}\n",
      "Accuracy 0.786 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 1}\n",
      "Accuracy 0.947 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.09, 'max_depth': 13}\n",
      "Accuracy 0.822 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.03, 'max_depth': 2}\n",
      "Accuracy 0.944 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.06, 'max_depth': 11}\n",
      "Accuracy 0.855 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 4}\n",
      "Accuracy 0.938 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.09, 'max_depth': 10}\n",
      "Accuracy 0.883 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.0, 'max_depth': 5}\n",
      "Accuracy 0.902 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.09, 'max_depth': 6}\n",
      "Accuracy 0.902 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 6}\n",
      "Accuracy 0.917 params {'objective': 'multi:softmax', 'subsample': 0.4, 'eta': 0.04, 'max_depth': 7}\n",
      "Accuracy 0.928 params {'objective': 'multi:softmax', 'subsample': 0.8, 'eta': 0.05, 'max_depth': 8}\n",
      "Accuracy 0.901 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.01, 'max_depth': 6}\n",
      "Accuracy 0.902 params {'objective': 'multi:softmax', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 6}\n",
      "Accuracy 0.901 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 6}\n",
      "Accuracy 0.944 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 11}\n",
      "Accuracy 0.949 params {'objective': 'multi:softmax', 'subsample': 0.7000000000000001, 'eta': 0.01, 'max_depth': 14}\n",
      "Accuracy 0.822 params {'objective': 'multi:softmax', 'subsample': 0.6000000000000001, 'eta': 0.05, 'max_depth': 2}\n",
      "100%|██████████| 20/20 [33:02<00:00, 94.66s/it, best loss: 0.05075] \n",
      "{'eta': 0.03, 'max_depth': 13, 'x_subsample': 0.7000000000000001}\n",
      "{'eta': 0.03, 'max_depth': 13, 'x_subsample': 0.7000000000000001}\n",
      "Test set has shape (2252, 11)\n",
      "Misclassified data has shape (205, 11)\n",
      "New test set has shape (2047, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.97459016  0.          0.00163934  0.02377049]\n",
      " [ 0.          0.94779582  0.05220418  0.        ]\n",
      " [ 0.07563025  0.64705882  0.27731092  0.        ]\n",
      " [ 0.84313725  0.          0.          0.15686275]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e08e510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    \n",
    "    '-----------------------------------------------------------------------------'\n",
    "                    # THIRD LAYER RR LYRAE: RRab, RRc, RRd, Blazhko\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_TL_RL     = 10000#6900\n",
    "    training_dir_TL_RL     = './data/GP/HC/layer3_RRLyrae/training_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_TL_RL = pd.read_csv(training_dir_TL_RL+'Training_features.csv',sep=',')\n",
    "    all_testing_set_TL_RL  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "        \n",
    "    RRab_class    = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_1].sample(n=num_examples_TL_RL)\n",
    "    RRc_class     = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_2].sample(n=num_examples_TL_RL)\n",
    "    RRd_class     = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_3].sample(n=num_examples_TL_RL)\n",
    "    blazhko_class = all_training_set_TL_RL[all_training_set_TL_RL.New_label==true_class_4].sample(n=num_examples_TL_RL)\n",
    "\n",
    "    RRab_class_test    = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_1]\n",
    "    RRc_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_2]\n",
    "    RRd_class_test     = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_3]\n",
    "    blazhko_class_test = all_testing_set_TL_RL[all_testing_set_TL_RL.True_class_labels==true_class_4]\n",
    "   \n",
    "    training_set_TL_RL = pd.concat([RRab_class,RRc_class,RRd_class,blazhko_class], axis=0) \n",
    "    testing_set_TL_RL  = pd.concat([RRab_class_test,RRc_class_test,RRd_class_test,blazhko_class_test], axis=0)\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_TL_RL, y_train_TL_RL, filename_train_np_TL_RL, X_test_TL_RL, y_test_TL_RL, filename_test_np_TL_RL = normalisation(training_set_TL_RL,testing_set_TL_RL,label='True_class_labels',normalisation=False)\n",
    "\n",
    "    X_tr = X_train_TL_RL; y_tr=y_train_TL_RL\n",
    "\n",
    "    def objectives_xgb(space):\n",
    "    #     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "        classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                                   subsample = space['subsample'])\n",
    "\n",
    "        classifier.fit(X_tr, y_tr)\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "        CrossValMean = accuracies.mean()\n",
    "\n",
    "        print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "        return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "    \n",
    "    # Random Forest Classifier    \n",
    "    classes_types_TL_RL = ['RRab', 'RRc', 'RRd', \"Blazhko\"]\n",
    "    types_TL_RL         ='Type_TL_RRLyrae_'+str(split_num)\n",
    "    nClasses_TL_RL      = len(classes_types_TL_RL)\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_TL_RL, fit_model_xgb_TL_RL = analysis_XGB(X_train_TL_RL, y_train_TL_RL, types_TL_RL, save_model,multi=True) # This part can be commented when no training\n",
    "    print(opt_xgb_TL_RL)    \n",
    "    ypred_xgb_TL_RL, accuracy_xgb_TL_RL, MCC_xgb_TL_RL, conf_mat_xgb_TL_RL,new_DF_xgb_TL_RL, misclassified_xgb_TL_RL = final_prediction_XGB(fit_model_xgb_TL_RL, X_train_TL_RL, y_train_TL_RL, X_test_TL_RL, y_test_TL_RL,testing_set_TL_RL, classes_types_TL_RL, types_TL_RL, nClasses_TL_RL, load_model) \n",
    "    \n",
    "    acc_xgb_TL_RL.append(accuracy_xgb_TL_RL)\n",
    "    mcc_xgb_TL_RL.append(MCC_xgb_TL_RL)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.04, 'max_depth': 5}\n",
      "Accuracy 0.952 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.02, 'max_depth': 2}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.03, 'max_depth': 10}\n",
      "Accuracy 0.993 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.01, 'max_depth': 4}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.02, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.09, 'max_depth': 13}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.09, 'max_depth': 11}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.02, 'max_depth': 5}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.07, 'max_depth': 12}\n",
      "Accuracy 0.980 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.04, 'max_depth': 3}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.0, 'max_depth': 9}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.05, 'max_depth': 6}\n",
      "Accuracy 0.948 params {'objective': 'binary:logistic', 'subsample': 0.7000000000000001, 'eta': 0.09, 'max_depth': 2}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.04, 'max_depth': 11}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.03, 'max_depth': 5}\n",
      "Accuracy 0.951 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 2}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.5, 'eta': 0.01, 'max_depth': 11}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.4, 'eta': 0.09, 'max_depth': 7}\n",
      "Accuracy 0.996 params {'objective': 'binary:logistic', 'subsample': 0.8, 'eta': 0.01, 'max_depth': 13}\n",
      "Accuracy 0.997 params {'objective': 'binary:logistic', 'subsample': 0.6000000000000001, 'eta': 0.03, 'max_depth': 6}\n",
      "100%|██████████| 20/20 [01:45<00:00,  5.53s/it, best loss: 0.0034]\n",
      "{'eta': 0.03, 'max_depth': 5, 'x_subsample': 0.6000000000000001}\n",
      "Test set has shape (50, 11)\n",
      "Misclassified data has shape (6, 11)\n",
      "New test set has shape (44, 11)\n",
      "Normalized confusion matrix\n",
      "[[ 0.9375      0.0625    ]\n",
      " [ 0.22222222  0.77777778]]\n",
      "--------------------------------------------------\n",
      "Classification for Split 1 is finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a028710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "    '-----------------------------------------------------------------------------'\n",
    "                                # THIRD LAYER Cepheids: ACEP and Cep-II\n",
    "    '-----------------------------------------------------------------------------'\n",
    "    num_examples_TL_Cep     = 5000#4800\n",
    "    training_dir_TL_Cep     = './data/GP/HC/layer3_Cepheids/training_set/Split_'+str(split_num)+'/'\n",
    "    all_training_set_TL_Cep = pd.read_csv(training_dir_TL_Cep+'Training_features.csv',sep=',')\n",
    "    all_testing_set_TL_Cep  = new_DF_xgb_SL_RLCD.drop([\"Prediction\"],axis=1)\n",
    "        \n",
    "    ACEP_class   = all_training_set_TL_Cep[all_training_set_TL_Cep.New_label==true_class_10].sample(n=num_examples_TL_Cep)\n",
    "    Cep_ii_class = all_training_set_TL_Cep[all_training_set_TL_Cep.New_label==true_class_12].sample(n=num_examples_TL_Cep)\n",
    "\n",
    "    ACEP_class_test   = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_10]\n",
    "    Cep_ii_class_test = all_testing_set_TL_Cep[all_testing_set_TL_Cep.True_class_labels==true_class_12]\n",
    "\n",
    "    \n",
    "    training_set_TL_Cep = pd.concat([ACEP_class,Cep_ii_class], axis=0) \n",
    "    testing_set_TL_Cep  = pd.concat([ACEP_class_test,Cep_ii_class_test], axis=0)\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train_TL_Cep, y_train_TL_Cep, filename_train_np_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, filename_test_np_TL_Cep = normalisation(training_set_TL_Cep,testing_set_TL_Cep, label='True_class_labels',normalisation=False)\n",
    "\n",
    "    X_tr = X_train_TL_Cep; y_tr=y_train_TL_Cep\n",
    "\n",
    "    def objectives_xgb(space):\n",
    "    #     classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'])\n",
    "\n",
    "        classifier = XGBClassifier(objective = space['objective'],max_depth=space['max_depth'],eta = space['eta'],\\\n",
    "                                   subsample = space['subsample'])\n",
    "\n",
    "        classifier.fit(X_tr, y_tr)\n",
    "\n",
    "        accuracies = cross_val_score(estimator=classifier, X=X_tr, y=y_tr, cv=StratifiedKFold(n_splits=5),scoring='balanced_accuracy')\n",
    "        CrossValMean = accuracies.mean()\n",
    "\n",
    "        print(\"Accuracy {:.3f} params {}\".format(CrossValMean, space))\n",
    "\n",
    "        return{'loss':1-CrossValMean, 'status': STATUS_OK }\n",
    "\n",
    "    \n",
    "    classes_types_TL_Cep = ['ACEP','CEP-II']\n",
    "    types_TL_Cep         ='Type_TL_Cepheids_'+str(split_num)\n",
    "    nClasses_TL_Cep      = 2\n",
    "\n",
    "    # XGBoost Classifier   \n",
    "    opt_xgb_TL_Cep, fit_model_xgb_TL_Cep = analysis_XGB(X_train_TL_Cep, y_train_TL_Cep, types_TL_Cep, save_model,multi=False) # This part can be commented when no training   \n",
    "    ypred_xgb_TL_Cep, accuracy_xgb_TL_Cep, MCC_xgb_TL_Cep, conf_mat_xgb_TL_Cep, new_DF_xgb_TL_Cep, misclassified_xgb_TL_Cep = final_prediction_XGB(fit_model_xgb_TL_Cep, X_train_TL_Cep, y_train_TL_Cep, X_test_TL_Cep, y_test_TL_Cep, testing_set_TL_Cep, classes_types_TL_Cep, types_TL_Cep, nClasses_TL_Cep, load_model) \n",
    "    \n",
    "    acc_xgb_TL_Cep.append(accuracy_xgb_TL_Cep)\n",
    "    mcc_xgb_TL_Cep.append(MCC_xgb_TL_Cep)\n",
    "        \n",
    "    print('-'*50)\n",
    "    print('Classification for Split {} is finished'.format(split_num))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = open(\"./hierarchical-results_gp/metrics.txt\", 'w')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_FL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_FL)*100,np.std(acc_xgb_FL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_FL)*100,np.std(mcc_xgb_FL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_EB) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_EB)*100,np.std(acc_xgb_SL_EB)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_EB)*100,np.std(mcc_xgb_SL_EB)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_SL_RLCD) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_SL_RLCD)*100,np.std(acc_xgb_SL_RLCD)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_SL_RLCD)*100,np.std(mcc_xgb_SL_RLCD)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_RL) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_RL)*100,np.std(acc_xgb_TL_RL)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_RL)*100,np.std(mcc_xgb_TL_RL)) + '\\n')\n",
    "\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types_TL_Cep) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb_TL_Cep)*100,np.std(acc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb_TL_Cep)*100,np.std(mcc_xgb_TL_Cep)) + '\\n')\n",
    "metrics.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
