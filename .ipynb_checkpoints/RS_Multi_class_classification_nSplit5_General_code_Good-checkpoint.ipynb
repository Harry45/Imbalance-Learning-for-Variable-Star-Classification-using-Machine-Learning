{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pickle\n",
    "#plt.switch_backend('agg')\n",
    "% matplotlib inline\n",
    "\n",
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import FATS\n",
    "\n",
    "\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "figSize  = (12, 8)\n",
    "fontSize = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from scipy import interp\n",
    "from itertools import cycle, islice\n",
    "#from keras.utils import np_utils\n",
    "\n",
    "# Some preprocessing utilities\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold.t_sne import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# The different classifiers\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This Section Needs to be changed each time\n",
    "\n",
    "\n",
    "nFeatures = 7\n",
    "\n",
    "true_class_1=1;true_class_2=2;true_class_3=3;true_class_4=4;true_class_5=5;true_class_6=6;true_class_7=7;\\\n",
    "true_class_8=8;true_class_9=9;true_class_10=10;true_class_11=11;true_class_12=12;true_class_13=13\n",
    "\n",
    "plots_dir                 = './multi-class-results_test/plots/'\n",
    "results_dir               = './multi-class-results_test/results/'\n",
    "misclassify_dir           = r'./multi-class-results_test/results/Misclassification_'\n",
    "dotfile_dir               = './multi-class-results_test/Decision_tree_plots/'\n",
    "savedir_decision_boundary ='./multi-class-results_test/decision_boundary_plots/'\n",
    "features_dir              ='./multi-class-results_test/feature_analysis/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23143, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lC_dir = '../SSS_Per_Var_Cat/'\n",
    "ascii_data = pd.read_csv('../Ascii_SSS_Per_Table.txt',delim_whitespace=True,names = [\"SSS_ID\", \"File_Name\", \"RA\", \"Dec\", \"Period\", \"V_CSS\", \"Npts\", \"V_amp\", \"Type\", \"Prior_ID\", \"No_Name1\", 'No_Name2'])\n",
    "\n",
    "ascii_files = ascii_data[['File_Name', 'Type']]\n",
    "type_5  = ascii_files[ascii_files.Type==true_class_5][4509::]\n",
    "type_11 = ascii_files[ascii_files.Type==true_class_11]\n",
    "type_13 = ascii_files[ascii_files.Type==true_class_13]\n",
    "ascii_files = ascii_files.drop(type_5.index)\n",
    "ascii_files = ascii_files.drop(type_11.index)\n",
    "ascii_files = ascii_files.drop(type_13.index)\n",
    "ascii_files.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_augmentation(nAugmentation, y_training_counts):    \n",
    "    '''\n",
    "    This section calculates the number of augmentation to be carried out for each specific class\n",
    "    '''\n",
    "    number_of_samples = []\n",
    "    for i in range(len(y_training_counts)):\n",
    "        floatNsamples   = nAugmentation/y_training_counts[i] # Calculate the number of times the class need to be augmented - in float\n",
    "        nSamples        = Decimal(str(floatNsamples)).quantize(Decimal(\"1\"), rounding=ROUND_HALF_UP) # convert float to integer values\n",
    "        total_augmented = y_training_counts[i]*nSamples\n",
    "        number_of_samples.append(int(nSamples))\n",
    "        print('The number of sample in Class {} is {} and is now augmented by {} times. The augmented samples are {}'.format(i,y_training_counts[i],nSamples,total_augmented))\n",
    "    return number_of_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_period(true_class,num_samples,distribution='Normal'):\n",
    "    period_data = ascii_data[['Period', 'File_Name', 'Type']]\n",
    "    types       = period_data[period_data.Type==true_class]\n",
    "    \n",
    "    if (distribution=='Normal'):\n",
    "        mu          = np.mean(types.Period)\n",
    "        std         = np.std(types.Period)#/3.0\n",
    "        new_period  = np.abs(np.random.normal(mu, std, num_samples))\n",
    "    \n",
    "    if (distribution=='Random'):\n",
    "        new_period  = np.abs(np.random.choice(types.Period, num_samples))\n",
    "    \n",
    "#     plt.figure()\n",
    "#     n, bins, patches = plt.hist(types.Period, bins=50, facecolor='yellow', alpha=0.5, label = 'True Period Distribution')\n",
    "#     plt.axvline(new_period, color='r', linestyle='--',linewidth=2.0, label = 'New Period')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('$Period \\ (days)$', fontsize= 16)\n",
    "#     plt.ylabel('$Frequency$', fontsize= 16)\n",
    "#     plt.tick_params(axis='both', labelsize=16)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     #plt.savefig('sampling_shapeletcoeff_from_gaussian.png')\n",
    "#     plt.show()\n",
    "    \n",
    "    return new_period\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_training(augmented_data,file_name,data_columns,features,true_class,save_dir= './data/training_features/'):\n",
    "\n",
    "    nFeatures = len(features)\n",
    "    feature_file = pd.DataFrame()\n",
    "        \n",
    "    #data  = pd.read_csv(augmented_data, sep=',', header=None)\n",
    "    data  = augmented_data\n",
    "    time  = data.iloc[:,0].values\n",
    "\n",
    "    \n",
    "    for j in range(1, data.shape[1]):\n",
    "        magnitude       = data.iloc[:,j].values\n",
    "        lc              = np.array([ magnitude, time])\n",
    "        feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "        features_cal    = feature_extract.calculateFeature(lc)\n",
    "        features_name   = features_cal.result(method='features')\n",
    "        features_value  = features_cal.result(method='array')\n",
    "\n",
    "        features_df = pd.DataFrame(features_value.reshape(1,nFeatures)) \n",
    "\n",
    "        features_df['Period']     = sampling_period(true_class,num_samples=1,distribution='Random')\n",
    "        features_df['File_Name']  =  str(file_name)+'_'+str(j)\n",
    "\n",
    "\n",
    "        features_df['True_class_labels'] =  true_class\n",
    "        feature_file = feature_file.append(features_df)\n",
    "\n",
    "    return feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augmentation_and_featureExtraction(data_dir, X_training,split_num,data_columns,features,update_ascii_period):    \n",
    "    \n",
    "    '''\n",
    "    This section finds all the filenames for each specific class, starting from\n",
    "    class 0 to class 12. For each specific class, it loads the filename, then \n",
    "    perform data augmentation of this object using the number of times this\n",
    "    object needs to be augmented from the above section\n",
    "    '''\n",
    "    save_folder_training = './data/rs/MC/training_set/'\n",
    "    foldername  = \"Split_\"+str(split_num)\n",
    "\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_training+foldername, 0o755)\n",
    "\n",
    "    \n",
    "    \n",
    "    files_trueClass = np.unique(X_training.Type.values)\n",
    "#    files_trueClass = files_trueClass[0:5] # Need to comment\n",
    "#    print(files_trueClass)\n",
    "    \n",
    "    feature_file = pd.DataFrame()\n",
    "    for files in files_trueClass:\n",
    "        \n",
    "        \n",
    "        file_name = X_training.File_Name[X_training.Type.values == files]\n",
    "        file_name = file_name.values\n",
    "#         file_name = file_name[0:3] # Need to comment\n",
    "#         print(file_name)\n",
    "        \n",
    "        for datafile in file_name:\n",
    "            file           = str(data_dir)+str(datafile)+'.dat'\n",
    "            data           = pd.read_csv(file, sep=' ', header=None)\n",
    "            time           = data.iloc[:,0]\n",
    "            magnitude      = data.iloc[:,1]\n",
    "            std_mag        = data.iloc[:,2]\n",
    "            N              = data.shape[0]\n",
    "            augmented_data = pd.DataFrame()\n",
    "\n",
    "            if (files <= 10): # Data augmentation for Class 1 to Class 10\n",
    "                s = np.random.multivariate_normal(np.array(magnitude), np.identity(N)*np.array(std_mag)**2, number_of_samples[files-1])\n",
    "                s = s.T\n",
    "            else: # Data augmentation for Class 12\n",
    "                s = np.random.multivariate_normal(np.array(magnitude), np.identity(N)*np.array(std_mag)**2, number_of_samples[files-2])\n",
    "                s = s.T\n",
    "                \n",
    "            df = pd.DataFrame(s)\n",
    "            df.insert(loc=0, column='0', value=time)\n",
    "            df.insert(loc=1, column='1', value=magnitude)  \n",
    "            augmented_data = augmented_data.append(df)\n",
    "            \n",
    "            features_df  = feature_extraction_training(augmented_data=augmented_data,file_name=datafile,true_class=files,data_columns=data_columns,features=features)\n",
    "            feature_file = feature_file.append(features_df)\n",
    "\n",
    "    Newfeature_file     = feature_file.join(update_ascii_period.set_index('File_Name'), on='File_Name', lsuffix='_sample', rsuffix='_true')    \n",
    "    newFeature_data     = Newfeature_file\n",
    "\n",
    "    newFeature_data.Period_true.fillna(newFeature_data.Period_sample, inplace=True)\n",
    "    newFeature_data_df = newFeature_data.drop(labels='Period_sample', axis=1)\n",
    "    final_feature_file = newFeature_data_df[[0,1,2,3,4,5,'Period_true', 'File_Name', 'True_class_labels']]\n",
    "    final_feature_file = final_feature_file.rename(columns={'Period_true': 'Period'})\n",
    "    final_feature_file.to_csv(save_folder_training+foldername+'/Training_features.csv',index=None)#'/Type'+str(files)+'_features.csv',index=None)\n",
    "\n",
    "\n",
    "    return augmented_data, feature_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_test_set(data_dir,X_testing,data_columns,features,split_num):\n",
    "    periods           = ascii_data[['File_Name', 'Period']]\n",
    "    save_folder_test  = './data/rs/MC/test_set/'\n",
    "    foldername        = \"Split_\"+str(split_num)\n",
    "\n",
    "    if os.path.exists(foldername):\n",
    "        shutil.rmtree(foldername)\n",
    "    os.makedirs(save_folder_test+foldername, 0o755)\n",
    "    nFeatures    = len(features)\n",
    "    feature_file_test = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    files_trueClass_test = np.unique(X_testing.Type.values)\n",
    "#     files_trueClass_test = files_trueClass_test[0:5] # Need to comment\n",
    "#     print(files_trueClass_test)\n",
    "    \n",
    "    \n",
    "    feature_file_test = pd.DataFrame()\n",
    "    for files_test in files_trueClass_test:\n",
    "        \n",
    "        \n",
    "        file_name_test    = X_testing.File_Name[X_testing.Type.values == files_test]\n",
    "        file_name_test = file_name_test.values\n",
    "#         file_name_test = file_name_test[0:3] # Need to comment\n",
    "#         print(file_name_test)\n",
    "        \n",
    "        for datafile_test in file_name_test:\n",
    "            file_test      = str(data_dir)+str(datafile_test)+'.dat'\n",
    "            data_test      = pd.read_csv(file_test, sep=' ', header=None)\n",
    "            time_test      = data_test.iloc[:,0]\n",
    "            magnitude_test = data_test.iloc[:,1]\n",
    "            std_mag_test   = data_test.iloc[:,2]\n",
    " \n",
    "            lc              = np.array([ magnitude_test, time_test])\n",
    "            feature_extract = FATS.FeatureSpace(Data=data_columns, featureList = features)\n",
    "            features_cal    = feature_extract.calculateFeature(lc)\n",
    "            features_name   = features_cal.result(method='features')\n",
    "            features_value  = features_cal.result(method='array')\n",
    "\n",
    "            features_df_test              = pd.DataFrame(features_value.reshape(1,nFeatures))\n",
    "            \n",
    "            features_df_test['File_Name'] =  str(datafile_test)\n",
    "            true_class                    = files_test\n",
    "            \n",
    "            features_df_test['True_class_labels'] =  true_class\n",
    "            feature_file_test                     = feature_file_test.append(features_df_test)          \n",
    "        \n",
    "        feature_file_test['File_Name'] = feature_file_test['File_Name'].astype(int)\n",
    "        feature_file_testSet = feature_file_test.join(periods.set_index('File_Name'), on='File_Name')\n",
    "        feature_file_testSet = feature_file_testSet[[0,1,2,3,4,5,'Period', 'File_Name', 'True_class_labels']]\n",
    "        print(feature_file_testSet.shape)\n",
    "        feature_file_testSet.to_csv(save_folder_test+foldername+'/Test_features.csv',index=None)\n",
    "    return feature_file_testSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sample in Class 0 is 2883 and is now augmented by 5 times. The augmented samples are 14415\n",
      "The number of sample in Class 1 is 2501 and is now augmented by 5 times. The augmented samples are 12505\n",
      "The number of sample in Class 2 is 334 and is now augmented by 44 times. The augmented samples are 14696\n",
      "The number of sample in Class 3 is 114 and is now augmented by 131 times. The augmented samples are 14934\n",
      "The number of sample in Class 4 is 3006 and is now augmented by 4 times. The augmented samples are 12024\n",
      "The number of sample in Class 5 is 3006 and is now augmented by 4 times. The augmented samples are 12024\n",
      "The number of sample in Class 6 is 2424 and is now augmented by 6 times. The augmented samples are 14544\n",
      "The number of sample in Class 7 is 857 and is now augmented by 17 times. The augmented samples are 14569\n",
      "The number of sample in Class 8 is 98 and is now augmented by 153 times. The augmented samples are 14994\n",
      "The number of sample in Class 9 is 102 and is now augmented by 147 times. The augmented samples are 14994\n",
      "The number of sample in Class 10 is 102 and is now augmented by 147 times. The augmented samples are 14994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6765d4047fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Each class is augmented using their respective number of samples and features are extracted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0maugmentation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation_and_featureExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m                                           \u001b[0mX_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m,\u001b[0m                                          \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_ascii_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_ascii_period\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Features from each class from the test set are extracted and save in a single\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e5fd949e3513>\u001b[0m in \u001b[0;36maugmentation_and_featureExtraction\u001b[0;34m(data_dir, X_training, split_num, data_columns, features, update_ascii_period)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0maugmented_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmented_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mfeatures_df\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmented_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mfeature_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-77bc743b8f50>\u001b[0m in \u001b[0;36mfeature_extraction_training\u001b[0;34m(augmented_data, file_name, data_columns, features, true_class, save_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfeatures_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'True_class_labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrue_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mfeature_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   4433\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4434\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 4435\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   4436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4437\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1450\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m                        copy=copy)\n\u001b[0;32m-> 1452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1648\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m   1649\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m   1651\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4823\u001b[0m     blocks = [make_block(\n\u001b[1;32m   4824\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4825\u001b[0;31m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[1;32m   4826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4827\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   4920\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[1;32m   4921\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[0;32m-> 4922\u001b[0;31m                  for ju in join_units]\n\u001b[0m\u001b[1;32m   4923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m   5209\u001b[0m                 \u001b[0;31m# No dtype upcasting is done here, it will be performed during\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5210\u001b[0m                 \u001b[0;31m# concatenation itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5211\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget_values\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0man\u001b[0m \u001b[0minternal\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrently\u001b[0m \u001b[0mjust\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_splits         = 5\n",
    "data_preparation = True\n",
    "num_Augment      = 15000\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "X   = ascii_files.File_Name\n",
    "y   = ascii_files.Type\n",
    "\n",
    "periods                       = ascii_data[['File_Name', 'Period']] # Use for Test set Period\n",
    "update_ascii_period           = pd.DataFrame(periods.File_Name.astype(str) + '_1') # Use for Training set Period\n",
    "update_ascii_period['Period'] = periods.Period\n",
    "\n",
    "split_num = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    '''\n",
    "    If data_preparation is True, we split the whole data into train and test set using Kfold\n",
    "    cross-validation. The training set is augmented and features are extracted using FATS and\n",
    "    Period features are obtained by sampling from the true distribution\n",
    "    '''\n",
    "    if data_preparation:\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X.index[train_index], X.index[test_index]\n",
    "        y_train, y_test = y.index[train_index], y.index[test_index]\n",
    "        X_training      = ascii_files.iloc[train_index]\n",
    "        X_testing       = ascii_files.iloc[test_index]\n",
    "        y_training, y_training_counts = np.unique(X_training.Type, return_counts=True)\n",
    "        y_testing, y_testing_counts   = np.unique(X_testing.Type, return_counts=True)\n",
    "\n",
    "\n",
    "        data_columns = ['magnitude', 'time']\n",
    "        features     = ['Skew', 'Mean', 'Std', 'SmallKurtosis', 'Amplitude', 'Meanvariance']\n",
    "        data_dir     = '../SSS_Per_Var_Cat/'\n",
    "\n",
    "\n",
    "#         This part is calculating the number of times each class need to be augmented\n",
    "        number_of_samples               = num_augmentation(nAugmentation=num_Augment, y_training_counts=y_training_counts)\n",
    "\n",
    "        # Each class is augmented using their respective number of samples and features are extracted\n",
    "        augmentation_data, feature_file = augmentation_and_featureExtraction(data_dir=data_dir, \\\n",
    "                                          X_training=X_training,data_columns=data_columns,\\\n",
    "                                          features=features,split_num=split_num,update_ascii_period=update_ascii_period)\n",
    "\n",
    "        # Features from each class from the test set are extracted and save in a single \n",
    "        feature_file_testSet               = feature_extraction_test_set(data_dir=data_dir,\\\n",
    "                                          X_testing=X_testing,data_columns=data_columns,\\\n",
    "                                          features=features,split_num=split_num)\n",
    "\n",
    "    print('Feature Extraction for Split {} is finished'.format(split_num))  \n",
    "    split_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalisation(x_train,x_test):\n",
    "    scaler                = StandardScaler().fit(x_train.iloc[:,0:nFeatures])\n",
    "    X_train_normalisation = pd.DataFrame(scaler.transform(x_train.iloc[:,0:nFeatures]))\n",
    "    y_train_label         = x_train.True_class_labels\n",
    "    filename_train        = x_train.File_Name\n",
    "\n",
    "    X_test_normalisation = pd.DataFrame(scaler.transform(x_test.iloc[:,0:nFeatures]))\n",
    "    y_test_label         = x_test.True_class_labels\n",
    "    filename_test        = x_test.File_Name\n",
    "    \n",
    "    # A check to see whether the mean of x_train and X_test are ~ 0 with std 1.0\n",
    "#     print(X_train_normalisation.mean(axis=0))\n",
    "#     print(X_train_normalisation.std(axis=0))\n",
    "#     print(X_test_normalisation.mean(axis=0))\n",
    "#     print(X_test_normalisation.std(axis=0))\n",
    "    \n",
    "    return X_train_normalisation, y_train_label, filename_train, X_test_normalisation,\\\n",
    "           y_test_label, filename_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gridsearch(classifer, param_grid, n_iter, cv, filename='./results'):\n",
    "    grid  = RandomizedSearchCV(classifer, param_grid, n_iter = n_iter, cv = cv, scoring = \"accuracy\", n_jobs = -1,random_state=1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    opt_parameters = grid.best_params_\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    params_file = open(filename, 'w')\n",
    "    params_file.write(str(grid.best_params_))\n",
    "    params_file.close()\n",
    "    return opt_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def model_save(classifier_optimize, X_train, y_train, filename_model, save_model=False):\n",
    "    fit_model      = classifier_optimize.fit(X_train, y_train)\n",
    "    \n",
    "    if save_model:\n",
    "        pickle.dump(fit_model, open(filename_model, 'wb'))\n",
    "        \n",
    "    return fit_model\n",
    "\n",
    "def model_fit(fit_model, filename_model, X_train, y_train, X_test, y_test, classifier_model='Random Forest Classifier',classes=[\"Type 1\" , \"Type 2\"], filename ='./results/',load_model=False):\n",
    "    if load_model:\n",
    "        fit_model      = pickle.load(open(filename_model, 'rb'))\n",
    "    \n",
    "    else:\n",
    "        fit_model = fit_model\n",
    "        \n",
    "    ypred          = fit_model.predict(X_test)\n",
    "    probability    = fit_model.predict_proba(X_test)\n",
    "    accuracy       = accuracy_score(y_test, ypred)\n",
    "    MCC            = matthews_corrcoef(y_test, ypred)\n",
    "    conf_mat       = confusion_matrix(y_test, ypred)\n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    misclassified     = np.where(y_test != ypred)[0]\n",
    "\n",
    "    \n",
    "    name_file = open(filename + \".txt\", 'w')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('******* Testing Phase '+ str(classifier_model) +' for ' + str(classes) + ' *******\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(\"Accuracy: \"                    + \"%f\" % float(accuracy) + '\\n')\n",
    "    name_file.write(\"Mathews Correlation Coef: \"    + \"%f\" % float(MCC)      + '\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write('Classification Report\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.write(classification_report(y_test, ypred, target_names = classes)+'\\n')\n",
    "    name_file.write('='*80+'\\n')\n",
    "    name_file.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return ypred, accuracy, MCC, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes_types,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    \n",
    "\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(9,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    cb=plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cb.ax.tick_params(labelsize=16)\n",
    "    tick_marks = np.arange(len(classes_types))\n",
    "    plt.xticks(tick_marks, classes_types, rotation=45)\n",
    "    plt.yticks(tick_marks, classes_types)\n",
    "    plt.tick_params(axis='x', labelsize=16)\n",
    "    plt.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if (cm[i, j] < 0.01) or (cm[i,j] >= 0.75)  else \"black\",fontsize=16)\n",
    "\n",
    "    \n",
    "    plt.ylabel('True label',fontsize = 16)\n",
    "    plt.xlabel('Predicted label', fontsize = 16)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(conf_mat, classes_types, classifier_model, plot_title, X_test, y_test, nClasses,cmap=plt.cm.Reds):\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    plot_confusion_matrix(conf_mat, classes_types, normalize=True, title='Confusion matrix for ' + str(classifier_model) )\n",
    "    plt.savefig(plot_title +'_CM.pdf')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analysis_rf(X_train, y_train, types,save_model=False):\n",
    "    n_estimators      = np.arange(50,1000,100)\n",
    "    max_features      = ['auto', 'sqrt', 'log2']\n",
    "    min_samples_split = np.arange(1,20,1)\n",
    "    #max_depth         = np.arange(1,10,2)\n",
    "    param_grid        = dict(n_estimators=n_estimators, max_features=max_features, \\\n",
    "                              min_samples_split=min_samples_split)#,max_depth=max_depth\n",
    "\n",
    "        \n",
    "    opt_parameters_rf = gridsearch(RandomForestClassifier(), param_grid, n_iter = 2, cv = 5, filename= results_dir + types+'_RF_hyparameters.txt')\n",
    "    fit_model = model_save(RandomForestClassifier(**opt_parameters_rf), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types+'_RF_model.sav', save_model=save_model)\n",
    "    return opt_parameters_rf, fit_model\n",
    "\n",
    "def final_prediction(fitModel,X_train, y_train, X_test, y_test, classes, types, nClasses,load_model=False):\n",
    "\n",
    "    ypred, accuracy, MCC, conf_mat = model_fit(fitModel,filename_model= results_dir + types +'_RF_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='Random Forest Classifier',classes=classes, filename =results_dir + types+'_RF',load_model=load_model)\n",
    "\n",
    "\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='Random Forest Classifier',\\\n",
    "                                  plot_title= plots_dir + types + '_RF', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Reds)\n",
    "\n",
    "    return ypred, accuracy, MCC, conf_mat\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analysis_XGB(X_train, y_train, types,save_model=False):\n",
    "    eta       = [0.01]\n",
    "    objective = ['multi:softmax']\n",
    "    max_depth = np.arange(1,12,2)\n",
    "    subsample  = [0.5]\n",
    "    param_grid        = dict(eta=eta,objective=objective,max_depth=max_depth,subsample=subsample)\n",
    "\n",
    "        \n",
    "    opt_parameters_XGB = gridsearch(XGBClassifier(), param_grid, n_iter = 5, cv = 5, filename= results_dir + types+ '_XGB_hyparameters.txt')\n",
    "    fit_model = model_save(XGBClassifier(**opt_parameters_XGB), X_train=X_train, y_train=y_train, \\\n",
    "                           filename_model= results_dir + types + '_XGB_model.sav', save_model=save_model)\n",
    "    return opt_parameters_XGB, fit_model\n",
    "\n",
    "def final_prediction_XGB(fitModel,X_train, y_train, X_test, y_test, classes, types,nClasses,load_model=False):\n",
    "    ypred, accuracy, MCC, conf_mat  = model_fit(fitModel,filename_model= results_dir + types +'_XGB_model.sav', X_train=X_train, y_train=y_train, X_test = X_test, y_test=y_test,\\\n",
    "                                                 classifier_model='XGBoost Classifier',classes=classes, filename =results_dir + types +'_XGB', load_model=load_model)\n",
    "\n",
    "\n",
    "    plotting = plot(conf_mat, classes_types=classes, classifier_model='XGBoost Classifier',\\\n",
    "                                  plot_title= plots_dir + types +'_XGB', X_test=X_test, y_test=y_test, nClasses=nClasses,cmap=plt.cm.Blues)\n",
    "    return ypred, accuracy, MCC, conf_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_examples = 12000\n",
    "multi_class  = True\n",
    "save_model=False\n",
    "load_model=False\n",
    "acc_rf = []\n",
    "mcc_rf = []\n",
    "acc_xgb = []\n",
    "mcc_xgb = []\n",
    "\n",
    "for split_num in range(n_splits):\n",
    "    split_num += 1\n",
    "\n",
    "    training_dir = './data/rs/MC/training_set/Split_'+str(split_num)+'/'\n",
    "    testing_dir  = './data/rs/MC/test_set/Split_'+str(split_num)+'/'\n",
    "\n",
    "    all_training_set = pd.read_csv(training_dir+'Training_features.csv',sep=',')\n",
    "    all_testing_set  = pd.read_csv(testing_dir+'Test_features.csv',sep=',')\n",
    "\n",
    "    type1  = all_training_set[all_training_set.True_class_labels==true_class_1].sample(n=num_examples)\n",
    "    type2  = all_training_set[all_training_set.True_class_labels==true_class_2].sample(n=num_examples)\n",
    "    type3  = all_training_set[all_training_set.True_class_labels==true_class_3].sample(n=num_examples)\n",
    "    type4  = all_training_set[all_training_set.True_class_labels==true_class_4].sample(n=num_examples)\n",
    "    type5  = all_training_set[all_training_set.True_class_labels==true_class_5].sample(n=num_examples)\n",
    "    type6  = all_training_set[all_training_set.True_class_labels==true_class_6].sample(n=num_examples)\n",
    "    type7  = all_training_set[all_training_set.True_class_labels==true_class_7].sample(n=num_examples)\n",
    "    type8  = all_training_set[all_training_set.True_class_labels==true_class_8].sample(n=num_examples)\n",
    "    type9  = all_training_set[all_training_set.True_class_labels==true_class_9].sample(n=num_examples)\n",
    "    type10 = all_training_set[all_training_set.True_class_labels==true_class_10].sample(n=num_examples)\n",
    "    type12 = all_training_set[all_training_set.True_class_labels==true_class_12].sample(n=num_examples)\n",
    "    if multi_class:\n",
    "        training_set = pd.concat([type1,type2,type3,type4,type5,type6,type7,type8,type9,type10,type12], axis=0)   \n",
    "        testing_set = all_testing_set\n",
    "    else:\n",
    "        training_set = pd.concat([type1,type2,type3,type4,type5], axis=0) \n",
    "        #training_set = pd.concat([type1,type2], axis=0)\n",
    "        testing_set = all_testing_set\n",
    "        \n",
    "    # Performing normalisation\n",
    "    X_train, y_train, filename_train_np, X_test, y_test, filename_test_np = normalisation(training_set,testing_set)\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    \n",
    "    # This Section Needs to be changed each time\n",
    "    if multi_class:\n",
    "        classes_types = ['RRab','RRc','RRd','Blazhko','Ecl','EA','Rot','LPV','$\\delta$-Scuti', 'ACEP','Cep-II']\n",
    "        types         ='Type_all_Split_'+str(split_num)\n",
    "        nClasses      = 11\n",
    "        \n",
    "    else:\n",
    "        classes_types = ['RRab','RRc','RRd','Blazhko','Ecl']\n",
    "        types         ='Type_Binary_Split_'+str(split_num)\n",
    "        nClasses      = 5\n",
    "\n",
    "    opt_rf, fit_model_rf = analysis_rf(X_train, y_train, types, save_model) # This part can be commented if you don't want to train the algorithm\n",
    "    print(opt_rf)\n",
    "    \n",
    "    ypred_rf, accuracy_rf, MCC_rf, conf_mat_rf = final_prediction(fit_model_rf,X_train, y_train, X_test, y_test, classes_types, types, nClasses,load_model)\n",
    " \n",
    "    acc_rf.append(accuracy_rf)\n",
    "    mcc_rf.append(MCC_rf)\n",
    "\n",
    "    # XGBoost Classifier\n",
    "    \n",
    "    opt_xgb, fit_model_xgb = analysis_XGB(X_train, y_train, types, save_model) # This part can be commented when no training\n",
    "    print(opt_xgb)\n",
    "    \n",
    "    ypred_xgb, accuracy_xgb, MCC_xgb, conf_mat_xgb = final_prediction_XGB(fit_model_xgb, X_train, y_train, X_test, y_test, classes_types, types, nClasses, load_model)\n",
    " \n",
    "    acc_xgb.append(accuracy_xgb)\n",
    "    mcc_xgb.append(MCC_xgb)\n",
    "\n",
    "    \n",
    "print('Accuracy for RF is {}%'.format(np.mean(acc_rf)*100))\n",
    "print('MCC for RF is {}'.format(np.mean(mcc_rf)))\n",
    "\n",
    "print('Accuracy for XGB is {}%'.format(np.mean(acc_xgb)*100))\n",
    "print('MCC for XGB is {}'.format(np.mean(mcc_xgb)))\n",
    "\n",
    "metrics = open(\"./multi-class-results_test/metrics.txt\", 'w')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase Random Forest for ' + str(classes_types) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_rf)*100,np.std(acc_rf)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_rf)*100,np.std(mcc_rf)) + '\\n')\n",
    "    \n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write('***Testing Phase XGBoost for ' + str(classes_types) + ' ***\\n')\n",
    "metrics.write('='*80+'\\n')\n",
    "metrics.write(\"Accuracy: ({} ± {}) %\".format(np.mean(acc_xgb)*100,np.std(acc_xgb)) + '\\n')\n",
    "metrics.write(\"MCC: ({} ± {})\".format(np.mean(mcc_xgb)*100,np.std(mcc_xgb)) + '\\n')\n",
    "metrics.close()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
